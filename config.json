{
  "architectures": ["BoraMoEForCausalLM"],
  "model_type": "bora_moe",

  "hidden_size": 1024,
  "vocab_size": 32128,
  "num_hidden_layers": 23,
  "dense_layer_idxs": [],

  "num_attention_heads": 8,
  "num_key_value_heads": 4,
  "max_position_embeddings": 131072,
  "rope_theta": 1000000.0,
  "norm_eps": 1e-5,

  "layer_types": [
    "ema",
    "ema",
    "ema",
    "attn",
    "ema",
    "ema",
    "ema",
    "attn",
    "ema",
    "ema",
    "ema",
    "attn",
    "ema",
    "ema",
    "attn",
    "ema",
    "ema",
    "attn",
    "ema",
    "attn",
    "ema",
    "attn",
    "ema"
  ],

  "d_conv": 4,
  "expand": 2,
  "ema_decay_init": 0.9,

  "num_sparse_experts": 16,
  "num_shared_experts": 2,
  "num_experts_per_tok": 2,
  "ffn_dim": 768,
  "gate_activation": "sigmoid",
  "adaptive_routing": true,
  "normalize_weights": true,

  "compress_block": 8,
  "select_block": 32,
  "num_selected": 16,
  "sliding_window": 512,

  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "tie_embedding": true,
  "dtype": "bfloat16"
}
