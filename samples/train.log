Per-layer gamma: 0.0001 ~ 0.0005 (sqrt, 23 MoE layers)

============================================================
학습 시작 (step=0, epoch=0)
============================================================

=== Epoch 1/1 ===
Step       1 | Loss: 10.7934 | LR: 6.25e-07 | GradNorm: 9.22 | Tok/s: 2747 | Mem: 56.4GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.1% std=1.27] | layer_5[3.5-9.5% std=1.25] | layer_11[3.3-11.1% std=2.17] | layer_17[1.3-17.9% std=4.22] | layer_22[1.3-14.2% std=3.90]
  Expert: layer_1[4.2-9.1% std=1.36]
  Expert: layer_2[3.8-7.9% std=1.13]
  Expert: layer_3[2.8-9.3% std=1.92]
  Expert: layer_4[3.4-8.5% std=1.27]
  Expert: layer_6[4.7-8.2% std=1.03]
  Expert: layer_7[4.4-9.9% std=1.46]
  Expert: layer_8[3.7-10.6% std=1.89]
  Expert: layer_9[3.8-12.7% std=2.39]
  Expert: layer_10[2.8-10.9% std=2.24]
  Expert: layer_12[2.7-11.1% std=2.16]
  Expert: layer_13[2.6-11.4% std=2.73]
  Expert: layer_14[0.8-16.5% std=3.32]
  Expert: layer_15[2.8-11.7% std=2.93]
  Expert: layer_16[2.3-14.4% std=3.40]
  Expert: layer_18[1.2-13.9% std=3.15]
  Expert: layer_19[1.2-15.1% std=4.05]
  Expert: layer_20[2.4-14.2% std=3.48]
  Expert: layer_21[0.3-14.9% std=3.84]
Step       2 | Loss: 10.7669 | LR: 1.25e-06 | GradNorm: 9.15 | Tok/s: 2743 | Mem: 69.9GB | Data: train_2k/037.npy
  Expert: layer_0[4.2-9.1% std=1.30] | layer_5[3.6-9.2% std=1.20] | layer_11[3.3-11.1% std=2.11] | layer_17[1.3-17.7% std=4.17] | layer_22[1.5-13.5% std=3.87]
  Expert: layer_1[4.2-9.0% std=1.34]
  Expert: layer_2[3.8-7.9% std=1.11]
  Expert: layer_3[2.8-9.3% std=1.91]
  Expert: layer_4[3.4-8.4% std=1.31]
  Expert: layer_6[4.9-8.1% std=1.03]
  Expert: layer_7[4.4-9.8% std=1.43]
  Expert: layer_8[3.8-10.6% std=1.89]
  Expert: layer_9[3.8-12.5% std=2.36]
  Expert: layer_10[2.8-11.2% std=2.25]
  Expert: layer_12[2.7-11.2% std=2.22]
  Expert: layer_13[2.8-11.1% std=2.70]
  Expert: layer_14[0.8-16.1% std=3.30]
  Expert: layer_15[2.7-11.4% std=2.82]
  Expert: layer_16[2.2-14.2% std=3.38]
  Expert: layer_18[1.2-14.3% std=3.20]
  Expert: layer_19[1.3-14.8% std=4.10]
  Expert: layer_20[2.1-12.9% std=3.35]
  Expert: layer_21[0.1-15.8% std=3.92]
Step       3 | Loss: 10.7180 | LR: 1.88e-06 | GradNorm: 8.85 | Tok/s: 2743 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.0% std=1.25] | layer_5[3.6-9.6% std=1.22] | layer_11[3.4-10.9% std=2.05] | layer_17[1.5-18.2% std=4.28] | layer_22[1.3-13.7% std=3.79]
  Expert: layer_1[4.3-9.1% std=1.32]
  Expert: layer_2[3.8-7.9% std=1.11]
  Expert: layer_3[2.8-9.4% std=1.95]
  Expert: layer_4[3.4-8.3% std=1.27]
  Expert: layer_6[4.7-8.2% std=1.03]
  Expert: layer_7[4.2-10.2% std=1.49]
  Expert: layer_8[3.7-10.2% std=1.86]
  Expert: layer_9[3.7-12.2% std=2.23]
  Expert: layer_10[2.8-11.0% std=2.24]
  Expert: layer_12[2.4-11.5% std=2.22]
  Expert: layer_13[2.4-11.6% std=2.73]
  Expert: layer_14[0.8-16.2% std=3.25]
  Expert: layer_15[2.4-11.6% std=2.88]
  Expert: layer_16[2.2-14.1% std=3.39]
  Expert: layer_18[1.2-14.6% std=3.13]
  Expert: layer_19[1.2-13.5% std=3.89]
  Expert: layer_20[2.8-13.7% std=3.32]
  Expert: layer_21[0.3-16.6% std=4.04]
Step       4 | Loss: 10.6181 | LR: 2.50e-06 | GradNorm: 8.53 | Tok/s: 2611 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.9% std=1.23] | layer_5[3.8-9.2% std=1.15] | layer_11[3.5-10.5% std=1.97] | layer_17[1.8-17.8% std=4.13] | layer_22[1.4-13.9% std=3.81]
  Expert: layer_1[4.3-8.9% std=1.27]
  Expert: layer_2[3.9-7.9% std=1.08]
  Expert: layer_3[3.0-9.3% std=1.94]
  Expert: layer_4[3.3-8.6% std=1.33]
  Expert: layer_6[4.9-8.1% std=0.99]
  Expert: layer_7[4.3-10.5% std=1.55]
  Expert: layer_8[3.7-10.3% std=1.90]
  Expert: layer_9[3.8-12.4% std=2.22]
  Expert: layer_10[2.8-11.3% std=2.27]
  Expert: layer_12[2.2-11.3% std=2.27]
  Expert: layer_13[2.4-11.3% std=2.66]
  Expert: layer_14[0.7-16.3% std=3.33]
  Expert: layer_15[1.9-11.6% std=3.12]
  Expert: layer_16[2.2-14.3% std=3.54]
  Expert: layer_18[1.5-15.2% std=3.10]
  Expert: layer_19[1.1-14.2% std=3.96]
  Expert: layer_20[2.3-13.2% std=3.21]
  Expert: layer_21[0.1-17.7% std=4.32]
Step       5 | Loss: 10.4896 | LR: 3.13e-06 | GradNorm: 7.76 | Tok/s: 245 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.1% std=1.26] | layer_5[3.9-9.4% std=1.16] | layer_11[3.1-10.6% std=2.14] | layer_17[1.3-19.1% std=4.84] | layer_22[1.3-16.8% std=4.45]
  Expert: layer_1[4.3-9.0% std=1.31]
  Expert: layer_2[3.9-7.9% std=1.09]
  Expert: layer_3[2.9-9.4% std=1.93]
  Expert: layer_4[3.3-8.5% std=1.28]
  Expert: layer_6[4.7-8.4% std=1.12]
  Expert: layer_7[4.0-11.1% std=1.72]
  Expert: layer_8[3.5-10.1% std=2.00]
  Expert: layer_9[4.2-12.2% std=2.13]
  Expert: layer_10[2.6-11.5% std=2.41]
  Expert: layer_12[1.6-11.0% std=2.42]
  Expert: layer_13[2.1-11.7% std=2.76]
  Expert: layer_14[0.8-16.3% std=3.52]
  Expert: layer_15[1.4-12.8% std=3.65]
  Expert: layer_16[1.7-15.2% std=3.93]
  Expert: layer_18[1.5-16.9% std=3.46]
  Expert: layer_19[1.3-15.1% std=4.28]
  Expert: layer_20[2.0-15.6% std=3.78]
  Expert: layer_21[0.2-20.6% std=5.05]
Step       6 | Loss: 10.3296 | LR: 3.75e-06 | GradNorm: 6.85 | Tok/s: 2735 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.0% std=1.24] | layer_5[4.0-9.2% std=1.16] | layer_11[3.2-10.3% std=2.06] | layer_17[1.5-18.4% std=4.61] | layer_22[1.2-19.9% std=4.96]
  Expert: layer_1[4.3-8.9% std=1.27]
  Expert: layer_2[3.9-7.9% std=1.06]
  Expert: layer_3[2.9-9.6% std=1.99]
  Expert: layer_4[3.1-8.6% std=1.37]
  Expert: layer_6[4.6-8.3% std=1.11]
  Expert: layer_7[3.5-11.6% std=1.83]
  Expert: layer_8[3.5-9.8% std=2.00]
  Expert: layer_9[4.2-11.8% std=1.98]
  Expert: layer_10[2.5-12.1% std=2.56]
  Expert: layer_12[1.2-11.8% std=2.65]
  Expert: layer_13[1.9-12.6% std=2.82]
  Expert: layer_14[0.8-16.5% std=3.94]
  Expert: layer_15[1.1-12.5% std=3.91]
  Expert: layer_16[1.7-14.0% std=3.95]
  Expert: layer_18[1.8-18.0% std=3.70]
  Expert: layer_19[1.5-15.0% std=3.87]
  Expert: layer_20[1.1-16.9% std=3.97]
  Expert: layer_21[0.3-22.8% std=5.31]
Step       7 | Loss: 10.1979 | LR: 4.38e-06 | GradNorm: 6.03 | Tok/s: 2760 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.2% std=1.29] | layer_5[4.6-9.6% std=1.20] | layer_11[2.9-11.0% std=2.52] | layer_17[1.1-18.8% std=5.28] | layer_22[0.8-24.7% std=6.68]
  Expert: layer_1[4.3-9.1% std=1.31]
  Expert: layer_2[3.9-8.0% std=1.09]
  Expert: layer_3[3.0-9.6% std=2.00]
  Expert: layer_4[3.2-8.6% std=1.37]
  Expert: layer_6[4.4-8.5% std=1.25]
  Expert: layer_7[3.2-13.4% std=2.26]
  Expert: layer_8[3.3-9.9% std=2.19]
  Expert: layer_9[3.9-11.4% std=1.89]
  Expert: layer_10[2.3-12.2% std=2.79]
  Expert: layer_12[0.8-11.4% std=2.74]
  Expert: layer_13[1.4-12.7% std=3.04]
  Expert: layer_14[0.7-17.5% std=4.63]
  Expert: layer_15[0.7-13.2% std=4.58]
  Expert: layer_16[0.6-15.3% std=4.51]
  Expert: layer_18[1.5-20.7% std=4.40]
  Expert: layer_19[0.6-15.5% std=4.18]
  Expert: layer_20[0.8-19.2% std=4.41]
  Expert: layer_21[0.1-25.9% std=6.11]
Step       8 | Loss: 10.0449 | LR: 5.00e-06 | GradNorm: 5.38 | Tok/s: 2756 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.1% std=1.25] | layer_5[4.2-9.1% std=1.18] | layer_11[3.0-10.6% std=2.48] | layer_17[1.0-19.1% std=5.19] | layer_22[0.6-29.7% std=7.89]
  Expert: layer_1[4.3-8.8% std=1.24]
  Expert: layer_2[3.9-7.8% std=1.05]
  Expert: layer_3[3.2-10.1% std=2.05]
  Expert: layer_4[3.1-8.4% std=1.40]
  Expert: layer_6[4.1-8.6% std=1.34]
  Expert: layer_7[2.8-13.8% std=2.39]
  Expert: layer_8[3.0-9.8% std=2.23]
  Expert: layer_9[3.7-11.4% std=1.92]
  Expert: layer_10[2.2-12.9% std=2.97]
  Expert: layer_12[0.6-12.8% std=3.08]
  Expert: layer_13[1.5-12.3% std=3.06]
  Expert: layer_14[0.8-18.4% std=5.02]
  Expert: layer_15[0.4-15.0% std=4.92]
  Expert: layer_16[0.7-15.2% std=4.54]
  Expert: layer_18[1.1-20.8% std=4.67]
  Expert: layer_19[0.9-14.3% std=4.17]
  Expert: layer_20[0.6-19.2% std=4.53]
  Expert: layer_21[0.3-26.9% std=6.51]
Step       9 | Loss: 9.9072 | LR: 5.63e-06 | GradNorm: 4.97 | Tok/s: 2754 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.2-9.0% std=1.26] | layer_5[3.8-8.8% std=1.19] | layer_11[2.6-11.5% std=2.75] | layer_17[0.8-19.0% std=5.45] | layer_22[0.3-33.8% std=9.50]
  Expert: layer_1[4.3-8.6% std=1.20]
  Expert: layer_2[3.9-7.7% std=1.05]
  Expert: layer_3[3.1-10.2% std=2.12]
  Expert: layer_4[2.9-8.4% std=1.53]
  Expert: layer_6[3.7-9.1% std=1.60]
  Expert: layer_7[2.3-15.5% std=2.85]
  Expert: layer_8[2.5-10.5% std=2.43]
  Expert: layer_9[3.5-11.2% std=1.99]
  Expert: layer_10[2.0-13.8% std=3.33]
  Expert: layer_12[0.4-13.3% std=3.46]
  Expert: layer_13[1.3-12.1% std=3.33]
  Expert: layer_14[1.0-20.4% std=5.51]
  Expert: layer_15[0.3-17.0% std=5.56]
  Expert: layer_16[0.4-16.7% std=5.10]
  Expert: layer_18[0.6-23.5% std=5.49]
  Expert: layer_19[0.5-16.4% std=4.50]
  Expert: layer_20[0.4-21.8% std=5.06]
  Expert: layer_21[0.2-26.1% std=6.70]
Step      10 | Loss: 9.8121 | LR: 6.25e-06 | GradNorm: 4.47 | Tok/s: 2752 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.1% std=1.26] | layer_5[3.4-8.5% std=1.27] | layer_11[2.3-12.1% std=2.85] | layer_17[0.6-19.0% std=5.38] | layer_22[0.2-36.4% std=10.41]
  Expert: layer_1[4.3-8.3% std=1.15]
  Expert: layer_2[3.9-7.8% std=1.04]
  Expert: layer_3[2.9-10.6% std=2.26]
  Expert: layer_4[3.0-8.2% std=1.56]
  Expert: layer_6[3.5-9.4% std=1.71]
  Expert: layer_7[1.8-17.0% std=3.32]
  Expert: layer_8[2.0-11.5% std=2.71]
  Expert: layer_9[3.0-10.8% std=2.17]
  Expert: layer_10[1.9-14.9% std=3.77]
  Expert: layer_12[0.3-14.3% std=3.74]
  Expert: layer_13[1.2-11.6% std=3.49]
  Expert: layer_14[1.0-21.1% std=5.90]
  Expert: layer_15[0.2-20.3% std=6.20]
  Expert: layer_16[0.2-18.1% std=5.51]
  Expert: layer_18[0.5-24.1% std=5.76]
  Expert: layer_19[0.2-19.8% std=5.14]
  Expert: layer_20[0.3-23.2% std=5.53]
  Expert: layer_21[0.1-25.4% std=7.09]
Step      11 | Loss: 9.6484 | LR: 6.87e-06 | GradNorm: 4.19 | Tok/s: 2708 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-9.0% std=1.24] | layer_5[3.2-7.7% std=1.29] | layer_11[2.1-12.5% std=2.89] | layer_17[0.6-20.8% std=5.75] | layer_22[0.1-39.0% std=11.06]
  Expert: layer_1[4.2-8.4% std=1.15]
  Expert: layer_2[4.1-7.6% std=0.98]
  Expert: layer_3[2.7-11.2% std=2.39]
  Expert: layer_4[3.1-8.4% std=1.64]
  Expert: layer_6[3.1-9.9% std=1.96]
  Expert: layer_7[1.5-17.8% std=3.62]
  Expert: layer_8[1.4-12.6% std=2.97]
  Expert: layer_9[3.0-10.7% std=2.29]
  Expert: layer_10[1.6-15.4% std=4.15]
  Expert: layer_12[0.2-15.9% std=4.37]
  Expert: layer_13[1.2-12.1% std=3.74]
  Expert: layer_14[0.6-23.7% std=6.47]
  Expert: layer_15[0.1-24.2% std=6.85]
  Expert: layer_16[0.3-19.7% std=6.06]
  Expert: layer_18[0.3-24.6% std=6.04]
  Expert: layer_19[0.1-21.6% std=5.47]
  Expert: layer_20[0.3-24.5% std=6.12]
  Expert: layer_21[0.0-25.7% std=7.79]
Step      12 | Loss: 9.5044 | LR: 7.50e-06 | GradNorm: 3.91 | Tok/s: 2740 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.9% std=1.23] | layer_5[3.0-8.3% std=1.37] | layer_11[1.7-11.6% std=2.89] | layer_17[0.5-21.5% std=6.15] | layer_22[0.1-40.6% std=11.54]
  Expert: layer_1[4.3-8.3% std=1.12]
  Expert: layer_2[4.1-7.6% std=0.96]
  Expert: layer_3[2.5-11.5% std=2.47]
  Expert: layer_4[3.1-8.7% std=1.67]
  Expert: layer_6[2.8-10.3% std=2.16]
  Expert: layer_7[1.2-18.7% std=3.92]
  Expert: layer_8[1.1-12.5% std=3.11]
  Expert: layer_9[3.0-11.2% std=2.49]
  Expert: layer_10[1.5-15.4% std=4.44]
  Expert: layer_12[0.1-18.0% std=5.09]
  Expert: layer_13[1.1-11.7% std=3.83]
  Expert: layer_14[0.3-25.9% std=6.79]
  Expert: layer_15[0.1-27.2% std=7.45]
  Expert: layer_16[0.4-20.4% std=6.12]
  Expert: layer_18[0.2-26.2% std=6.37]
  Expert: layer_19[0.1-22.9% std=5.99]
  Expert: layer_20[0.2-26.1% std=7.07]
  Expert: layer_21[0.0-28.6% std=8.22]
Step      13 | Loss: 9.3724 | LR: 8.13e-06 | GradNorm: 3.64 | Tok/s: 2735 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.8% std=1.22] | layer_5[2.8-8.7% std=1.52] | layer_11[1.5-12.7% std=3.09] | layer_17[0.2-23.3% std=6.86] | layer_22[0.0-41.8% std=12.07]
  Expert: layer_1[4.3-8.4% std=1.11]
  Expert: layer_2[4.3-7.5% std=0.88]
  Expert: layer_3[2.2-11.5% std=2.58]
  Expert: layer_4[3.3-9.1% std=1.73]
  Expert: layer_6[2.6-10.5% std=2.29]
  Expert: layer_7[1.1-18.6% std=4.02]
  Expert: layer_8[0.9-13.4% std=3.36]
  Expert: layer_9[2.7-12.2% std=2.78]
  Expert: layer_10[1.3-16.8% std=4.71]
  Expert: layer_12[0.1-19.7% std=5.71]
  Expert: layer_13[1.0-12.3% std=4.01]
  Expert: layer_14[0.2-28.5% std=7.31]
  Expert: layer_15[0.1-29.3% std=7.84]
  Expert: layer_16[0.3-20.7% std=6.36]
  Expert: layer_18[0.2-26.8% std=6.41]
  Expert: layer_19[0.1-25.0% std=6.60]
  Expert: layer_20[0.1-27.1% std=7.80]
  Expert: layer_21[0.0-31.3% std=8.98]
Step      14 | Loss: 9.2545 | LR: 8.75e-06 | GradNorm: 3.35 | Tok/s: 2735 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.2-8.8% std=1.23] | layer_5[2.6-9.1% std=1.63] | layer_11[1.3-13.2% std=3.22] | layer_17[0.2-26.6% std=7.52] | layer_22[0.0-41.8% std=12.10]
  Expert: layer_1[4.3-8.3% std=1.08]
  Expert: layer_2[4.4-7.5% std=0.80]
  Expert: layer_3[1.5-11.4% std=2.71]
  Expert: layer_4[3.3-9.4% std=1.80]
  Expert: layer_6[2.5-10.8% std=2.37]
  Expert: layer_7[1.0-17.9% std=3.98]
  Expert: layer_8[0.7-14.4% std=3.60]
  Expert: layer_9[2.6-13.1% std=3.03]
  Expert: layer_10[1.3-18.2% std=4.82]
  Expert: layer_12[0.0-20.5% std=6.02]
  Expert: layer_13[1.1-13.2% std=4.13]
  Expert: layer_14[0.2-30.1% std=7.59]
  Expert: layer_15[0.1-29.9% std=7.84]
  Expert: layer_16[0.3-21.4% std=6.54]
  Expert: layer_18[0.2-26.5% std=6.29]
  Expert: layer_19[0.2-26.3% std=6.96]
  Expert: layer_20[0.1-27.3% std=8.02]
  Expert: layer_21[0.0-29.4% std=9.08]
Step      15 | Loss: 9.1196 | LR: 9.37e-06 | GradNorm: 3.17 | Tok/s: 2733 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.7% std=1.21] | layer_5[2.4-9.3% std=1.75] | layer_11[1.1-13.2% std=3.46] | layer_17[0.1-29.0% std=8.29] | layer_22[0.1-41.1% std=12.18]
  Expert: layer_1[4.4-8.3% std=1.07]
  Expert: layer_2[4.4-7.4% std=0.81]
  Expert: layer_3[1.3-11.2% std=2.80]
  Expert: layer_4[3.3-9.8% std=1.87]
  Expert: layer_6[2.5-10.6% std=2.41]
  Expert: layer_7[0.8-17.7% std=4.03]
  Expert: layer_8[0.6-15.0% std=3.78]
  Expert: layer_9[2.4-13.1% std=3.17]
  Expert: layer_10[1.1-19.2% std=5.09]
  Expert: layer_12[0.0-21.3% std=6.27]
  Expert: layer_13[1.0-14.3% std=4.36]
  Expert: layer_14[0.2-32.5% std=7.99]
  Expert: layer_15[0.1-30.4% std=7.91]
  Expert: layer_16[0.4-23.5% std=6.69]
  Expert: layer_18[0.3-28.2% std=6.77]
  Expert: layer_19[0.1-30.9% std=7.84]
  Expert: layer_20[0.1-26.5% std=8.25]
  Expert: layer_21[0.0-28.9% std=9.42]
Step      16 | Loss: 9.0302 | LR: 1.00e-05 | GradNorm: 2.88 | Tok/s: 2701 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.8% std=1.22] | layer_5[2.4-9.2% std=1.77] | layer_11[1.0-12.6% std=3.60] | layer_17[0.1-32.1% std=9.09] | layer_22[0.1-41.7% std=12.55]
  Expert: layer_1[4.5-8.3% std=1.08]
  Expert: layer_2[4.4-7.6% std=0.83]
  Expert: layer_3[1.2-11.6% std=2.89]
  Expert: layer_4[3.2-10.0% std=1.94]
  Expert: layer_6[2.2-11.2% std=2.57]
  Expert: layer_7[0.7-17.6% std=4.06]
  Expert: layer_8[0.5-15.0% std=3.89]
  Expert: layer_9[2.3-14.1% std=3.54]
  Expert: layer_10[1.1-20.0% std=5.30]
  Expert: layer_12[0.0-22.8% std=6.79]
  Expert: layer_13[0.9-15.1% std=4.42]
  Expert: layer_14[0.1-34.5% std=8.35]
  Expert: layer_15[0.1-32.6% std=8.31]
  Expert: layer_16[0.3-25.2% std=6.95]
  Expert: layer_18[0.4-31.7% std=7.54]
  Expert: layer_19[0.1-33.3% std=8.45]
  Expert: layer_20[0.1-27.1% std=8.06]
  Expert: layer_21[0.0-29.3% std=9.82]
Step      17 | Loss: 8.9499 | LR: 1.06e-05 | GradNorm: 2.58 | Tok/s: 2736 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.2-9.0% std=1.23] | layer_5[2.4-9.2% std=1.71] | layer_11[1.0-13.8% std=3.85] | layer_17[0.1-32.6% std=9.20] | layer_22[0.0-42.8% std=12.61]
  Expert: layer_1[4.5-8.3% std=1.07]
  Expert: layer_2[4.2-7.4% std=0.92]
  Expert: layer_3[1.2-12.4% std=2.96]
  Expert: layer_4[3.2-10.1% std=1.96]
  Expert: layer_6[2.2-11.4% std=2.64]
  Expert: layer_7[0.6-17.9% std=4.16]
  Expert: layer_8[0.5-15.0% std=3.91]
  Expert: layer_9[2.2-14.1% std=3.66]
  Expert: layer_10[1.1-19.7% std=5.38]
  Expert: layer_12[0.0-24.3% std=7.20]
  Expert: layer_13[0.7-15.3% std=4.53]
  Expert: layer_14[0.1-34.8% std=8.45]
  Expert: layer_15[0.1-34.5% std=8.82]
  Expert: layer_16[0.2-27.2% std=7.06]
  Expert: layer_18[0.3-33.2% std=7.96]
  Expert: layer_19[0.1-36.9% std=9.39]
  Expert: layer_20[0.1-27.4% std=7.88]
  Expert: layer_21[0.0-30.5% std=10.23]
Step      18 | Loss: 8.8275 | LR: 1.13e-05 | GradNorm: 2.38 | Tok/s: 2735 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.9% std=1.21] | layer_5[2.4-9.3% std=1.66] | layer_11[1.0-14.5% std=3.89] | layer_17[0.1-30.7% std=8.93] | layer_22[0.1-44.5% std=12.51]
  Expert: layer_1[4.6-8.3% std=1.05]
  Expert: layer_2[4.3-7.8% std=0.96]
  Expert: layer_3[1.1-13.1% std=3.07]
  Expert: layer_4[3.0-10.1% std=1.96]
  Expert: layer_6[2.3-11.5% std=2.66]
  Expert: layer_7[0.6-18.8% std=4.30]
  Expert: layer_8[0.5-15.1% std=4.00]
  Expert: layer_9[2.2-14.1% std=3.57]
  Expert: layer_10[1.1-18.3% std=5.24]
  Expert: layer_12[0.0-25.5% std=7.67]
  Expert: layer_13[0.8-17.1% std=4.70]
  Expert: layer_14[0.1-33.5% std=8.32]
  Expert: layer_15[0.1-34.8% std=8.92]
  Expert: layer_16[0.3-28.1% std=7.07]
  Expert: layer_18[0.3-33.8% std=8.15]
  Expert: layer_19[0.1-38.4% std=9.92]
  Expert: layer_20[0.1-28.2% std=7.68]
  Expert: layer_21[0.0-30.4% std=10.19]
Step      19 | Loss: 8.7441 | LR: 1.19e-05 | GradNorm: 2.25 | Tok/s: 2731 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.8% std=1.17] | layer_5[2.5-9.2% std=1.60] | layer_11[0.8-17.0% std=4.30] | layer_17[0.1-31.4% std=9.10] | layer_22[0.0-45.6% std=12.27]
  Expert: layer_1[4.6-8.4% std=1.04]
  Expert: layer_2[4.3-8.1% std=1.01]
  Expert: layer_3[1.0-14.4% std=3.25]
  Expert: layer_4[3.2-10.2% std=1.92]
  Expert: layer_6[2.3-11.2% std=2.56]
  Expert: layer_7[0.8-17.9% std=4.07]
  Expert: layer_8[0.5-14.4% std=4.00]
  Expert: layer_9[2.2-13.9% std=3.51]
  Expert: layer_10[1.0-17.0% std=5.13]
  Expert: layer_12[0.0-27.2% std=8.13]
  Expert: layer_13[0.7-19.0% std=4.93]
  Expert: layer_14[0.1-33.8% std=8.57]
  Expert: layer_15[0.1-36.6% std=9.26]
  Expert: layer_16[0.2-28.8% std=7.26]
  Expert: layer_18[0.3-34.5% std=8.39]
  Expert: layer_19[0.1-39.4% std=10.41]
  Expert: layer_20[0.2-29.1% std=7.63]
  Expert: layer_21[0.0-29.1% std=9.88]
Step      20 | Loss: 8.6572 | LR: 1.25e-05 | GradNorm: 2.14 | Tok/s: 2733 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.7% std=1.14] | layer_5[2.7-9.2% std=1.57] | layer_11[0.8-18.5% std=4.63] | layer_17[0.2-31.6% std=9.18] | layer_22[0.1-47.1% std=12.18]
  Expert: layer_1[4.8-8.3% std=0.99]
  Expert: layer_2[4.3-7.9% std=1.02]
  Expert: layer_3[1.0-15.8% std=3.44]
  Expert: layer_4[3.3-10.0% std=1.84]
  Expert: layer_6[2.4-10.8% std=2.40]
  Expert: layer_7[0.9-17.7% std=3.99]
  Expert: layer_8[0.5-14.2% std=4.00]
  Expert: layer_9[2.2-13.3% std=3.31]
  Expert: layer_10[1.1-15.5% std=4.96]
  Expert: layer_12[0.0-27.7% std=8.35]
  Expert: layer_13[0.6-19.6% std=5.08]
  Expert: layer_14[0.0-32.3% std=8.59]
  Expert: layer_15[0.1-38.1% std=9.54]
  Expert: layer_16[0.1-28.2% std=7.28]
  Expert: layer_18[0.2-35.0% std=8.51]
  Expert: layer_19[0.0-40.9% std=10.89]
  Expert: layer_20[0.1-27.6% std=7.43]
  Expert: layer_21[0.0-30.5% std=9.88]
Step      21 | Loss: 8.5811 | LR: 1.31e-05 | GradNorm: 1.96 | Tok/s: 2701 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.9% std=1.18] | layer_5[2.9-9.1% std=1.51] | layer_11[0.7-18.0% std=4.54] | layer_17[0.1-30.1% std=8.97] | layer_22[0.0-48.1% std=12.15]
  Expert: layer_1[4.8-8.4% std=1.03]
  Expert: layer_2[4.3-7.8% std=1.06]
  Expert: layer_3[1.1-16.3% std=3.56]
  Expert: layer_4[3.3-10.0% std=1.80]
  Expert: layer_6[2.3-10.4% std=2.28]
  Expert: layer_7[0.9-17.4% std=3.92]
  Expert: layer_8[0.5-13.8% std=3.99]
  Expert: layer_9[2.3-12.9% std=3.19]
  Expert: layer_10[1.0-15.0% std=4.91]
  Expert: layer_12[0.0-27.2% std=8.38]
  Expert: layer_13[0.5-19.2% std=5.07]
  Expert: layer_14[0.0-30.0% std=8.55]
  Expert: layer_15[0.1-40.0% std=9.78]
  Expert: layer_16[0.1-26.8% std=7.01]
  Expert: layer_18[0.2-35.4% std=8.53]
  Expert: layer_19[0.0-41.7% std=11.20]
  Expert: layer_20[0.1-24.6% std=7.12]
  Expert: layer_21[0.0-34.0% std=10.22]
Step      22 | Loss: 8.4629 | LR: 1.37e-05 | GradNorm: 1.87 | Tok/s: 2733 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.9% std=1.15] | layer_5[3.2-8.8% std=1.44] | layer_11[0.5-18.3% std=4.52] | layer_17[0.1-28.2% std=8.80] | layer_22[0.0-48.0% std=11.94]
  Expert: layer_1[5.0-8.6% std=1.01]
  Expert: layer_2[4.3-7.8% std=1.09]
  Expert: layer_3[1.1-16.7% std=3.57]
  Expert: layer_4[3.1-9.6% std=1.79]
  Expert: layer_6[2.3-10.1% std=2.19]
  Expert: layer_7[1.1-16.4% std=3.74]
  Expert: layer_8[0.6-13.6% std=3.96]
  Expert: layer_9[2.5-12.2% std=2.96]
  Expert: layer_10[1.0-15.4% std=4.84]
  Expert: layer_12[0.0-27.0% std=8.42]
  Expert: layer_13[0.5-18.6% std=5.04]
  Expert: layer_14[0.0-29.1% std=8.44]
  Expert: layer_15[0.1-40.8% std=9.90]
  Expert: layer_16[0.1-26.9% std=7.16]
  Expert: layer_18[0.2-35.2% std=8.51]
  Expert: layer_19[0.0-42.9% std=11.38]
  Expert: layer_20[0.1-20.8% std=6.63]
  Expert: layer_21[0.0-37.3% std=10.63]
Step      23 | Loss: 8.3026 | LR: 1.44e-05 | GradNorm: 1.80 | Tok/s: 2734 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.2-8.8% std=1.15] | layer_5[3.3-8.5% std=1.36] | layer_11[0.5-17.3% std=4.31] | layer_17[0.2-30.3% std=8.63] | layer_22[0.0-47.2% std=11.54]
  Expert: layer_1[4.8-8.7% std=1.02]
  Expert: layer_2[4.3-7.8% std=1.09]
  Expert: layer_3[1.2-16.7% std=3.57]
  Expert: layer_4[3.1-9.5% std=1.67]
  Expert: layer_6[2.4-9.7% std=2.15]
  Expert: layer_7[1.2-15.2% std=3.49]
  Expert: layer_8[0.8-12.7% std=3.78]
  Expert: layer_9[2.6-11.9% std=2.84]
  Expert: layer_10[1.2-15.6% std=4.68]
  Expert: layer_12[0.0-25.3% std=8.01]
  Expert: layer_13[0.5-17.0% std=4.78]
  Expert: layer_14[0.0-28.1% std=8.36]
  Expert: layer_15[0.1-41.9% std=10.05]
  Expert: layer_16[0.1-25.4% std=6.70]
  Expert: layer_18[0.2-34.1% std=8.16]
  Expert: layer_19[0.0-43.4% std=11.53]
  Expert: layer_20[0.1-20.2% std=5.89]
  Expert: layer_21[0.0-37.9% std=10.36]
Step      24 | Loss: 8.2529 | LR: 1.50e-05 | GradNorm: 1.69 | Tok/s: 2731 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.7% std=1.12] | layer_5[3.7-8.3% std=1.29] | layer_11[0.6-16.9% std=4.09] | layer_17[0.2-30.9% std=8.37] | layer_22[0.0-47.0% std=11.54]
  Expert: layer_1[4.8-8.6% std=1.07]
  Expert: layer_2[4.3-7.7% std=1.06]
  Expert: layer_3[1.4-16.7% std=3.49]
  Expert: layer_4[3.1-8.8% std=1.58]
  Expert: layer_6[2.5-9.6% std=2.08]
  Expert: layer_7[1.3-15.5% std=3.51]
  Expert: layer_8[0.9-12.5% std=3.63]
  Expert: layer_9[2.8-11.4% std=2.62]
  Expert: layer_10[1.4-15.7% std=4.52]
  Expert: layer_12[0.0-25.4% std=7.89]
  Expert: layer_13[0.5-15.6% std=4.60]
  Expert: layer_14[0.0-27.6% std=8.14]
  Expert: layer_15[0.1-42.0% std=9.99]
  Expert: layer_16[0.2-26.3% std=6.74]
  Expert: layer_18[0.2-31.5% std=7.65]
  Expert: layer_19[0.0-44.3% std=11.69]
  Expert: layer_20[0.1-21.7% std=5.89]
  Expert: layer_21[0.0-39.9% std=10.67]
Step      25 | Loss: 8.1583 | LR: 1.56e-05 | GradNorm: 1.59 | Tok/s: 2729 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.2-8.6% std=1.13] | layer_5[4.0-8.7% std=1.27] | layer_11[0.6-17.5% std=4.18] | layer_17[0.2-30.7% std=7.89] | layer_22[0.0-47.8% std=11.76]
  Expert: layer_1[4.8-8.8% std=1.11]
  Expert: layer_2[4.3-8.1% std=1.06]
  Expert: layer_3[1.4-16.5% std=3.44]
  Expert: layer_4[3.3-8.7% std=1.53]
  Expert: layer_6[2.3-9.7% std=2.10]
  Expert: layer_7[1.3-15.9% std=3.48]
  Expert: layer_8[0.9-12.1% std=3.51]
  Expert: layer_9[2.8-11.3% std=2.48]
  Expert: layer_10[1.4-15.5% std=4.40]
  Expert: layer_12[0.0-25.6% std=7.96]
  Expert: layer_13[0.6-16.0% std=4.55]
  Expert: layer_14[0.0-27.1% std=7.88]
  Expert: layer_15[0.2-41.7% std=9.95]
  Expert: layer_16[0.2-26.2% std=6.72]
  Expert: layer_18[0.1-29.1% std=7.23]
  Expert: layer_19[0.1-45.9% std=11.96]
  Expert: layer_20[0.0-22.2% std=6.14]
  Expert: layer_21[0.0-41.4% std=11.11]
Step      26 | Loss: 8.0923 | LR: 1.63e-05 | GradNorm: 1.49 | Tok/s: 2695 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.6% std=1.09] | layer_5[4.1-8.6% std=1.25] | layer_11[0.7-17.6% std=4.21] | layer_17[0.2-29.1% std=7.50] | layer_22[0.0-48.2% std=11.85]
  Expert: layer_1[4.6-8.6% std=1.08]
  Expert: layer_2[4.3-8.2% std=0.99]
  Expert: layer_3[1.6-16.1% std=3.33]
  Expert: layer_4[3.5-8.3% std=1.45]
  Expert: layer_6[2.2-9.9% std=2.07]
  Expert: layer_7[1.3-15.1% std=3.33]
  Expert: layer_8[1.1-12.3% std=3.36]
  Expert: layer_9[2.6-11.1% std=2.33]
  Expert: layer_10[1.4-15.1% std=4.25]
  Expert: layer_12[0.0-26.3% std=8.04]
  Expert: layer_13[0.7-15.7% std=4.40]
  Expert: layer_14[0.1-26.7% std=7.62]
  Expert: layer_15[0.2-40.6% std=9.69]
  Expert: layer_16[0.2-25.3% std=6.53]
  Expert: layer_18[0.2-27.0% std=6.84]
  Expert: layer_19[0.1-46.1% std=11.87]
  Expert: layer_20[0.0-22.5% std=6.29]
  Expert: layer_21[0.0-41.7% std=11.12]
Step      27 | Loss: 8.0541 | LR: 1.69e-05 | GradNorm: 1.44 | Tok/s: 2729 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.7% std=1.12] | layer_5[4.2-8.6% std=1.21] | layer_11[0.8-18.0% std=4.41] | layer_17[0.2-29.3% std=7.63] | layer_22[0.0-48.6% std=11.95]
  Expert: layer_1[4.7-8.6% std=1.05]
  Expert: layer_2[4.4-8.1% std=0.95]
  Expert: layer_3[1.6-15.4% std=3.22]
  Expert: layer_4[3.4-8.3% std=1.48]
  Expert: layer_6[2.2-10.2% std=2.13]
  Expert: layer_7[1.3-15.4% std=3.30]
  Expert: layer_8[1.1-12.1% std=3.28]
  Expert: layer_9[2.6-11.1% std=2.24]
  Expert: layer_10[1.4-14.5% std=4.20]
  Expert: layer_12[0.1-27.5% std=8.01]
  Expert: layer_13[0.8-15.7% std=4.27]
  Expert: layer_14[0.1-27.3% std=7.54]
  Expert: layer_15[0.2-39.7% std=9.48]
  Expert: layer_16[0.2-23.0% std=6.14]
  Expert: layer_18[0.2-24.8% std=6.48]
  Expert: layer_19[0.1-46.1% std=11.88]
  Expert: layer_20[0.0-22.4% std=6.46]
  Expert: layer_21[0.0-40.4% std=10.60]
Step      28 | Loss: 7.9321 | LR: 1.75e-05 | GradNorm: 1.39 | Tok/s: 2723 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.6% std=1.09] | layer_5[4.2-8.6% std=1.19] | layer_11[0.8-17.6% std=4.40] | layer_17[0.1-30.2% std=7.98] | layer_22[0.0-48.7% std=12.35]
  Expert: layer_1[5.0-8.5% std=0.99]
  Expert: layer_2[4.4-7.8% std=0.91]
  Expert: layer_3[1.6-15.0% std=3.20]
  Expert: layer_4[3.3-8.3% std=1.50]
  Expert: layer_6[2.3-10.3% std=2.13]
  Expert: layer_7[1.1-15.7% std=3.37]
  Expert: layer_8[1.0-12.0% std=3.30]
  Expert: layer_9[2.8-11.0% std=2.14]
  Expert: layer_10[1.3-14.5% std=4.26]
  Expert: layer_12[0.1-28.3% std=7.94]
  Expert: layer_13[1.0-15.2% std=4.25]
  Expert: layer_14[0.1-27.1% std=7.19]
  Expert: layer_15[0.2-39.8% std=9.44]
  Expert: layer_16[0.2-21.8% std=5.85]
  Expert: layer_18[0.2-24.9% std=6.55]
  Expert: layer_19[0.0-45.5% std=11.65]
  Expert: layer_20[0.0-23.5% std=6.66]
  Expert: layer_21[0.0-39.6% std=10.32]
Step      29 | Loss: 7.8738 | LR: 1.81e-05 | GradNorm: 1.29 | Tok/s: 2722 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.7% std=1.11] | layer_5[4.2-8.4% std=1.12] | layer_11[0.9-17.4% std=4.32] | layer_17[0.2-30.8% std=8.04] | layer_22[0.0-48.9% std=12.26]
  Expert: layer_1[5.1-8.5% std=0.94]
  Expert: layer_2[4.5-7.4% std=0.83]
  Expert: layer_3[1.5-14.5% std=3.11]
  Expert: layer_4[3.1-8.3% std=1.50]
  Expert: layer_6[2.3-10.6% std=2.18]
  Expert: layer_7[1.0-16.3% std=3.41]
  Expert: layer_8[1.1-12.0% std=3.31]
  Expert: layer_9[3.0-10.9% std=2.11]
  Expert: layer_10[1.2-15.2% std=4.32]
  Expert: layer_12[0.1-28.8% std=7.87]
  Expert: layer_13[1.4-15.2% std=4.31]
  Expert: layer_14[0.1-27.1% std=7.03]
  Expert: layer_15[0.3-40.0% std=9.50]
  Expert: layer_16[0.2-20.6% std=5.65]
  Expert: layer_18[0.1-24.8% std=6.51]
  Expert: layer_19[0.1-45.1% std=11.40]
  Expert: layer_20[0.0-25.6% std=7.02]
  Expert: layer_21[0.1-40.6% std=10.41]
Step      30 | Loss: 7.8260 | LR: 1.87e-05 | GradNorm: 1.25 | Tok/s: 2667 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.6% std=1.10] | layer_5[4.3-8.2% std=1.06] | layer_11[0.8-17.2% std=4.35] | layer_17[0.2-30.0% std=7.99] | layer_22[0.0-48.9% std=12.28]
  Expert: layer_1[5.0-8.3% std=0.89]
  Expert: layer_2[4.6-7.2% std=0.82]
  Expert: layer_3[1.5-14.3% std=3.04]
  Expert: layer_4[3.2-8.2% std=1.49]
  Expert: layer_6[2.3-10.5% std=2.14]
  Expert: layer_7[1.2-16.7% std=3.41]
  Expert: layer_8[1.2-11.3% std=3.26]
  Expert: layer_9[3.2-11.1% std=2.05]
  Expert: layer_10[1.1-15.7% std=4.20]
  Expert: layer_12[0.1-27.7% std=7.61]
  Expert: layer_13[1.4-14.8% std=4.21]
  Expert: layer_14[0.1-25.9% std=6.82]
  Expert: layer_15[0.3-39.2% std=9.22]
  Expert: layer_16[0.1-19.1% std=5.25]
  Expert: layer_18[0.1-24.2% std=6.21]
  Expert: layer_19[0.0-44.9% std=11.29]
  Expert: layer_20[0.0-26.5% std=7.17]
  Expert: layer_21[0.1-41.2% std=10.45]
Step      31 | Loss: 7.7690 | LR: 1.94e-05 | GradNorm: 1.20 | Tok/s: 729 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.8% std=1.14] | layer_5[4.0-7.9% std=1.07] | layer_11[0.7-16.8% std=4.15] | layer_17[0.2-28.6% std=7.85] | layer_22[0.0-48.9% std=12.18]
  Expert: layer_1[4.8-8.3% std=0.88]
  Expert: layer_2[4.5-7.3% std=0.86]
  Expert: layer_3[1.6-14.0% std=2.95]
  Expert: layer_4[3.1-8.0% std=1.48]
  Expert: layer_6[2.4-10.5% std=2.11]
  Expert: layer_7[1.3-16.7% std=3.35]
  Expert: layer_8[1.2-11.4% std=3.24]
  Expert: layer_9[3.2-11.4% std=2.06]
  Expert: layer_10[1.0-16.2% std=4.17]
  Expert: layer_12[0.2-26.5% std=7.39]
  Expert: layer_13[1.4-14.8% std=4.19]
  Expert: layer_14[0.0-24.6% std=6.73]
  Expert: layer_15[0.3-38.4% std=9.02]
  Expert: layer_16[0.1-17.9% std=5.21]
  Expert: layer_18[0.2-22.8% std=5.94]
  Expert: layer_19[0.0-44.2% std=10.99]
  Expert: layer_20[0.0-26.4% std=7.10]
  Expert: layer_21[0.1-41.3% std=10.50]
Step      32 | Loss: 7.6534 | LR: 2.00e-05 | GradNorm: 1.23 | Tok/s: 571 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.7% std=1.14] | layer_5[3.9-8.0% std=1.11] | layer_11[0.8-16.4% std=3.98] | layer_17[0.2-28.6% std=7.80] | layer_22[0.1-48.8% std=12.28]
  Expert: layer_1[4.8-8.2% std=0.89]
  Expert: layer_2[4.6-7.3% std=0.83]
  Expert: layer_3[1.6-13.5% std=2.87]
  Expert: layer_4[3.2-8.1% std=1.43]
  Expert: layer_6[2.5-10.5% std=2.08]
  Expert: layer_7[1.4-16.3% std=3.31]
  Expert: layer_8[1.2-11.1% std=3.23]
  Expert: layer_9[3.4-11.1% std=1.97]
  Expert: layer_10[1.0-15.9% std=4.04]
  Expert: layer_12[0.2-25.2% std=7.18]
  Expert: layer_13[1.4-15.7% std=4.19]
  Expert: layer_14[0.1-25.0% std=6.83]
  Expert: layer_15[0.4-37.6% std=8.83]
  Expert: layer_16[0.1-17.4% std=5.17]
  Expert: layer_18[0.1-21.3% std=5.69]
  Expert: layer_19[0.0-43.8% std=10.72]
  Expert: layer_20[0.0-23.8% std=6.40]
  Expert: layer_21[0.1-40.6% std=10.55]
Step      33 | Loss: 7.6528 | LR: 2.06e-05 | GradNorm: 1.14 | Tok/s: 2621 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.8% std=1.15] | layer_5[3.7-8.3% std=1.22] | layer_11[0.9-15.7% std=3.72] | layer_17[0.3-28.3% std=7.71] | layer_22[0.1-48.8% std=12.29]
  Expert: layer_1[4.6-8.1% std=0.91]
  Expert: layer_2[4.4-7.3% std=0.87]
  Expert: layer_3[1.8-13.7% std=2.83]
  Expert: layer_4[3.2-8.4% std=1.43]
  Expert: layer_6[2.8-10.4% std=1.96]
  Expert: layer_7[1.4-16.5% std=3.36]
  Expert: layer_8[1.2-11.1% std=3.19]
  Expert: layer_9[3.3-10.7% std=1.91]
  Expert: layer_10[0.8-16.3% std=4.05]
  Expert: layer_12[0.3-24.5% std=7.06]
  Expert: layer_13[1.1-15.9% std=4.21]
  Expert: layer_14[0.0-24.9% std=6.99]
  Expert: layer_15[0.3-36.6% std=8.59]
  Expert: layer_16[0.0-17.2% std=5.29]
  Expert: layer_18[0.1-21.1% std=5.84]
  Expert: layer_19[0.0-43.0% std=10.52]
  Expert: layer_20[0.0-22.2% std=6.13]
  Expert: layer_21[0.2-41.6% std=10.91]
Step      34 | Loss: 7.5500 | LR: 2.13e-05 | GradNorm: 1.12 | Tok/s: 2582 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.6% std=1.14] | layer_5[3.5-8.1% std=1.25] | layer_11[0.9-15.4% std=3.61] | layer_17[0.4-26.3% std=7.27] | layer_22[0.0-48.7% std=12.25]
  Expert: layer_1[4.6-8.2% std=0.93]
  Expert: layer_2[4.6-7.2% std=0.81]
  Expert: layer_3[1.9-13.6% std=2.76]
  Expert: layer_4[3.2-8.6% std=1.41]
  Expert: layer_6[2.8-10.4% std=1.89]
  Expert: layer_7[1.6-16.3% std=3.33]
  Expert: layer_8[1.3-10.9% std=3.08]
  Expert: layer_9[3.3-10.7% std=1.84]
  Expert: layer_10[0.8-16.1% std=3.95]
  Expert: layer_12[0.3-24.3% std=7.02]
  Expert: layer_13[1.2-16.4% std=4.22]
  Expert: layer_14[0.1-25.2% std=7.04]
  Expert: layer_15[0.4-36.0% std=8.43]
  Expert: layer_16[0.0-17.5% std=5.30]
  Expert: layer_18[0.1-22.0% std=5.95]
  Expert: layer_19[0.0-42.9% std=10.43]
  Expert: layer_20[0.0-21.2% std=5.98]
  Expert: layer_21[0.3-41.9% std=10.92]
Step      35 | Loss: 7.5169 | LR: 2.19e-05 | GradNorm: 1.07 | Tok/s: 2582 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.7% std=1.16] | layer_5[3.4-8.1% std=1.29] | layer_11[0.8-15.7% std=3.65] | layer_17[0.4-24.7% std=6.92] | layer_22[0.0-48.6% std=12.26]
  Expert: layer_1[4.5-8.5% std=0.97]
  Expert: layer_2[4.7-7.4% std=0.75]
  Expert: layer_3[1.9-13.7% std=2.78]
  Expert: layer_4[3.3-9.0% std=1.47]
  Expert: layer_6[2.7-10.5% std=1.90]
  Expert: layer_7[1.6-16.5% std=3.37]
  Expert: layer_8[1.2-11.0% std=3.00]
  Expert: layer_9[3.6-10.1% std=1.78]
  Expert: layer_10[0.7-16.2% std=3.96]
  Expert: layer_12[0.3-24.2% std=7.02]
  Expert: layer_13[1.1-16.9% std=4.31]
  Expert: layer_14[0.0-25.8% std=7.14]
  Expert: layer_15[0.3-36.3% std=8.49]
  Expert: layer_16[0.0-17.6% std=5.27]
  Expert: layer_18[0.1-22.7% std=6.02]
  Expert: layer_19[0.0-43.9% std=10.63]
  Expert: layer_20[0.0-21.8% std=6.02]
  Expert: layer_21[0.2-42.4% std=11.01]
Step      36 | Loss: 7.4358 | LR: 2.25e-05 | GradNorm: 1.06 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.7% std=1.16] | layer_5[3.3-8.0% std=1.28] | layer_11[0.7-16.2% std=3.64] | layer_17[0.3-23.9% std=6.73] | layer_22[0.0-48.5% std=12.06]
  Expert: layer_1[4.5-8.6% std=0.99]
  Expert: layer_2[4.8-7.4% std=0.73]
  Expert: layer_3[2.0-14.1% std=2.83]
  Expert: layer_4[3.4-9.1% std=1.47]
  Expert: layer_6[2.7-10.7% std=1.93]
  Expert: layer_7[1.5-16.7% std=3.46]
  Expert: layer_8[1.3-10.3% std=2.79]
  Expert: layer_9[3.5-9.8% std=1.77]
  Expert: layer_10[0.8-15.9% std=3.92]
  Expert: layer_12[0.3-24.0% std=6.72]
  Expert: layer_13[1.3-17.4% std=4.36]
  Expert: layer_14[0.0-25.9% std=6.95]
  Expert: layer_15[0.4-35.3% std=8.25]
  Expert: layer_16[0.1-17.2% std=5.07]
  Expert: layer_18[0.1-23.0% std=5.89]
  Expert: layer_19[0.0-43.8% std=10.57]
  Expert: layer_20[0.0-22.7% std=5.92]
  Expert: layer_21[0.2-42.7% std=11.00]
Step      37 | Loss: 7.4090 | LR: 2.31e-05 | GradNorm: 1.02 | Tok/s: 2585 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.6% std=1.16] | layer_5[3.5-8.0% std=1.27] | layer_11[0.8-16.3% std=3.63] | layer_17[0.3-23.2% std=6.70] | layer_22[0.1-48.2% std=11.84]
  Expert: layer_1[4.7-8.8% std=0.98]
  Expert: layer_2[4.7-7.7% std=0.74]
  Expert: layer_3[2.0-13.8% std=2.78]
  Expert: layer_4[3.2-8.9% std=1.43]
  Expert: layer_6[2.6-10.6% std=1.95]
  Expert: layer_7[1.2-16.6% std=3.43]
  Expert: layer_8[1.2-10.1% std=2.84]
  Expert: layer_9[3.7-9.6% std=1.78]
  Expert: layer_10[0.8-16.2% std=3.93]
  Expert: layer_12[0.4-24.1% std=6.61]
  Expert: layer_13[1.3-18.0% std=4.49]
  Expert: layer_14[0.0-25.0% std=6.78]
  Expert: layer_15[0.4-34.9% std=8.21]
  Expert: layer_16[0.1-17.9% std=5.08]
  Expert: layer_18[0.1-23.8% std=5.97]
  Expert: layer_19[0.0-44.2% std=10.77]
  Expert: layer_20[0.1-23.1% std=5.87]
  Expert: layer_21[0.1-43.5% std=11.23]
Step      38 | Loss: 7.3717 | LR: 2.37e-05 | GradNorm: 0.99 | Tok/s: 2578 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.7% std=1.16] | layer_5[3.4-8.0% std=1.23] | layer_11[0.9-15.6% std=3.50] | layer_17[0.4-23.1% std=6.61] | layer_22[0.1-48.0% std=11.70]
  Expert: layer_1[4.7-8.7% std=0.97]
  Expert: layer_2[4.8-8.0% std=0.75]
  Expert: layer_3[2.0-13.8% std=2.82]
  Expert: layer_4[3.1-8.6% std=1.33]
  Expert: layer_6[2.6-11.0% std=1.98]
  Expert: layer_7[1.2-16.0% std=3.33]
  Expert: layer_8[1.3-9.6% std=2.79]
  Expert: layer_9[3.8-9.4% std=1.70]
  Expert: layer_10[0.9-15.9% std=3.83]
  Expert: layer_12[0.4-23.6% std=6.45]
  Expert: layer_13[1.3-17.5% std=4.40]
  Expert: layer_14[0.1-24.3% std=6.65]
  Expert: layer_15[0.4-34.8% std=8.19]
  Expert: layer_16[0.1-17.7% std=5.01]
  Expert: layer_18[0.1-24.2% std=5.98]
  Expert: layer_19[0.1-43.9% std=10.58]
  Expert: layer_20[0.1-25.1% std=6.08]
  Expert: layer_21[0.1-43.6% std=11.27]
Step      39 | Loss: 7.3133 | LR: 2.44e-05 | GradNorm: 0.98 | Tok/s: 2563 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.6% std=1.16] | layer_5[3.6-8.2% std=1.23] | layer_11[1.0-15.3% std=3.52] | layer_17[0.5-22.9% std=6.66] | layer_22[0.1-48.0% std=11.68]
  Expert: layer_1[4.7-8.9% std=0.98]
  Expert: layer_2[4.9-8.1% std=0.75]
  Expert: layer_3[2.0-13.8% std=2.82]
  Expert: layer_4[2.9-8.4% std=1.32]
  Expert: layer_6[2.5-10.8% std=1.95]
  Expert: layer_7[1.1-17.0% std=3.47]
  Expert: layer_8[1.2-9.9% std=2.81]
  Expert: layer_9[3.9-10.3% std=1.76]
  Expert: layer_10[1.0-15.9% std=3.82]
  Expert: layer_12[0.4-24.1% std=6.54]
  Expert: layer_13[1.1-17.3% std=4.37]
  Expert: layer_14[0.1-23.1% std=6.47]
  Expert: layer_15[0.4-35.2% std=8.26]
  Expert: layer_16[0.0-18.0% std=5.19]
  Expert: layer_18[0.2-24.7% std=5.99]
  Expert: layer_19[0.1-43.3% std=10.36]
  Expert: layer_20[0.0-26.8% std=6.36]
  Expert: layer_21[0.1-44.1% std=11.26]
Step      40 | Loss: 7.2399 | LR: 2.50e-05 | GradNorm: 0.95 | Tok/s: 2570 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.6% std=1.15] | layer_5[3.8-8.4% std=1.18] | layer_11[1.2-14.2% std=3.41] | layer_17[0.6-23.8% std=7.01] | layer_22[0.2-47.7% std=11.54]
  Expert: layer_1[4.7-8.9% std=0.96]
  Expert: layer_2[4.8-7.7% std=0.73]
  Expert: layer_3[1.9-13.8% std=2.81]
  Expert: layer_4[3.0-7.9% std=1.25]
  Expert: layer_6[2.4-10.8% std=1.92]
  Expert: layer_7[1.1-17.2% std=3.52]
  Expert: layer_8[1.2-10.2% std=2.73]
  Expert: layer_9[3.8-10.5% std=1.75]
  Expert: layer_10[1.0-15.5% std=3.73]
  Expert: layer_12[0.3-24.0% std=6.50]
  Expert: layer_13[1.1-16.0% std=4.22]
  Expert: layer_14[0.1-22.9% std=6.35]
  Expert: layer_15[0.2-36.0% std=8.37]
  Expert: layer_16[0.0-17.0% std=5.11]
  Expert: layer_18[0.2-24.3% std=5.88]
  Expert: layer_19[0.2-42.5% std=10.16]
  Expert: layer_20[0.0-27.2% std=6.40]
  Expert: layer_21[0.1-43.6% std=11.12]
Step      41 | Loss: 7.1344 | LR: 2.56e-05 | GradNorm: 0.94 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.6% std=1.16] | layer_5[4.0-8.4% std=1.14] | layer_11[1.5-14.0% std=3.27] | layer_17[0.6-25.3% std=7.43] | layer_22[0.2-47.5% std=11.43]
  Expert: layer_1[4.9-8.9% std=0.95]
  Expert: layer_2[4.7-7.4% std=0.73]
  Expert: layer_3[1.9-13.4% std=2.74]
  Expert: layer_4[3.0-7.8% std=1.22]
  Expert: layer_6[2.4-10.9% std=1.93]
  Expert: layer_7[1.1-16.5% std=3.45]
  Expert: layer_8[1.2-10.3% std=2.71]
  Expert: layer_9[3.5-10.1% std=1.71]
  Expert: layer_10[1.1-15.6% std=3.82]
  Expert: layer_12[0.3-24.2% std=6.47]
  Expert: layer_13[1.1-15.0% std=4.06]
  Expert: layer_14[0.1-24.1% std=6.51]
  Expert: layer_15[0.2-37.1% std=8.57]
  Expert: layer_16[0.0-16.6% std=5.01]
  Expert: layer_18[0.3-24.3% std=5.79]
  Expert: layer_19[0.2-41.9% std=10.05]
  Expert: layer_20[0.1-26.1% std=6.20]
  Expert: layer_21[0.1-43.0% std=10.91]
Step      42 | Loss: 7.1275 | LR: 2.63e-05 | GradNorm: 0.91 | Tok/s: 2541 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.6% std=1.17] | layer_5[4.1-8.1% std=1.11] | layer_11[1.6-13.8% std=3.26] | layer_17[0.6-26.8% std=7.75] | layer_22[0.2-47.5% std=11.36]
  Expert: layer_1[4.8-8.9% std=0.96]
  Expert: layer_2[4.6-7.5% std=0.76]
  Expert: layer_3[1.9-13.9% std=2.81]
  Expert: layer_4[3.0-7.7% std=1.16]
  Expert: layer_6[2.4-10.9% std=1.92]
  Expert: layer_7[1.1-16.5% std=3.42]
  Expert: layer_8[1.2-10.2% std=2.63]
  Expert: layer_9[3.6-10.1% std=1.72]
  Expert: layer_10[1.2-15.1% std=3.74]
  Expert: layer_12[0.3-23.3% std=6.33]
  Expert: layer_13[1.1-15.1% std=3.99]
  Expert: layer_14[0.1-24.4% std=6.59]
  Expert: layer_15[0.2-38.3% std=8.82]
  Expert: layer_16[0.1-17.0% std=5.04]
  Expert: layer_18[0.3-25.1% std=5.92]
  Expert: layer_19[0.1-41.8% std=10.04]
  Expert: layer_20[0.1-25.7% std=6.05]
  Expert: layer_21[0.1-42.7% std=10.53]
Step      43 | Loss: 7.0937 | LR: 2.69e-05 | GradNorm: 0.91 | Tok/s: 2573 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.7% std=1.16] | layer_5[4.2-8.0% std=1.01] | layer_11[1.8-14.3% std=3.25] | layer_17[0.6-26.9% std=8.00] | layer_22[0.2-47.4% std=11.22]
  Expert: layer_1[4.8-8.8% std=0.99]
  Expert: layer_2[4.7-7.7% std=0.80]
  Expert: layer_3[1.9-14.0% std=2.82]
  Expert: layer_4[3.1-7.8% std=1.16]
  Expert: layer_6[2.5-11.1% std=1.93]
  Expert: layer_7[1.1-15.9% std=3.37]
  Expert: layer_8[1.3-9.9% std=2.57]
  Expert: layer_9[3.6-9.8% std=1.76]
  Expert: layer_10[1.2-14.5% std=3.63]
  Expert: layer_12[0.3-22.4% std=6.22]
  Expert: layer_13[1.1-15.2% std=3.90]
  Expert: layer_14[0.1-25.0% std=6.75]
  Expert: layer_15[0.2-38.3% std=8.80]
  Expert: layer_16[0.1-16.7% std=5.02]
  Expert: layer_18[0.3-25.9% std=6.05]
  Expert: layer_19[0.2-41.5% std=9.96]
  Expert: layer_20[0.2-25.4% std=5.92]
  Expert: layer_21[0.1-42.5% std=10.32]
Step      44 | Loss: 7.0165 | LR: 2.75e-05 | GradNorm: 0.86 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.6% std=1.15] | layer_5[4.2-8.1% std=0.97] | layer_11[1.8-14.3% std=3.17] | layer_17[0.7-28.2% std=8.16] | layer_22[0.2-47.5% std=11.22]
  Expert: layer_1[4.9-8.9% std=1.00]
  Expert: layer_2[4.6-8.1% std=0.85]
  Expert: layer_3[1.9-13.9% std=2.84]
  Expert: layer_4[3.3-7.8% std=1.12]
  Expert: layer_6[2.7-11.1% std=1.92]
  Expert: layer_7[1.1-15.5% std=3.38]
  Expert: layer_8[1.3-9.8% std=2.57]
  Expert: layer_9[3.5-10.0% std=1.89]
  Expert: layer_10[1.2-14.5% std=3.62]
  Expert: layer_12[0.3-21.6% std=6.17]
  Expert: layer_13[1.3-15.2% std=3.89]
  Expert: layer_14[0.2-26.1% std=6.89]
  Expert: layer_15[0.2-38.9% std=8.93]
  Expert: layer_16[0.1-16.6% std=5.02]
  Expert: layer_18[0.3-26.4% std=6.14]
  Expert: layer_19[0.3-41.3% std=9.84]
  Expert: layer_20[0.1-27.2% std=6.23]
  Expert: layer_21[0.1-41.6% std=10.08]
Step      45 | Loss: 6.9905 | LR: 2.81e-05 | GradNorm: 0.86 | Tok/s: 2611 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.6% std=1.16] | layer_5[4.2-8.2% std=0.92] | layer_11[1.7-14.4% std=3.11] | layer_17[0.7-29.6% std=8.17] | layer_22[0.1-47.7% std=11.23]
  Expert: layer_1[4.9-8.7% std=0.96]
  Expert: layer_2[4.7-8.2% std=0.89]
  Expert: layer_3[2.0-13.7% std=2.79]
  Expert: layer_4[3.4-7.9% std=1.12]
  Expert: layer_6[2.8-11.1% std=1.98]
  Expert: layer_7[1.1-15.2% std=3.27]
  Expert: layer_8[1.3-9.8% std=2.54]
  Expert: layer_9[3.4-10.2% std=1.89]
  Expert: layer_10[1.2-14.7% std=3.61]
  Expert: layer_12[0.3-20.8% std=6.04]
  Expert: layer_13[1.4-14.5% std=3.76]
  Expert: layer_14[0.1-26.8% std=6.92]
  Expert: layer_15[0.2-39.5% std=9.07]
  Expert: layer_16[0.1-16.9% std=4.97]
  Expert: layer_18[0.3-26.3% std=6.15]
  Expert: layer_19[0.4-40.8% std=9.63]
  Expert: layer_20[0.1-29.1% std=6.56]
  Expert: layer_21[0.2-41.5% std=10.01]
Step      46 | Loss: 6.9707 | LR: 2.88e-05 | GradNorm: 0.93 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.6% std=1.15] | layer_5[4.5-7.8% std=0.83] | layer_11[1.8-14.5% std=3.03] | layer_17[0.7-29.3% std=7.87] | layer_22[0.2-47.9% std=11.25]
  Expert: layer_1[4.9-8.6% std=0.97]
  Expert: layer_2[4.8-8.0% std=0.81]
  Expert: layer_3[2.2-13.8% std=2.79]
  Expert: layer_4[3.6-7.9% std=1.14]
  Expert: layer_6[2.8-11.1% std=2.01]
  Expert: layer_7[1.1-15.2% std=3.25]
  Expert: layer_8[1.4-10.0% std=2.54]
  Expert: layer_9[3.3-10.0% std=1.87]
  Expert: layer_10[1.3-15.0% std=3.65]
  Expert: layer_12[0.3-20.5% std=5.97]
  Expert: layer_13[1.5-13.6% std=3.63]
  Expert: layer_14[0.1-27.5% std=7.03]
  Expert: layer_15[0.3-39.7% std=9.08]
  Expert: layer_16[0.1-17.1% std=4.89]
  Expert: layer_18[0.3-26.5% std=6.21]
  Expert: layer_19[0.4-40.4% std=9.52]
  Expert: layer_20[0.1-29.9% std=6.76]
  Expert: layer_21[0.4-41.3% std=9.84]
Step      47 | Loss: 6.8675 | LR: 2.94e-05 | GradNorm: 0.82 | Tok/s: 2615 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.5% std=1.15] | layer_5[4.5-8.1% std=0.89] | layer_11[1.8-14.2% std=3.05] | layer_17[0.6-29.1% std=7.72] | layer_22[0.2-48.0% std=11.28]
  Expert: layer_1[4.9-8.6% std=0.90]
  Expert: layer_2[5.0-7.9% std=0.79]
  Expert: layer_3[2.1-13.6% std=2.74]
  Expert: layer_4[3.8-8.0% std=1.17]
  Expert: layer_6[2.7-11.2% std=2.04]
  Expert: layer_7[1.1-15.0% std=3.16]
  Expert: layer_8[1.4-10.3% std=2.57]
  Expert: layer_9[3.2-10.2% std=1.97]
  Expert: layer_10[1.2-16.0% std=3.75]
  Expert: layer_12[0.4-20.4% std=6.01]
  Expert: layer_13[1.6-13.9% std=3.68]
  Expert: layer_14[0.1-28.7% std=7.17]
  Expert: layer_15[0.3-40.3% std=9.23]
  Expert: layer_16[0.1-17.9% std=4.96]
  Expert: layer_18[0.4-26.9% std=6.31]
  Expert: layer_19[0.3-40.1% std=9.48]
  Expert: layer_20[0.1-31.1% std=7.07]
  Expert: layer_21[0.3-41.9% std=9.91]
Step      48 | Loss: 6.9020 | LR: 3.00e-05 | GradNorm: 0.80 | Tok/s: 2613 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.4% std=1.15] | layer_5[4.6-8.1% std=0.95] | layer_11[1.9-14.4% std=3.12] | layer_17[0.5-29.5% std=7.75] | layer_22[0.1-48.2% std=11.30]
  Expert: layer_1[5.1-8.6% std=0.87]
  Expert: layer_2[4.9-7.9% std=0.81]
  Expert: layer_3[2.2-13.7% std=2.73]
  Expert: layer_4[3.9-8.1% std=1.23]
  Expert: layer_6[2.6-11.1% std=2.06]
  Expert: layer_7[1.1-14.9% std=3.14]
  Expert: layer_8[1.2-10.5% std=2.65]
  Expert: layer_9[3.2-10.3% std=2.01]
  Expert: layer_10[1.2-16.6% std=3.85]
  Expert: layer_12[0.4-20.6% std=6.05]
  Expert: layer_13[1.4-14.1% std=3.80]
  Expert: layer_14[0.1-29.6% std=7.30]
  Expert: layer_15[0.2-40.5% std=9.26]
  Expert: layer_16[0.1-17.9% std=4.93]
  Expert: layer_18[0.4-26.5% std=6.27]
  Expert: layer_19[0.4-40.6% std=9.60]
  Expert: layer_20[0.1-31.2% std=7.18]
  Expert: layer_21[0.3-42.0% std=9.95]
Step      49 | Loss: 6.8222 | LR: 3.06e-05 | GradNorm: 0.88 | Tok/s: 2618 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.5% std=1.15] | layer_5[4.1-8.0% std=1.00] | layer_11[1.8-14.1% std=3.10] | layer_17[0.4-30.0% std=8.04] | layer_22[0.1-48.2% std=11.27]
  Expert: layer_1[5.1-8.8% std=0.92]
  Expert: layer_2[5.0-7.7% std=0.74]
  Expert: layer_3[2.2-13.7% std=2.76]
  Expert: layer_4[4.0-8.1% std=1.25]
  Expert: layer_6[2.5-11.1% std=2.10]
  Expert: layer_7[1.2-14.8% std=3.04]
  Expert: layer_8[1.3-11.2% std=2.72]
  Expert: layer_9[3.2-10.3% std=2.01]
  Expert: layer_10[1.4-16.7% std=3.91]
  Expert: layer_12[0.4-19.8% std=5.88]
  Expert: layer_13[1.5-14.2% std=3.69]
  Expert: layer_14[0.1-30.0% std=7.27]
  Expert: layer_15[0.2-40.8% std=9.33]
  Expert: layer_16[0.1-17.1% std=4.75]
  Expert: layer_18[0.4-24.9% std=5.92]
  Expert: layer_19[0.4-41.0% std=9.72]
  Expert: layer_20[0.4-29.8% std=6.97]
  Expert: layer_21[0.2-41.3% std=9.83]
Step      50 | Loss: 6.8139 | LR: 3.13e-05 | GradNorm: 0.83 | Tok/s: 2614 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.4% std=1.14] | layer_5[4.0-7.6% std=0.94] | layer_11[1.8-13.7% std=3.06] | layer_17[0.4-29.6% std=7.96] | layer_22[0.0-47.8% std=11.18]
  Expert: layer_1[5.1-8.7% std=0.89]
  Expert: layer_2[5.0-7.9% std=0.81]
  Expert: layer_3[2.2-13.9% std=2.76]
  Expert: layer_4[4.0-7.6% std=1.14]
  Expert: layer_6[2.4-11.2% std=2.15]
  Expert: layer_7[1.4-14.9% std=3.00]
  Expert: layer_8[1.2-11.5% std=2.73]
  Expert: layer_9[3.2-11.1% std=2.12]
  Expert: layer_10[1.5-17.0% std=3.86]
  Expert: layer_12[0.4-20.2% std=5.94]
  Expert: layer_13[1.4-14.5% std=3.71]
  Expert: layer_14[0.2-29.5% std=7.11]
  Expert: layer_15[0.2-41.8% std=9.56]
  Expert: layer_16[0.1-17.8% std=4.81]
  Expert: layer_18[0.4-23.6% std=5.73]
  Expert: layer_19[0.2-41.0% std=9.71]
  Expert: layer_20[0.2-28.9% std=6.91]
  Expert: layer_21[0.2-41.4% std=9.85]
Step      51 | Loss: 6.7405 | LR: 3.19e-05 | GradNorm: 0.75 | Tok/s: 2599 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.5% std=1.16] | layer_5[4.1-7.8% std=0.92] | layer_11[1.9-13.8% std=3.11] | layer_17[0.4-29.3% std=7.96] | layer_22[0.0-47.7% std=11.09]
  Expert: layer_1[5.1-8.5% std=0.86]
  Expert: layer_2[5.0-7.9% std=0.75]
  Expert: layer_3[2.1-13.9% std=2.80]
  Expert: layer_4[3.8-7.6% std=1.15]
  Expert: layer_6[2.3-11.1% std=2.17]
  Expert: layer_7[1.3-15.7% std=3.16]
  Expert: layer_8[1.2-11.8% std=2.79]
  Expert: layer_9[3.0-11.8% std=2.30]
  Expert: layer_10[1.5-17.4% std=3.94]
  Expert: layer_12[0.4-21.1% std=6.17]
  Expert: layer_13[1.5-15.0% std=3.78]
  Expert: layer_14[0.2-29.6% std=7.12]
  Expert: layer_15[0.2-42.5% std=9.70]
  Expert: layer_16[0.1-18.0% std=4.77]
  Expert: layer_18[0.4-22.8% std=5.74]
  Expert: layer_19[0.1-41.5% std=9.81]
  Expert: layer_20[0.2-30.4% std=7.35]
  Expert: layer_21[0.2-41.9% std=9.91]
Step      52 | Loss: 6.7028 | LR: 3.25e-05 | GradNorm: 0.73 | Tok/s: 2611 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.3-8.6% std=1.17] | layer_5[4.1-8.0% std=0.86] | layer_11[1.8-13.5% std=3.04] | layer_17[0.4-28.9% std=8.08] | layer_22[0.0-47.8% std=11.11]
  Expert: layer_1[5.2-8.7% std=0.88]
  Expert: layer_2[4.9-7.8% std=0.70]
  Expert: layer_3[2.2-14.0% std=2.83]
  Expert: layer_4[3.9-7.8% std=1.18]
  Expert: layer_6[2.3-10.9% std=2.15]
  Expert: layer_7[1.0-16.4% std=3.34]
  Expert: layer_8[1.0-12.2% std=2.92]
  Expert: layer_9[3.0-11.8% std=2.35]
  Expert: layer_10[1.4-17.3% std=3.99]
  Expert: layer_12[0.4-21.3% std=6.31]
  Expert: layer_13[1.2-15.5% std=3.88]
  Expert: layer_14[0.2-30.0% std=7.27]
  Expert: layer_15[0.2-42.9% std=9.80]
  Expert: layer_16[0.1-17.9% std=4.76]
  Expert: layer_18[0.4-22.0% std=5.68]
  Expert: layer_19[0.1-42.0% std=9.95]
  Expert: layer_20[0.2-31.0% std=7.54]
  Expert: layer_21[0.1-42.5% std=10.10]
Step      53 | Loss: 6.6437 | LR: 3.31e-05 | GradNorm: 0.74 | Tok/s: 2612 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.13] | layer_5[4.2-8.2% std=0.83] | layer_11[1.8-13.5% std=2.96] | layer_17[0.5-27.4% std=7.97] | layer_22[0.0-48.0% std=11.18]
  Expert: layer_1[5.1-8.4% std=0.84]
  Expert: layer_2[5.0-7.4% std=0.66]
  Expert: layer_3[2.3-14.2% std=2.78]
  Expert: layer_4[3.7-7.9% std=1.23]
  Expert: layer_6[2.3-11.4% std=2.17]
  Expert: layer_7[1.1-16.3% std=3.31]
  Expert: layer_8[1.2-11.8% std=2.84]
  Expert: layer_9[3.3-11.6% std=2.18]
  Expert: layer_10[1.5-16.8% std=3.86]
  Expert: layer_12[0.4-21.8% std=6.34]
  Expert: layer_13[1.5-15.4% std=3.78]
  Expert: layer_14[0.1-29.5% std=7.21]
  Expert: layer_15[0.2-43.0% std=9.82]
  Expert: layer_16[0.1-17.9% std=4.71]
  Expert: layer_18[0.4-22.8% std=5.78]
  Expert: layer_19[0.1-42.5% std=10.07]
  Expert: layer_20[0.2-30.9% std=7.51]
  Expert: layer_21[0.3-42.7% std=10.09]
Step      54 | Loss: 6.6420 | LR: 3.38e-05 | GradNorm: 0.74 | Tok/s: 2613 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.5% std=1.13] | layer_5[4.2-8.3% std=0.84] | layer_11[1.7-13.5% std=2.96] | layer_17[0.5-27.1% std=8.06] | layer_22[0.0-48.0% std=11.23]
  Expert: layer_1[5.2-8.5% std=0.87]
  Expert: layer_2[4.9-7.4% std=0.63]
  Expert: layer_3[2.4-14.0% std=2.77]
  Expert: layer_4[3.9-7.8% std=1.20]
  Expert: layer_6[2.2-11.6% std=2.22]
  Expert: layer_7[0.9-16.6% std=3.41]
  Expert: layer_8[1.1-12.4% std=3.04]
  Expert: layer_9[3.3-11.7% std=2.17]
  Expert: layer_10[1.4-17.1% std=3.96]
  Expert: layer_12[0.5-22.5% std=6.46]
  Expert: layer_13[1.3-15.7% std=3.89]
  Expert: layer_14[0.2-30.3% std=7.42]
  Expert: layer_15[0.3-43.2% std=9.86]
  Expert: layer_16[0.1-17.7% std=4.68]
  Expert: layer_18[0.4-24.4% std=6.00]
  Expert: layer_19[0.2-43.3% std=10.26]
  Expert: layer_20[0.2-31.9% std=7.70]
  Expert: layer_21[0.2-43.5% std=10.22]
Step      55 | Loss: 6.5541 | LR: 3.44e-05 | GradNorm: 0.69 | Tok/s: 2611 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.15] | layer_5[4.3-8.7% std=0.89] | layer_11[1.7-13.8% std=2.98] | layer_17[0.4-27.1% std=8.17] | layer_22[0.0-48.0% std=11.24]
  Expert: layer_1[5.1-8.6% std=0.88]
  Expert: layer_2[4.8-7.6% std=0.62]
  Expert: layer_3[2.4-13.7% std=2.66]
  Expert: layer_4[3.7-7.9% std=1.25]
  Expert: layer_6[2.3-12.1% std=2.27]
  Expert: layer_7[0.9-16.4% std=3.43]
  Expert: layer_8[1.2-12.9% std=3.07]
  Expert: layer_9[3.4-11.6% std=2.11]
  Expert: layer_10[1.4-17.7% std=4.05]
  Expert: layer_12[0.5-22.6% std=6.38]
  Expert: layer_13[1.3-15.4% std=3.92]
  Expert: layer_14[0.2-31.7% std=7.68]
  Expert: layer_15[0.3-43.9% std=10.01]
  Expert: layer_16[0.1-18.3% std=4.75]
  Expert: layer_18[0.3-25.5% std=6.12]
  Expert: layer_19[0.2-43.3% std=10.30]
  Expert: layer_20[0.1-32.5% std=7.87]
  Expert: layer_21[0.2-43.9% std=10.25]
Step      56 | Loss: 6.4675 | LR: 3.50e-05 | GradNorm: 0.73 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.4% std=1.14] | layer_5[4.3-8.6% std=0.82] | layer_11[1.5-13.5% std=3.03] | layer_17[0.4-28.0% std=8.31] | layer_22[0.0-48.1% std=11.25]
  Expert: layer_1[5.0-8.5% std=0.89]
  Expert: layer_2[4.8-7.5% std=0.63]
  Expert: layer_3[2.5-14.1% std=2.71]
  Expert: layer_4[3.7-8.0% std=1.19]
  Expert: layer_6[2.3-12.3% std=2.30]
  Expert: layer_7[0.9-16.3% std=3.40]
  Expert: layer_8[1.4-13.2% std=3.07]
  Expert: layer_9[3.5-11.9% std=2.07]
  Expert: layer_10[1.4-18.0% std=4.06]
  Expert: layer_12[0.4-22.8% std=6.46]
  Expert: layer_13[1.4-15.1% std=3.89]
  Expert: layer_14[0.2-32.2% std=7.73]
  Expert: layer_15[0.3-44.6% std=10.17]
  Expert: layer_16[0.1-18.7% std=4.85]
  Expert: layer_18[0.3-26.4% std=6.35]
  Expert: layer_19[0.2-43.7% std=10.37]
  Expert: layer_20[0.1-32.9% std=7.96]
  Expert: layer_21[0.2-44.3% std=10.30]
Step      57 | Loss: 6.4726 | LR: 3.56e-05 | GradNorm: 0.73 | Tok/s: 2612 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.14] | layer_5[4.4-8.7% std=0.83] | layer_11[1.5-14.1% std=3.09] | layer_17[0.3-28.7% std=8.35] | layer_22[0.0-48.0% std=11.27]
  Expert: layer_1[5.2-8.4% std=0.87]
  Expert: layer_2[4.7-7.7% std=0.67]
  Expert: layer_3[2.5-13.9% std=2.66]
  Expert: layer_4[3.8-7.6% std=1.12]
  Expert: layer_6[2.4-12.4% std=2.26]
  Expert: layer_7[1.0-16.5% std=3.42]
  Expert: layer_8[1.4-13.0% std=3.05]
  Expert: layer_9[3.4-12.3% std=2.19]
  Expert: layer_10[1.3-18.7% std=4.19]
  Expert: layer_12[0.4-23.1% std=6.47]
  Expert: layer_13[1.3-14.3% std=3.94]
  Expert: layer_14[0.2-32.4% std=7.79]
  Expert: layer_15[0.3-44.5% std=10.17]
  Expert: layer_16[0.1-19.2% std=4.95]
  Expert: layer_18[0.3-25.8% std=6.35]
  Expert: layer_19[0.2-44.0% std=10.41]
  Expert: layer_20[0.1-33.5% std=8.17]
  Expert: layer_21[0.2-44.2% std=10.31]
Step      58 | Loss: 6.4450 | LR: 3.63e-05 | GradNorm: 0.72 | Tok/s: 2614 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.12] | layer_5[4.6-8.7% std=0.78] | layer_11[1.5-14.5% std=3.15] | layer_17[0.4-27.4% std=8.05] | layer_22[0.0-48.0% std=11.22]
  Expert: layer_1[5.1-8.4% std=0.85]
  Expert: layer_2[4.6-7.8% std=0.67]
  Expert: layer_3[2.5-13.9% std=2.67]
  Expert: layer_4[3.9-7.5% std=1.08]
  Expert: layer_6[2.4-12.5% std=2.26]
  Expert: layer_7[1.1-16.6% std=3.39]
  Expert: layer_8[1.4-13.2% std=2.97]
  Expert: layer_9[3.5-12.3% std=2.18]
  Expert: layer_10[1.4-18.2% std=4.03]
  Expert: layer_12[0.4-23.0% std=6.42]
  Expert: layer_13[1.5-14.3% std=3.91]
  Expert: layer_14[0.2-31.9% std=7.62]
  Expert: layer_15[0.4-44.5% std=10.16]
  Expert: layer_16[0.1-19.5% std=5.02]
  Expert: layer_18[0.4-24.8% std=6.39]
  Expert: layer_19[0.1-44.2% std=10.37]
  Expert: layer_20[0.2-33.1% std=8.10]
  Expert: layer_21[0.2-44.0% std=10.24]
Step      59 | Loss: 6.4245 | LR: 3.69e-05 | GradNorm: 0.66 | Tok/s: 2613 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.4% std=1.13] | layer_5[4.8-8.7% std=0.79] | layer_11[1.5-14.4% std=3.19] | layer_17[0.4-28.1% std=7.84] | layer_22[0.0-48.2% std=11.27]
  Expert: layer_1[5.0-8.6% std=0.89]
  Expert: layer_2[4.6-7.7% std=0.63]
  Expert: layer_3[2.6-13.9% std=2.68]
  Expert: layer_4[3.9-7.7% std=1.02]
  Expert: layer_6[2.4-12.3% std=2.19]
  Expert: layer_7[1.0-16.5% std=3.37]
  Expert: layer_8[1.5-13.5% std=3.00]
  Expert: layer_9[3.4-12.9% std=2.36]
  Expert: layer_10[1.4-17.9% std=3.97]
  Expert: layer_12[0.4-23.6% std=6.64]
  Expert: layer_13[1.4-13.5% std=3.73]
  Expert: layer_14[0.2-32.3% std=7.67]
  Expert: layer_15[0.4-45.0% std=10.29]
  Expert: layer_16[0.1-20.6% std=5.26]
  Expert: layer_18[0.4-25.0% std=6.78]
  Expert: layer_19[0.1-45.0% std=10.55]
  Expert: layer_20[0.1-33.5% std=8.26]
  Expert: layer_21[0.2-44.5% std=10.36]
Step      60 | Loss: 6.3928 | LR: 3.75e-05 | GradNorm: 0.74 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.3% std=1.12] | layer_5[4.8-8.8% std=0.84] | layer_11[1.6-14.5% std=3.16] | layer_17[0.5-28.4% std=7.83] | layer_22[0.0-48.3% std=11.29]
  Expert: layer_1[4.8-8.8% std=0.98]
  Expert: layer_2[4.5-7.9% std=0.68]
  Expert: layer_3[2.7-13.8% std=2.68]
  Expert: layer_4[3.9-7.6% std=1.03]
  Expert: layer_6[2.4-12.2% std=2.14]
  Expert: layer_7[0.9-16.9% std=3.43]
  Expert: layer_8[1.4-13.2% std=2.95]
  Expert: layer_9[3.3-12.7% std=2.39]
  Expert: layer_10[1.4-17.9% std=4.03]
  Expert: layer_12[0.5-24.5% std=6.71]
  Expert: layer_13[1.4-13.9% std=3.70]
  Expert: layer_14[0.2-33.0% std=7.78]
  Expert: layer_15[0.4-45.4% std=10.37]
  Expert: layer_16[0.0-20.7% std=5.25]
  Expert: layer_18[0.4-25.2% std=6.77]
  Expert: layer_19[0.1-45.6% std=10.73]
  Expert: layer_20[0.2-34.0% std=8.39]
  Expert: layer_21[0.3-45.0% std=10.44]
Step      61 | Loss: 6.2775 | LR: 3.81e-05 | GradNorm: 0.94 | Tok/s: 2585 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.10] | layer_5[4.8-8.7% std=0.86] | layer_11[1.7-14.2% std=3.09] | layer_17[0.6-28.7% std=7.87] | layer_22[0.0-48.3% std=11.25]
  Expert: layer_1[4.8-8.7% std=0.96]
  Expert: layer_2[4.6-8.0% std=0.68]
  Expert: layer_3[2.8-14.0% std=2.69]
  Expert: layer_4[4.0-7.6% std=1.03]
  Expert: layer_6[2.3-12.1% std=2.15]
  Expert: layer_7[1.0-16.9% std=3.43]
  Expert: layer_8[1.5-12.4% std=2.73]
  Expert: layer_9[3.4-12.5% std=2.37]
  Expert: layer_10[1.5-17.5% std=3.91]
  Expert: layer_12[0.4-24.5% std=6.67]
  Expert: layer_13[1.5-14.2% std=3.68]
  Expert: layer_14[0.3-32.4% std=7.62]
  Expert: layer_15[0.3-45.5% std=10.41]
  Expert: layer_16[0.1-21.0% std=5.15]
  Expert: layer_18[0.4-26.0% std=6.68]
  Expert: layer_19[0.1-45.3% std=10.67]
  Expert: layer_20[0.1-33.9% std=8.42]
  Expert: layer_21[0.3-44.8% std=10.41]
Step      62 | Loss: 6.2937 | LR: 3.87e-05 | GradNorm: 1.10 | Tok/s: 2581 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.3% std=1.09] | layer_5[4.8-8.6% std=0.88] | layer_11[1.7-13.8% std=3.07] | layer_17[0.8-29.1% std=8.24] | layer_22[0.0-48.3% std=11.25]
  Expert: layer_1[4.7-8.7% std=0.97]
  Expert: layer_2[4.6-8.3% std=0.70]
  Expert: layer_3[2.8-13.7% std=2.62]
  Expert: layer_4[4.0-7.6% std=1.04]
  Expert: layer_6[2.2-12.0% std=2.15]
  Expert: layer_7[1.0-17.0% std=3.50]
  Expert: layer_8[1.5-12.4% std=2.71]
  Expert: layer_9[3.3-12.6% std=2.38]
  Expert: layer_10[1.5-17.1% std=3.90]
  Expert: layer_12[0.4-25.3% std=6.89]
  Expert: layer_13[1.5-15.6% std=3.84]
  Expert: layer_14[0.3-33.1% std=7.79]
  Expert: layer_15[0.3-46.0% std=10.54]
  Expert: layer_16[0.1-21.7% std=5.24]
  Expert: layer_18[0.2-27.7% std=7.10]
  Expert: layer_19[0.1-45.8% std=10.82]
  Expert: layer_20[0.1-35.0% std=8.70]
  Expert: layer_21[0.3-45.2% std=10.52]
Step      63 | Loss: 6.2633 | LR: 3.94e-05 | GradNorm: 0.69 | Tok/s: 2611 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.3% std=1.13] | layer_5[4.6-9.0% std=0.97] | layer_11[1.9-13.7% std=3.02] | layer_17[1.0-29.1% std=8.00] | layer_22[0.1-48.4% std=11.26]
  Expert: layer_1[4.7-8.9% std=1.00]
  Expert: layer_2[4.6-7.6% std=0.59]
  Expert: layer_3[2.8-13.6% std=2.54]
  Expert: layer_4[3.9-7.9% std=1.13]
  Expert: layer_6[2.3-11.9% std=2.13]
  Expert: layer_7[0.9-17.4% std=3.57]
  Expert: layer_8[1.4-12.3% std=2.76]
  Expert: layer_9[3.2-12.6% std=2.43]
  Expert: layer_10[1.5-17.2% std=4.04]
  Expert: layer_12[0.4-24.9% std=6.69]
  Expert: layer_13[1.4-14.7% std=3.74]
  Expert: layer_14[0.3-33.3% std=7.82]
  Expert: layer_15[0.3-45.8% std=10.48]
  Expert: layer_16[0.1-21.8% std=5.24]
  Expert: layer_18[0.2-27.1% std=6.85]
  Expert: layer_19[0.1-45.3% std=10.74]
  Expert: layer_20[0.1-35.7% std=8.88]
  Expert: layer_21[0.2-45.4% std=10.64]
Step      64 | Loss: 6.2737 | LR: 4.00e-05 | GradNorm: 0.84 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.11] | layer_5[4.6-8.9% std=0.95] | layer_11[2.1-14.0% std=3.01] | layer_17[1.2-27.8% std=7.87] | layer_22[0.1-48.2% std=11.20]
  Expert: layer_1[4.8-8.8% std=0.97]
  Expert: layer_2[4.7-7.5% std=0.56]
  Expert: layer_3[2.7-13.5% std=2.50]
  Expert: layer_4[3.8-7.8% std=1.11]
  Expert: layer_6[2.3-11.8% std=2.08]
  Expert: layer_7[1.0-17.0% std=3.46]
  Expert: layer_8[1.4-12.5% std=2.77]
  Expert: layer_9[3.6-12.4% std=2.33]
  Expert: layer_10[1.6-16.8% std=3.89]
  Expert: layer_12[0.4-24.5% std=6.52]
  Expert: layer_13[1.5-14.8% std=3.66]
  Expert: layer_14[0.3-33.3% std=7.77]
  Expert: layer_15[0.3-45.9% std=10.51]
  Expert: layer_16[0.1-21.6% std=5.11]
  Expert: layer_18[0.2-27.1% std=6.70]
  Expert: layer_19[0.1-45.0% std=10.60]
  Expert: layer_20[0.1-35.6% std=8.86]
  Expert: layer_21[0.3-45.2% std=10.64]
Step      65 | Loss: 6.2062 | LR: 4.06e-05 | GradNorm: 0.89 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.4% std=1.10] | layer_5[4.7-8.6% std=0.92] | layer_11[2.4-14.5% std=3.05] | layer_17[1.1-28.2% std=8.21] | layer_22[0.1-48.2% std=11.19]
  Expert: layer_1[4.8-8.8% std=0.95]
  Expert: layer_2[4.8-7.6% std=0.58]
  Expert: layer_3[2.6-13.4% std=2.49]
  Expert: layer_4[3.9-7.5% std=1.06]
  Expert: layer_6[2.4-11.7% std=2.07]
  Expert: layer_7[1.0-16.8% std=3.47]
  Expert: layer_8[1.5-12.6% std=2.77]
  Expert: layer_9[3.7-12.5% std=2.26]
  Expert: layer_10[1.6-16.6% std=3.83]
  Expert: layer_12[0.3-25.2% std=6.63]
  Expert: layer_13[1.5-16.0% std=3.82]
  Expert: layer_14[0.3-34.2% std=7.91]
  Expert: layer_15[0.3-46.3% std=10.60]
  Expert: layer_16[0.1-21.7% std=5.15]
  Expert: layer_18[0.2-27.6% std=6.88]
  Expert: layer_19[0.1-45.8% std=10.79]
  Expert: layer_20[0.1-36.5% std=9.03]
  Expert: layer_21[0.4-45.6% std=10.72]
Step      66 | Loss: 6.1382 | LR: 4.12e-05 | GradNorm: 0.65 | Tok/s: 2586 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.10] | layer_5[4.7-8.7% std=0.91] | layer_11[2.2-14.9% std=3.20] | layer_17[1.1-30.1% std=8.37] | layer_22[0.1-48.2% std=11.23]
  Expert: layer_1[4.8-8.8% std=0.95]
  Expert: layer_2[4.9-8.5% std=0.74]
  Expert: layer_3[2.6-13.4% std=2.48]
  Expert: layer_4[4.1-7.5% std=1.02]
  Expert: layer_6[2.3-11.7% std=2.09]
  Expert: layer_7[1.1-16.4% std=3.33]
  Expert: layer_8[1.5-12.6% std=2.69]
  Expert: layer_9[3.7-13.1% std=2.30]
  Expert: layer_10[1.7-16.8% std=3.74]
  Expert: layer_12[0.3-25.6% std=6.79]
  Expert: layer_13[1.6-15.3% std=3.69]
  Expert: layer_14[0.3-33.8% std=7.78]
  Expert: layer_15[0.3-46.9% std=10.75]
  Expert: layer_16[0.1-22.1% std=5.30]
  Expert: layer_18[0.3-28.1% std=6.94]
  Expert: layer_19[0.1-46.0% std=10.85]
  Expert: layer_20[0.1-36.5% std=9.09]
  Expert: layer_21[0.4-46.0% std=10.87]
Step      67 | Loss: 6.1043 | LR: 4.19e-05 | GradNorm: 0.68 | Tok/s: 2611 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.08] | layer_5[4.7-8.6% std=0.89] | layer_11[2.3-14.6% std=3.13] | layer_17[1.2-29.7% std=8.09] | layer_22[0.1-48.2% std=11.23]
  Expert: layer_1[4.9-8.6% std=0.92]
  Expert: layer_2[5.0-8.5% std=0.73]
  Expert: layer_3[2.7-13.3% std=2.49]
  Expert: layer_4[4.1-7.4% std=0.97]
  Expert: layer_6[2.3-11.7% std=2.08]
  Expert: layer_7[1.2-15.7% std=3.11]
  Expert: layer_8[1.6-12.9% std=2.66]
  Expert: layer_9[3.3-13.0% std=2.25]
  Expert: layer_10[1.9-16.4% std=3.54]
  Expert: layer_12[0.4-25.4% std=6.82]
  Expert: layer_13[1.7-14.8% std=3.58]
  Expert: layer_14[0.3-32.5% std=7.43]
  Expert: layer_15[0.3-46.8% std=10.70]
  Expert: layer_16[0.1-21.6% std=5.28]
  Expert: layer_18[0.4-28.0% std=6.89]
  Expert: layer_19[0.1-46.3% std=10.86]
  Expert: layer_20[0.2-35.7% std=8.98]
  Expert: layer_21[0.5-46.1% std=10.85]
Step      68 | Loss: 6.1569 | LR: 4.25e-05 | GradNorm: 0.72 | Tok/s: 2610 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.12] | layer_5[4.7-8.3% std=0.81] | layer_11[2.4-14.7% std=3.02] | layer_17[1.4-26.6% std=7.59] | layer_22[0.1-48.1% std=11.19]
  Expert: layer_1[4.9-8.9% std=0.94]
  Expert: layer_2[4.7-7.7% std=0.59]
  Expert: layer_3[2.9-13.1% std=2.46]
  Expert: layer_4[4.0-7.6% std=1.04]
  Expert: layer_6[2.2-11.6% std=2.05]
  Expert: layer_7[1.4-15.7% std=3.08]
  Expert: layer_8[1.5-12.9% std=2.66]
  Expert: layer_9[3.1-12.2% std=2.15]
  Expert: layer_10[1.9-15.7% std=3.42]
  Expert: layer_12[0.4-24.6% std=6.61]
  Expert: layer_13[1.7-14.6% std=3.54]
  Expert: layer_14[0.3-31.9% std=7.24]
  Expert: layer_15[0.3-46.3% std=10.58]
  Expert: layer_16[0.1-21.2% std=5.10]
  Expert: layer_18[0.4-26.0% std=6.47]
  Expert: layer_19[0.2-46.7% std=10.93]
  Expert: layer_20[0.2-35.9% std=8.84]
  Expert: layer_21[0.4-45.9% std=10.75]
Step      69 | Loss: 6.0585 | LR: 4.31e-05 | GradNorm: 0.60 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.13] | layer_5[4.8-8.3% std=0.81] | layer_11[2.3-15.1% std=3.12] | layer_17[1.3-25.5% std=7.42] | layer_22[0.1-48.3% std=11.21]
  Expert: layer_1[4.9-8.8% std=0.93]
  Expert: layer_2[4.8-7.4% std=0.55]
  Expert: layer_3[2.7-13.0% std=2.50]
  Expert: layer_4[3.7-7.5% std=1.09]
  Expert: layer_6[2.2-11.7% std=2.04]
  Expert: layer_7[1.4-16.1% std=3.20]
  Expert: layer_8[1.5-13.2% std=2.71]
  Expert: layer_9[3.0-12.4% std=2.24]
  Expert: layer_10[1.9-16.2% std=3.51]
  Expert: layer_12[0.4-26.0% std=6.90]
  Expert: layer_13[1.8-14.0% std=3.38]
  Expert: layer_14[0.3-32.6% std=7.42]
  Expert: layer_15[0.3-46.6% std=10.61]
  Expert: layer_16[0.1-22.3% std=5.30]
  Expert: layer_18[0.5-26.3% std=6.71]
  Expert: layer_19[0.2-47.1% std=11.06]
  Expert: layer_20[0.2-37.8% std=9.13]
  Expert: layer_21[0.5-45.9% std=10.74]
Step      70 | Loss: 6.0446 | LR: 4.38e-05 | GradNorm: 0.68 | Tok/s: 2556 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.13] | layer_5[4.8-8.3% std=0.78] | layer_11[2.0-15.3% std=3.27] | layer_17[1.3-25.5% std=7.47] | layer_22[0.2-48.6% std=11.28]
  Expert: layer_1[5.0-8.8% std=0.92]
  Expert: layer_2[4.8-7.2% std=0.57]
  Expert: layer_3[2.8-12.7% std=2.46]
  Expert: layer_4[3.8-7.6% std=1.11]
  Expert: layer_6[2.3-11.8% std=2.01]
  Expert: layer_7[1.5-15.7% std=3.15]
  Expert: layer_8[1.5-13.1% std=2.76]
  Expert: layer_9[3.3-11.9% std=2.17]
  Expert: layer_10[1.8-16.7% std=3.66]
  Expert: layer_12[0.4-27.0% std=7.07]
  Expert: layer_13[1.8-13.5% std=3.33]
  Expert: layer_14[0.3-34.1% std=7.83]
  Expert: layer_15[0.3-47.1% std=10.73]
  Expert: layer_16[0.0-22.4% std=5.46]
  Expert: layer_18[0.4-27.1% std=7.02]
  Expert: layer_19[0.2-47.4% std=11.20]
  Expert: layer_20[0.1-39.0% std=9.40]
  Expert: layer_21[0.4-46.5% std=10.90]
Step      71 | Loss: 6.0470 | LR: 4.44e-05 | GradNorm: 0.69 | Tok/s: 2574 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.3% std=1.12] | layer_5[4.7-8.3% std=0.81] | layer_11[2.1-15.7% std=3.30] | layer_17[1.5-25.8% std=7.44] | layer_22[0.1-48.3% std=11.24]
  Expert: layer_1[4.9-8.7% std=0.92]
  Expert: layer_2[4.9-7.7% std=0.68]
  Expert: layer_3[2.7-13.1% std=2.51]
  Expert: layer_4[3.7-7.7% std=1.15]
  Expert: layer_6[2.4-12.0% std=2.03]
  Expert: layer_7[1.6-15.4% std=3.05]
  Expert: layer_8[1.6-12.6% std=2.63]
  Expert: layer_9[3.3-11.7% std=2.12]
  Expert: layer_10[1.9-16.1% std=3.47]
  Expert: layer_12[0.4-26.9% std=6.88]
  Expert: layer_13[1.7-14.2% std=3.38]
  Expert: layer_14[0.4-33.8% std=7.82]
  Expert: layer_15[0.3-47.1% std=10.73]
  Expert: layer_16[0.1-21.9% std=5.42]
  Expert: layer_18[0.4-28.0% std=6.95]
  Expert: layer_19[0.2-47.4% std=11.10]
  Expert: layer_20[0.2-38.4% std=9.20]
  Expert: layer_21[0.3-46.5% std=10.89]
Step      72 | Loss: 6.0169 | LR: 4.50e-05 | GradNorm: 0.61 | Tok/s: 2598 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.2% std=1.11] | layer_5[4.8-8.5% std=0.84] | layer_11[2.0-16.1% std=3.43] | layer_17[1.8-25.6% std=7.41] | layer_22[0.1-48.2% std=11.22]
  Expert: layer_1[4.9-8.8% std=0.93]
  Expert: layer_2[4.9-7.4% std=0.59]
  Expert: layer_3[2.7-13.2% std=2.49]
  Expert: layer_4[3.9-7.6% std=1.10]
  Expert: layer_6[2.6-12.0% std=1.97]
  Expert: layer_7[1.6-15.8% std=3.11]
  Expert: layer_8[1.7-12.5% std=2.60]
  Expert: layer_9[3.4-10.9% std=1.97]
  Expert: layer_10[2.0-16.5% std=3.53]
  Expert: layer_12[0.4-26.8% std=6.83]
  Expert: layer_13[1.8-14.7% std=3.41]
  Expert: layer_14[0.4-34.2% std=7.92]
  Expert: layer_15[0.3-47.1% std=10.74]
  Expert: layer_16[0.1-22.1% std=5.52]
  Expert: layer_18[0.3-27.7% std=6.92]
  Expert: layer_19[0.2-47.3% std=11.06]
  Expert: layer_20[0.1-38.7% std=9.28]
  Expert: layer_21[0.3-46.0% std=10.74]
Step      73 | Loss: 5.9792 | LR: 4.56e-05 | GradNorm: 0.67 | Tok/s: 2602 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.09] | layer_5[4.7-8.7% std=0.87] | layer_11[1.9-16.5% std=3.56] | layer_17[1.8-26.4% std=7.51] | layer_22[0.1-48.2% std=11.23]
  Expert: layer_1[4.8-8.6% std=0.95]
  Expert: layer_2[5.1-7.4% std=0.55]
  Expert: layer_3[2.6-13.5% std=2.53]
  Expert: layer_4[4.2-7.6% std=1.05]
  Expert: layer_6[2.5-12.0% std=1.97]
  Expert: layer_7[1.6-15.7% std=3.06]
  Expert: layer_8[1.7-13.1% std=2.68]
  Expert: layer_9[3.3-10.7% std=1.94]
  Expert: layer_10[1.9-17.3% std=3.71]
  Expert: layer_12[0.4-26.6% std=6.82]
  Expert: layer_13[1.7-14.5% std=3.38]
  Expert: layer_14[0.3-35.3% std=8.17]
  Expert: layer_15[0.3-47.3% std=10.75]
  Expert: layer_16[0.1-22.8% std=5.66]
  Expert: layer_18[0.4-27.4% std=7.07]
  Expert: layer_19[0.2-47.4% std=11.10]
  Expert: layer_20[0.1-40.0% std=9.57]
  Expert: layer_21[0.3-45.8% std=10.70]
Step      74 | Loss: 6.0053 | LR: 4.62e-05 | GradNorm: 0.66 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.3% std=1.11] | layer_5[4.7-8.5% std=0.85] | layer_11[1.9-16.7% std=3.59] | layer_17[1.8-27.8% std=7.79] | layer_22[0.2-48.1% std=11.20]
  Expert: layer_1[4.8-8.9% std=1.07]
  Expert: layer_2[5.0-7.3% std=0.55]
  Expert: layer_3[2.7-13.4% std=2.49]
  Expert: layer_4[4.3-8.0% std=1.09]
  Expert: layer_6[2.4-12.3% std=2.08]
  Expert: layer_7[1.5-16.0% std=3.12]
  Expert: layer_8[1.6-13.5% std=2.79]
  Expert: layer_9[3.0-10.9% std=1.98]
  Expert: layer_10[1.7-18.2% std=3.93]
  Expert: layer_12[0.5-26.9% std=6.95]
  Expert: layer_13[1.7-14.6% std=3.36]
  Expert: layer_14[0.3-36.2% std=8.39]
  Expert: layer_15[0.2-47.3% std=10.77]
  Expert: layer_16[0.0-22.1% std=5.45]
  Expert: layer_18[0.3-27.0% std=6.85]
  Expert: layer_19[0.2-47.3% std=11.08]
  Expert: layer_20[0.1-40.8% std=9.74]
  Expert: layer_21[0.2-46.1% std=10.76]
Step      75 | Loss: 5.9785 | LR: 4.69e-05 | GradNorm: 0.74 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.08] | layer_5[4.4-8.2% std=0.84] | layer_11[1.9-16.5% std=3.54] | layer_17[1.7-26.9% std=7.49] | layer_22[0.2-48.2% std=11.19]
  Expert: layer_1[4.8-8.7% std=1.00]
  Expert: layer_2[5.1-7.4% std=0.49]
  Expert: layer_3[2.8-13.2% std=2.44]
  Expert: layer_4[4.2-7.9% std=1.07]
  Expert: layer_6[2.6-12.1% std=2.05]
  Expert: layer_7[1.6-15.4% std=2.92]
  Expert: layer_8[1.7-13.5% std=2.75]
  Expert: layer_9[3.0-10.9% std=1.89]
  Expert: layer_10[1.6-18.2% std=3.89]
  Expert: layer_12[0.4-25.6% std=6.66]
  Expert: layer_13[1.7-14.4% std=3.30]
  Expert: layer_14[0.3-35.6% std=8.21]
  Expert: layer_15[0.2-47.3% std=10.77]
  Expert: layer_16[0.1-20.8% std=5.20]
  Expert: layer_18[0.5-25.6% std=6.43]
  Expert: layer_19[0.2-47.2% std=10.96]
  Expert: layer_20[0.1-39.4% std=9.54]
  Expert: layer_21[0.3-46.1% std=10.76]
Step      76 | Loss: 5.9124 | LR: 4.75e-05 | GradNorm: 0.60 | Tok/s: 2590 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.3% std=1.10] | layer_5[4.4-8.2% std=0.84] | layer_11[1.7-16.5% std=3.55] | layer_17[1.7-26.3% std=7.34] | layer_22[0.2-48.3% std=11.25]
  Expert: layer_1[4.9-8.8% std=1.03]
  Expert: layer_2[5.1-7.3% std=0.54]
  Expert: layer_3[2.8-13.3% std=2.45]
  Expert: layer_4[4.2-7.9% std=1.11]
  Expert: layer_6[2.5-12.0% std=2.05]
  Expert: layer_7[1.5-15.7% std=3.03]
  Expert: layer_8[1.8-13.8% std=2.72]
  Expert: layer_9[3.0-11.1% std=1.87]
  Expert: layer_10[1.6-18.5% std=3.97]
  Expert: layer_12[0.4-25.5% std=6.61]
  Expert: layer_13[1.8-14.1% std=3.27]
  Expert: layer_14[0.3-35.1% std=8.07]
  Expert: layer_15[0.2-47.1% std=10.73]
  Expert: layer_16[0.1-22.3% std=5.47]
  Expert: layer_18[0.6-24.2% std=6.28]
  Expert: layer_19[0.2-47.4% std=11.03]
  Expert: layer_20[0.1-39.5% std=9.67]
  Expert: layer_21[0.4-46.1% std=10.75]
Step      77 | Loss: 5.8553 | LR: 4.81e-05 | GradNorm: 0.71 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.3% std=1.08] | layer_5[4.5-7.8% std=0.78] | layer_11[1.7-16.7% std=3.53] | layer_17[1.8-24.1% std=6.92] | layer_22[0.2-48.3% std=11.30]
  Expert: layer_1[4.9-8.8% std=1.01]
  Expert: layer_2[5.2-7.4% std=0.56]
  Expert: layer_3[2.8-13.3% std=2.47]
  Expert: layer_4[4.1-7.4% std=1.06]
  Expert: layer_6[2.3-12.0% std=2.05]
  Expert: layer_7[1.6-15.7% std=3.05]
  Expert: layer_8[1.8-13.5% std=2.60]
  Expert: layer_9[3.2-11.2% std=1.77]
  Expert: layer_10[1.7-18.3% std=3.85]
  Expert: layer_12[0.4-25.6% std=6.56]
  Expert: layer_13[1.9-14.1% std=3.25]
  Expert: layer_14[0.3-34.0% std=7.78]
  Expert: layer_15[0.3-46.8% std=10.65]
  Expert: layer_16[0.1-23.1% std=5.54]
  Expert: layer_18[0.7-23.4% std=6.01]
  Expert: layer_19[0.3-47.2% std=10.93]
  Expert: layer_20[0.2-39.1% std=9.48]
  Expert: layer_21[0.4-45.7% std=10.62]
Step      78 | Loss: 5.7697 | LR: 4.87e-05 | GradNorm: 0.69 | Tok/s: 2601 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.4% std=1.08] | layer_5[4.3-8.0% std=0.84] | layer_11[1.9-17.7% std=3.64] | layer_17[1.7-23.2% std=6.60] | layer_22[0.3-48.4% std=11.31]
  Expert: layer_1[4.8-8.8% std=0.99]
  Expert: layer_2[5.3-7.6% std=0.52]
  Expert: layer_3[2.6-13.0% std=2.40]
  Expert: layer_4[4.1-7.3% std=1.07]
  Expert: layer_6[2.4-12.3% std=2.07]
  Expert: layer_7[1.7-15.6% std=3.06]
  Expert: layer_8[1.9-13.5% std=2.60]
  Expert: layer_9[3.3-11.4% std=1.79]
  Expert: layer_10[1.8-18.8% std=3.96]
  Expert: layer_12[0.4-24.7% std=6.34]
  Expert: layer_13[2.0-14.0% std=3.19]
  Expert: layer_14[0.3-33.9% std=7.72]
  Expert: layer_15[0.4-46.6% std=10.58]
  Expert: layer_16[0.1-22.2% std=5.28]
  Expert: layer_18[0.8-22.5% std=5.80]
  Expert: layer_19[0.3-46.6% std=10.75]
  Expert: layer_20[0.4-38.1% std=9.22]
  Expert: layer_21[0.5-44.6% std=10.34]
Step      79 | Loss: 5.8498 | LR: 4.94e-05 | GradNorm: 0.61 | Tok/s: 2600 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.3% std=1.11] | layer_5[4.4-7.9% std=0.82] | layer_11[1.7-19.5% std=4.02] | layer_17[1.7-24.0% std=6.76] | layer_22[0.3-48.8% std=11.37]
  Expert: layer_1[4.8-8.8% std=0.98]
  Expert: layer_2[5.5-7.5% std=0.43]
  Expert: layer_3[2.5-13.5% std=2.49]
  Expert: layer_4[4.1-7.3% std=1.05]
  Expert: layer_6[2.4-12.4% std=2.12]
  Expert: layer_7[1.3-16.8% std=3.36]
  Expert: layer_8[1.7-13.5% std=2.69]
  Expert: layer_9[3.3-11.6% std=1.87]
  Expert: layer_10[1.6-20.1% std=4.28]
  Expert: layer_12[0.5-24.5% std=6.41]
  Expert: layer_13[1.8-14.7% std=3.38]
  Expert: layer_14[0.3-36.1% std=8.20]
  Expert: layer_15[0.3-46.9% std=10.67]
  Expert: layer_16[0.1-22.8% std=5.35]
  Expert: layer_18[0.6-23.4% std=6.11]
  Expert: layer_19[0.3-47.3% std=10.94]
  Expert: layer_20[0.2-39.5% std=9.45]
  Expert: layer_21[0.5-45.5% std=10.55]
Step      80 | Loss: 5.7402 | LR: 5.00e-05 | GradNorm: 0.70 | Tok/s: 2598 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.4% std=1.11] | layer_5[4.3-7.9% std=0.84] | layer_11[1.7-19.7% std=4.07] | layer_17[1.6-25.8% std=7.17] | layer_22[0.3-48.8% std=11.36]
  Expert: layer_1[4.9-8.9% std=1.00]
  Expert: layer_2[5.6-7.5% std=0.44]
  Expert: layer_3[2.5-13.9% std=2.55]
  Expert: layer_4[4.2-7.2% std=1.02]
  Expert: layer_6[2.3-12.4% std=2.12]
  Expert: layer_7[1.0-17.1% std=3.44]
  Expert: layer_8[1.8-13.3% std=2.65]
  Expert: layer_9[3.2-11.7% std=1.89]
  Expert: layer_10[1.6-20.1% std=4.17]
  Expert: layer_12[0.4-24.8% std=6.57]
  Expert: layer_13[1.9-15.4% std=3.46]
  Expert: layer_14[0.3-36.6% std=8.31]
  Expert: layer_15[0.3-47.1% std=10.72]
  Expert: layer_16[0.1-23.3% std=5.53]
  Expert: layer_18[0.7-25.8% std=6.56]
  Expert: layer_19[0.3-47.6% std=11.00]
  Expert: layer_20[0.3-40.0% std=9.52]
  Expert: layer_21[0.5-45.6% std=10.59]
Step      81 | Loss: 5.7817 | LR: 5.06e-05 | GradNorm: 0.59 | Tok/s: 2587 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.4% std=1.09] | layer_5[4.4-7.8% std=0.77] | layer_11[1.6-19.3% std=3.94] | layer_17[1.7-26.9% std=7.06] | layer_22[0.3-48.4% std=11.27]
  Expert: layer_1[4.8-8.7% std=0.96]
  Expert: layer_2[5.6-7.4% std=0.43]
  Expert: layer_3[2.7-14.1% std=2.57]
  Expert: layer_4[4.4-7.4% std=1.00]
  Expert: layer_6[2.5-12.4% std=2.09]
  Expert: layer_7[1.2-17.3% std=3.47]
  Expert: layer_8[1.6-13.3% std=2.63]
  Expert: layer_9[3.3-11.7% std=1.84]
  Expert: layer_10[1.6-19.5% std=4.02]
  Expert: layer_12[0.5-24.2% std=6.42]
  Expert: layer_13[2.0-15.3% std=3.36]
  Expert: layer_14[0.3-35.9% std=8.13]
  Expert: layer_15[0.4-46.7% std=10.61]
  Expert: layer_16[0.1-23.2% std=5.42]
  Expert: layer_18[0.6-26.3% std=6.49]
  Expert: layer_19[0.3-47.2% std=10.91]
  Expert: layer_20[0.4-39.1% std=9.29]
  Expert: layer_21[0.6-44.9% std=10.48]
Step      82 | Loss: 5.7385 | LR: 5.12e-05 | GradNorm: 0.63 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.08] | layer_5[4.6-8.0% std=0.71] | layer_11[1.9-18.3% std=3.76] | layer_17[1.7-26.0% std=6.63] | layer_22[0.3-48.3% std=11.20]
  Expert: layer_1[5.0-8.7% std=0.94]
  Expert: layer_2[5.4-7.1% std=0.42]
  Expert: layer_3[2.5-13.6% std=2.51]
  Expert: layer_4[4.2-7.3% std=1.01]
  Expert: layer_6[2.5-12.6% std=2.12]
  Expert: layer_7[1.2-17.4% std=3.45]
  Expert: layer_8[1.7-13.5% std=2.63]
  Expert: layer_9[3.2-11.3% std=1.82]
  Expert: layer_10[1.8-18.7% std=3.81]
  Expert: layer_12[0.5-24.0% std=6.28]
  Expert: layer_13[2.0-14.7% std=3.18]
  Expert: layer_14[0.3-35.4% std=7.97]
  Expert: layer_15[0.5-46.7% std=10.59]
  Expert: layer_16[0.1-22.0% std=5.11]
  Expert: layer_18[0.8-25.4% std=6.16]
  Expert: layer_19[0.3-46.8% std=10.84]
  Expert: layer_20[0.5-38.1% std=9.04]
  Expert: layer_21[0.7-44.3% std=10.33]
Step      83 | Loss: 5.7547 | LR: 5.19e-05 | GradNorm: 0.62 | Tok/s: 2598 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.3% std=1.10] | layer_5[4.5-7.8% std=0.66] | layer_11[1.9-17.1% std=3.51] | layer_17[1.7-25.7% std=6.67] | layer_22[0.3-48.4% std=11.21]
  Expert: layer_1[4.9-8.7% std=0.92]
  Expert: layer_2[5.6-6.8% std=0.33]
  Expert: layer_3[2.6-14.3% std=2.68]
  Expert: layer_4[4.3-7.3% std=1.01]
  Expert: layer_6[2.6-12.5% std=2.09]
  Expert: layer_7[1.1-17.9% std=3.54]
  Expert: layer_8[1.6-13.9% std=2.71]
  Expert: layer_9[3.2-10.8% std=1.73]
  Expert: layer_10[1.6-18.3% std=3.80]
  Expert: layer_12[0.5-24.1% std=6.30]
  Expert: layer_13[1.8-14.1% std=3.12]
  Expert: layer_14[0.4-35.5% std=8.01]
  Expert: layer_15[0.3-46.6% std=10.57]
  Expert: layer_16[0.1-20.8% std=4.84]
  Expert: layer_18[0.7-24.8% std=6.09]
  Expert: layer_19[0.3-47.3% std=10.96]
  Expert: layer_20[0.4-38.8% std=9.20]
  Expert: layer_21[0.6-45.0% std=10.48]
Step      84 | Loss: 5.7473 | LR: 5.25e-05 | GradNorm: 0.59 | Tok/s: 2597 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.2% std=1.07] | layer_5[4.5-8.0% std=0.70] | layer_11[1.9-15.7% std=3.28] | layer_17[1.5-25.0% std=6.60] | layer_22[0.3-48.3% std=11.16]
  Expert: layer_1[4.9-8.6% std=0.91]
  Expert: layer_2[5.5-6.9% std=0.34]
  Expert: layer_3[2.6-14.6% std=2.74]
  Expert: layer_4[4.2-7.5% std=0.98]
  Expert: layer_6[2.6-12.5% std=2.10]
  Expert: layer_7[1.2-17.9% std=3.53]
  Expert: layer_8[1.6-13.8% std=2.68]
  Expert: layer_9[3.2-10.8% std=1.71]
  Expert: layer_10[1.6-18.4% std=3.82]
  Expert: layer_12[0.5-24.1% std=6.35]
  Expert: layer_13[1.7-13.7% std=3.04]
  Expert: layer_14[0.4-35.8% std=8.04]
  Expert: layer_15[0.3-46.9% std=10.64]
  Expert: layer_16[0.1-20.3% std=4.81]
  Expert: layer_18[1.0-24.6% std=6.04]
  Expert: layer_19[0.4-47.5% std=10.94]
  Expert: layer_20[0.5-39.6% std=9.33]
  Expert: layer_21[0.6-45.7% std=10.62]
Step      85 | Loss: 5.6770 | LR: 5.31e-05 | GradNorm: 0.59 | Tok/s: 2601 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.4-8.2% std=1.06] | layer_5[4.5-8.4% std=0.78] | layer_11[1.8-14.9% std=3.18] | layer_17[1.5-25.3% std=6.57] | layer_22[0.3-48.0% std=11.11]
  Expert: layer_1[4.8-8.5% std=0.91]
  Expert: layer_2[5.5-6.7% std=0.32]
  Expert: layer_3[2.7-14.4% std=2.70]
  Expert: layer_4[4.2-7.5% std=0.96]
  Expert: layer_6[2.8-12.3% std=2.08]
  Expert: layer_7[1.1-17.8% std=3.55]
  Expert: layer_8[1.6-13.7% std=2.66]
  Expert: layer_9[3.4-11.0% std=1.69]
  Expert: layer_10[1.5-18.3% std=3.83]
  Expert: layer_12[0.5-24.3% std=6.42]
  Expert: layer_13[1.8-14.6% std=3.19]
  Expert: layer_14[0.4-36.3% std=8.19]
  Expert: layer_15[0.4-46.9% std=10.65]
  Expert: layer_16[0.1-20.8% std=4.94]
  Expert: layer_18[0.9-24.4% std=6.08]
  Expert: layer_19[0.4-47.4% std=10.88]
  Expert: layer_20[0.8-39.7% std=9.36]
  Expert: layer_21[0.7-45.9% std=10.71]
Step      86 | Loss: 5.6605 | LR: 5.38e-05 | GradNorm: 0.53 | Tok/s: 2590 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.08] | layer_5[4.4-8.3% std=0.75] | layer_11[2.0-14.5% std=3.04] | layer_17[1.6-24.2% std=6.19] | layer_22[0.3-47.7% std=11.01]
  Expert: layer_1[4.9-8.6% std=0.91]
  Expert: layer_2[5.6-6.7% std=0.31]
  Expert: layer_3[2.5-13.7% std=2.58]
  Expert: layer_4[4.2-7.4% std=0.97]
  Expert: layer_6[2.8-12.1% std=1.97]
  Expert: layer_7[1.1-17.6% std=3.47]
  Expert: layer_8[1.7-13.3% std=2.58]
  Expert: layer_9[3.6-10.8% std=1.64]
  Expert: layer_10[1.6-17.8% std=3.77]
  Expert: layer_12[0.5-24.1% std=6.38]
  Expert: layer_13[1.8-13.9% std=3.10]
  Expert: layer_14[0.4-35.7% std=8.08]
  Expert: layer_15[0.3-46.5% std=10.54]
  Expert: layer_16[0.1-19.3% std=4.63]
  Expert: layer_18[1.1-23.2% std=5.58]
  Expert: layer_19[0.4-46.4% std=10.62]
  Expert: layer_20[1.0-37.8% std=8.90]
  Expert: layer_21[0.7-44.4% std=10.36]
Step      87 | Loss: 5.6172 | LR: 5.44e-05 | GradNorm: 0.55 | Tok/s: 2610 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.1% std=1.02] | layer_5[4.6-8.0% std=0.66] | layer_11[2.3-14.8% std=3.09] | layer_17[1.7-23.1% std=6.04] | layer_22[0.4-47.6% std=10.98]
  Expert: layer_1[5.0-8.5% std=0.88]
  Expert: layer_2[5.5-6.6% std=0.33]
  Expert: layer_3[2.4-13.6% std=2.54]
  Expert: layer_4[4.3-7.4% std=1.01]
  Expert: layer_6[2.7-12.3% std=2.03]
  Expert: layer_7[1.1-16.7% std=3.29]
  Expert: layer_8[1.8-13.8% std=2.63]
  Expert: layer_9[3.6-11.3% std=1.73]
  Expert: layer_10[1.8-17.9% std=3.72]
  Expert: layer_12[0.4-24.0% std=6.40]
  Expert: layer_13[1.9-14.2% std=3.09]
  Expert: layer_14[0.4-35.4% std=8.01]
  Expert: layer_15[0.4-46.6% std=10.58]
  Expert: layer_16[0.2-19.9% std=4.67]
  Expert: layer_18[1.5-24.2% std=5.89]
  Expert: layer_19[0.4-46.6% std=10.65]
  Expert: layer_20[0.9-37.7% std=8.95]
  Expert: layer_21[0.7-44.0% std=10.22]
Step      88 | Loss: 5.6328 | LR: 5.50e-05 | GradNorm: 0.63 | Tok/s: 2610 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.07] | layer_5[4.3-8.4% std=0.77] | layer_11[2.0-15.5% std=3.33] | layer_17[1.9-22.5% std=6.20] | layer_22[0.5-47.6% std=10.99]
  Expert: layer_1[5.0-8.7% std=0.83]
  Expert: layer_2[5.5-6.6% std=0.28]
  Expert: layer_3[2.4-13.6% std=2.56]
  Expert: layer_4[4.3-7.4% std=1.10]
  Expert: layer_6[2.7-12.1% std=2.03]
  Expert: layer_7[1.2-16.8% std=3.28]
  Expert: layer_8[1.7-14.1% std=2.76]
  Expert: layer_9[3.3-12.0% std=1.94]
  Expert: layer_10[1.7-18.8% std=3.98]
  Expert: layer_12[0.4-23.4% std=6.41]
  Expert: layer_13[1.9-14.6% std=3.25]
  Expert: layer_14[0.4-35.7% std=8.07]
  Expert: layer_15[0.3-46.7% std=10.59]
  Expert: layer_16[0.2-21.7% std=4.99]
  Expert: layer_18[1.1-25.1% std=6.30]
  Expert: layer_19[0.4-46.9% std=10.74]
  Expert: layer_20[0.7-38.5% std=9.13]
  Expert: layer_21[0.7-44.0% std=10.18]
Step      89 | Loss: 5.6130 | LR: 5.56e-05 | GradNorm: 0.55 | Tok/s: 2611 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.2% std=1.04] | layer_5[4.4-8.1% std=0.71] | layer_11[2.2-15.4% std=3.22] | layer_17[2.0-21.6% std=5.96] | layer_22[0.7-47.5% std=10.99]
  Expert: layer_1[5.0-8.6% std=0.79]
  Expert: layer_2[5.7-6.7% std=0.26]
  Expert: layer_3[2.3-13.3% std=2.47]
  Expert: layer_4[4.2-7.4% std=1.10]
  Expert: layer_6[2.8-11.9% std=1.90]
  Expert: layer_7[1.3-15.5% std=3.00]
  Expert: layer_8[1.9-14.0% std=2.71]
  Expert: layer_9[3.1-11.9% std=1.89]
  Expert: layer_10[1.9-18.2% std=3.82]
  Expert: layer_12[0.4-22.7% std=6.13]
  Expert: layer_13[1.9-14.1% std=3.19]
  Expert: layer_14[0.3-35.1% std=7.91]
  Expert: layer_15[0.4-46.4% std=10.50]
  Expert: layer_16[0.2-20.8% std=4.78]
  Expert: layer_18[1.3-24.5% std=6.12]
  Expert: layer_19[0.3-46.4% std=10.60]
  Expert: layer_20[0.7-36.6% std=8.66]
  Expert: layer_21[0.7-42.2% std=9.73]
Step      90 | Loss: 5.5697 | LR: 5.62e-05 | GradNorm: 0.58 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.04] | layer_5[4.3-8.1% std=0.71] | layer_11[2.7-15.4% std=3.20] | layer_17[2.0-21.2% std=5.73] | layer_22[0.6-47.4% std=11.02]
  Expert: layer_1[5.1-8.7% std=0.83]
  Expert: layer_2[5.7-6.7% std=0.30]
  Expert: layer_3[2.1-13.2% std=2.46]
  Expert: layer_4[4.1-7.4% std=1.12]
  Expert: layer_6[2.7-12.3% std=2.00]
  Expert: layer_7[1.5-15.4% std=2.93]
  Expert: layer_8[2.0-14.0% std=2.66]
  Expert: layer_9[3.0-12.2% std=1.94]
  Expert: layer_10[1.9-18.1% std=3.75]
  Expert: layer_12[0.4-22.6% std=6.07]
  Expert: layer_13[2.0-14.4% std=3.19]
  Expert: layer_14[0.3-35.3% std=7.94]
  Expert: layer_15[0.5-46.6% std=10.55]
  Expert: layer_16[0.2-20.4% std=4.76]
  Expert: layer_18[1.3-24.9% std=6.14]
  Expert: layer_19[0.3-46.2% std=10.55]
  Expert: layer_20[0.5-35.2% std=8.35]
  Expert: layer_21[0.7-41.7% std=9.66]
Step      91 | Loss: 5.5335 | LR: 5.69e-05 | GradNorm: 0.54 | Tok/s: 2565 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.5-8.2% std=1.06] | layer_5[4.3-7.9% std=0.69] | layer_11[2.5-15.6% std=3.32] | layer_17[2.1-21.9% std=5.98] | layer_22[0.4-47.6% std=11.13]
  Expert: layer_1[5.1-8.7% std=0.82]
  Expert: layer_2[5.8-6.8% std=0.28]
  Expert: layer_3[2.1-13.2% std=2.44]
  Expert: layer_4[4.3-7.6% std=1.09]
  Expert: layer_6[2.7-12.4% std=2.03]
  Expert: layer_7[1.5-16.2% std=3.10]
  Expert: layer_8[1.9-14.2% std=2.76]
  Expert: layer_9[2.7-12.4% std=2.02]
  Expert: layer_10[1.7-19.2% std=4.00]
  Expert: layer_12[0.4-22.5% std=6.17]
  Expert: layer_13[1.9-15.0% std=3.36]
  Expert: layer_14[0.4-36.2% std=8.16]
  Expert: layer_15[0.4-46.8% std=10.61]
  Expert: layer_16[0.1-21.5% std=5.02]
  Expert: layer_18[1.2-24.5% std=6.21]
  Expert: layer_19[0.3-46.7% std=10.64]
  Expert: layer_20[0.5-37.3% std=8.93]
  Expert: layer_21[0.6-42.5% std=9.91]
Step      92 | Loss: 5.5646 | LR: 5.75e-05 | GradNorm: 0.73 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.2% std=1.05] | layer_5[4.2-7.6% std=0.67] | layer_11[2.3-16.1% std=3.38] | layer_17[2.1-21.7% std=6.04] | layer_22[0.4-47.6% std=11.12]
  Expert: layer_1[5.0-8.7% std=0.82]
  Expert: layer_2[5.8-7.1% std=0.31]
  Expert: layer_3[2.2-13.2% std=2.44]
  Expert: layer_4[4.4-7.9% std=1.09]
  Expert: layer_6[2.6-12.5% std=2.09]
  Expert: layer_7[1.7-16.9% std=3.23]
  Expert: layer_8[1.7-14.3% std=2.88]
  Expert: layer_9[2.6-11.8% std=1.97]
  Expert: layer_10[1.5-19.2% std=4.02]
  Expert: layer_12[0.5-22.7% std=6.23]
  Expert: layer_13[1.8-15.3% std=3.43]
  Expert: layer_14[0.3-36.5% std=8.22]
  Expert: layer_15[0.3-46.9% std=10.64]
  Expert: layer_16[0.1-21.6% std=5.03]
  Expert: layer_18[1.2-23.7% std=5.92]
  Expert: layer_19[0.3-46.4% std=10.55]
  Expert: layer_20[0.7-39.2% std=9.33]
  Expert: layer_21[0.7-42.8% std=9.99]
Step      93 | Loss: 5.5516 | LR: 5.81e-05 | GradNorm: 0.82 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.3% std=1.07] | layer_5[4.3-7.0% std=0.58] | layer_11[2.5-16.0% std=3.30] | layer_17[2.3-22.2% std=5.68] | layer_22[0.4-47.5% std=11.02]
  Expert: layer_1[5.0-8.8% std=0.83]
  Expert: layer_2[5.8-6.8% std=0.29]
  Expert: layer_3[2.4-13.4% std=2.45]
  Expert: layer_4[4.2-7.5% std=1.05]
  Expert: layer_6[2.5-12.7% std=2.15]
  Expert: layer_7[1.9-17.0% std=3.24]
  Expert: layer_8[1.8-14.6% std=2.85]
  Expert: layer_9[2.5-11.6% std=1.92]
  Expert: layer_10[1.5-18.1% std=3.76]
  Expert: layer_12[0.5-22.3% std=6.05]
  Expert: layer_13[1.9-14.7% std=3.28]
  Expert: layer_14[0.3-36.4% std=8.17]
  Expert: layer_15[0.4-46.7% std=10.59]
  Expert: layer_16[0.2-21.3% std=4.94]
  Expert: layer_18[1.3-22.8% std=5.63]
  Expert: layer_19[0.3-45.9% std=10.44]
  Expert: layer_20[0.6-38.3% std=9.07]
  Expert: layer_21[0.7-42.9% std=9.96]
Step      94 | Loss: 5.4591 | LR: 5.87e-05 | GradNorm: 0.90 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.3% std=1.05] | layer_5[4.5-7.0% std=0.58] | layer_11[2.4-16.1% std=3.26] | layer_17[2.3-22.3% std=5.78] | layer_22[0.6-47.5% std=11.01]
  Expert: layer_1[5.1-8.5% std=0.76]
  Expert: layer_2[5.5-7.0% std=0.41]
  Expert: layer_3[2.5-13.4% std=2.47]
  Expert: layer_4[4.1-7.4% std=1.02]
  Expert: layer_6[2.5-12.4% std=2.10]
  Expert: layer_7[2.0-16.8% std=3.22]
  Expert: layer_8[1.8-14.9% std=2.81]
  Expert: layer_9[2.6-11.4% std=1.87]
  Expert: layer_10[1.6-17.2% std=3.63]
  Expert: layer_12[0.5-23.0% std=6.12]
  Expert: layer_13[2.0-15.3% std=3.34]
  Expert: layer_14[0.3-37.0% std=8.27]
  Expert: layer_15[0.6-46.4% std=10.51]
  Expert: layer_16[0.2-22.4% std=5.08]
  Expert: layer_18[1.4-22.6% std=5.91]
  Expert: layer_19[0.3-45.4% std=10.37]
  Expert: layer_20[0.7-38.1% std=9.00]
  Expert: layer_21[0.6-42.4% std=9.87]
Step      95 | Loss: 5.5348 | LR: 5.94e-05 | GradNorm: 0.67 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.3% std=1.07] | layer_5[4.5-7.4% std=0.65] | layer_11[2.5-15.8% std=3.20] | layer_17[2.1-23.0% std=5.81] | layer_22[0.6-46.9% std=10.86]
  Expert: layer_1[5.1-8.6% std=0.78]
  Expert: layer_2[5.7-7.0% std=0.34]
  Expert: layer_3[2.5-13.7% std=2.51]
  Expert: layer_4[4.3-7.4% std=0.93]
  Expert: layer_6[2.4-12.4% std=2.14]
  Expert: layer_7[1.9-17.0% std=3.20]
  Expert: layer_8[1.6-14.7% std=2.80]
  Expert: layer_9[2.5-11.8% std=2.05]
  Expert: layer_10[1.5-18.1% std=3.71]
  Expert: layer_12[0.5-22.8% std=6.14]
  Expert: layer_13[2.0-15.4% std=3.34]
  Expert: layer_14[0.3-36.5% std=8.17]
  Expert: layer_15[0.4-46.2% std=10.47]
  Expert: layer_16[0.2-22.3% std=5.14]
  Expert: layer_18[1.1-22.7% std=5.76]
  Expert: layer_19[0.4-44.2% std=10.02]
  Expert: layer_20[0.8-37.0% std=8.68]
  Expert: layer_21[0.7-41.4% std=9.71]
Step      96 | Loss: 5.5392 | LR: 6.00e-05 | GradNorm: 0.73 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.4% std=1.05] | layer_5[4.6-7.3% std=0.67] | layer_11[2.0-16.3% std=3.40] | layer_17[2.5-23.3% std=5.71] | layer_22[0.7-46.7% std=10.79]
  Expert: layer_1[5.1-8.6% std=0.76]
  Expert: layer_2[5.6-7.1% std=0.36]
  Expert: layer_3[2.5-13.7% std=2.51]
  Expert: layer_4[4.4-7.7% std=0.91]
  Expert: layer_6[2.4-12.5% std=2.15]
  Expert: layer_7[2.1-17.0% std=3.19]
  Expert: layer_8[1.5-14.9% std=2.84]
  Expert: layer_9[2.6-11.7% std=1.96]
  Expert: layer_10[1.5-18.5% std=3.78]
  Expert: layer_12[0.5-22.2% std=6.08]
  Expert: layer_13[2.0-16.0% std=3.43]
  Expert: layer_14[0.3-37.1% std=8.32]
  Expert: layer_15[0.4-46.4% std=10.52]
  Expert: layer_16[0.3-22.5% std=5.10]
  Expert: layer_18[1.2-23.3% std=5.79]
  Expert: layer_19[0.2-43.2% std=9.78]
  Expert: layer_20[0.8-37.5% std=8.78]
  Expert: layer_21[0.7-41.4% std=9.64]
Step      97 | Loss: 5.3814 | LR: 6.06e-05 | GradNorm: 0.70 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.4% std=1.04] | layer_5[4.7-7.1% std=0.66] | layer_11[1.7-15.7% std=3.20] | layer_17[2.2-22.7% std=5.74] | layer_22[0.7-47.1% std=10.88]
  Expert: layer_1[5.2-8.5% std=0.73]
  Expert: layer_2[5.7-7.1% std=0.36]
  Expert: layer_3[2.6-13.3% std=2.43]
  Expert: layer_4[4.3-7.6% std=0.93]
  Expert: layer_6[2.7-12.2% std=2.09]
  Expert: layer_7[2.1-17.0% std=3.20]
  Expert: layer_8[1.7-15.4% std=2.93]
  Expert: layer_9[2.5-11.4% std=1.93]
  Expert: layer_10[1.4-18.9% std=3.97]
  Expert: layer_12[0.5-22.4% std=6.19]
  Expert: layer_13[2.1-15.2% std=3.35]
  Expert: layer_14[0.2-37.6% std=8.40]
  Expert: layer_15[0.6-46.3% std=10.48]
  Expert: layer_16[0.3-23.3% std=5.13]
  Expert: layer_18[1.6-22.4% std=5.96]
  Expert: layer_19[0.3-43.8% std=9.95]
  Expert: layer_20[0.8-40.6% std=9.43]
  Expert: layer_21[0.8-41.9% std=9.65]
Step      98 | Loss: 5.4044 | LR: 6.12e-05 | GradNorm: 0.73 | Tok/s: 2602 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.4% std=1.02] | layer_5[4.6-7.0% std=0.63] | layer_11[1.7-15.4% std=3.06] | layer_17[2.4-20.5% std=5.26] | layer_22[0.8-45.9% std=10.56]
  Expert: layer_1[5.2-8.5% std=0.75]
  Expert: layer_2[5.7-6.9% std=0.28]
  Expert: layer_3[2.5-13.5% std=2.49]
  Expert: layer_4[4.3-7.6% std=0.91]
  Expert: layer_6[2.9-11.9% std=1.98]
  Expert: layer_7[2.1-16.6% std=3.09]
  Expert: layer_8[1.8-14.9% std=2.79]
  Expert: layer_9[2.7-11.3% std=1.86]
  Expert: layer_10[1.6-18.5% std=3.75]
  Expert: layer_12[0.6-22.6% std=6.03]
  Expert: layer_13[2.2-15.3% std=3.17]
  Expert: layer_14[0.3-35.8% std=7.99]
  Expert: layer_15[0.6-46.1% std=10.42]
  Expert: layer_16[0.4-21.4% std=4.76]
  Expert: layer_18[1.6-22.6% std=5.54]
  Expert: layer_19[0.4-41.1% std=9.22]
  Expert: layer_20[0.9-38.8% std=8.92]
  Expert: layer_21[0.7-39.7% std=9.13]
Step      99 | Loss: 5.5041 | LR: 6.19e-05 | GradNorm: 0.65 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=1.02] | layer_5[4.8-7.2% std=0.64] | layer_11[1.6-16.3% std=3.32] | layer_17[2.2-21.4% std=5.24] | layer_22[0.7-45.9% std=10.61]
  Expert: layer_1[5.2-8.7% std=0.74]
  Expert: layer_2[5.5-6.8% std=0.29]
  Expert: layer_3[2.6-13.9% std=2.56]
  Expert: layer_4[4.4-7.8% std=0.90]
  Expert: layer_6[2.6-11.8% std=2.02]
  Expert: layer_7[2.0-16.7% std=3.12]
  Expert: layer_8[1.6-14.7% std=2.78]
  Expert: layer_9[2.7-12.0% std=1.96]
  Expert: layer_10[1.5-19.0% std=3.81]
  Expert: layer_12[0.6-22.9% std=6.09]
  Expert: layer_13[2.1-14.8% std=3.14]
  Expert: layer_14[0.4-36.2% std=8.12]
  Expert: layer_15[0.5-46.4% std=10.51]
  Expert: layer_16[0.3-21.8% std=4.88]
  Expert: layer_18[1.0-24.1% std=5.93]
  Expert: layer_19[0.4-40.5% std=9.12]
  Expert: layer_20[0.8-38.2% std=8.80]
  Expert: layer_21[0.6-39.8% std=9.28]
Step     100 | Loss: 5.4344 | LR: 6.25e-05 | GradNorm: 0.64 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=1.01] | layer_5[5.1-7.2% std=0.57] | layer_11[1.5-16.2% std=3.40] | layer_17[2.0-20.8% std=5.50] | layer_22[0.8-46.0% std=10.62]
  Expert: layer_1[5.0-8.6% std=0.71]
  Expert: layer_2[5.5-6.8% std=0.32]
  Expert: layer_3[2.6-14.1% std=2.63]
  Expert: layer_4[4.3-8.1% std=0.93]
  Expert: layer_6[2.7-12.0% std=2.05]
  Expert: layer_7[1.9-17.0% std=3.26]
  Expert: layer_8[1.6-14.0% std=2.70]
  Expert: layer_9[2.4-12.1% std=2.02]
  Expert: layer_10[1.4-20.1% std=4.06]
  Expert: layer_12[0.5-23.3% std=6.30]
  Expert: layer_13[1.9-14.3% std=3.21]
  Expert: layer_14[0.4-37.1% std=8.31]
  Expert: layer_15[0.5-46.6% std=10.54]
  Expert: layer_16[0.3-22.4% std=4.94]
  Expert: layer_18[0.9-23.7% std=6.19]
  Expert: layer_19[0.5-42.6% std=9.63]
  Expert: layer_20[0.7-40.0% std=9.24]
  Expert: layer_21[0.7-41.4% std=9.66]
Step     101 | Loss: 5.3686 | LR: 6.31e-05 | GradNorm: 0.67 | Tok/s: 2595 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=1.03] | layer_5[4.8-7.2% std=0.58] | layer_11[1.8-15.3% std=3.11] | layer_17[2.1-20.7% std=5.45] | layer_22[1.0-45.4% std=10.38]
  Expert: layer_1[5.0-8.5% std=0.68]
  Expert: layer_2[5.3-6.8% std=0.35]
  Expert: layer_3[2.8-13.6% std=2.55]
  Expert: layer_4[4.4-7.5% std=0.88]
  Expert: layer_6[2.7-11.8% std=1.98]
  Expert: layer_7[1.9-16.2% std=3.07]
  Expert: layer_8[1.8-13.7% std=2.61]
  Expert: layer_9[2.6-11.5% std=1.87]
  Expert: layer_10[1.5-19.4% std=3.93]
  Expert: layer_12[0.6-22.8% std=6.00]
  Expert: layer_13[1.9-13.6% std=2.99]
  Expert: layer_14[0.4-36.1% std=8.06]
  Expert: layer_15[0.5-46.0% std=10.40]
  Expert: layer_16[0.5-19.3% std=4.30]
  Expert: layer_18[1.3-22.3% std=5.34]
  Expert: layer_19[0.4-41.3% std=9.27]
  Expert: layer_20[1.0-38.0% std=8.70]
  Expert: layer_21[0.8-39.6% std=9.14]
Step     102 | Loss: 5.3632 | LR: 6.38e-05 | GradNorm: 0.56 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.3% std=1.03] | layer_5[4.7-7.6% std=0.68] | layer_11[1.8-15.1% std=3.09] | layer_17[2.2-19.6% std=5.29] | layer_22[0.7-46.0% std=10.54]
  Expert: layer_1[5.1-8.4% std=0.69]
  Expert: layer_2[5.5-6.7% std=0.32]
  Expert: layer_3[2.9-13.5% std=2.47]
  Expert: layer_4[4.6-7.6% std=0.90]
  Expert: layer_6[2.6-11.9% std=1.98]
  Expert: layer_7[2.0-16.8% std=3.15]
  Expert: layer_8[1.9-13.9% std=2.60]
  Expert: layer_9[2.8-11.7% std=1.87]
  Expert: layer_10[1.5-19.3% std=3.95]
  Expert: layer_12[0.6-22.6% std=5.89]
  Expert: layer_13[2.1-12.9% std=2.94]
  Expert: layer_14[0.4-35.9% std=8.02]
  Expert: layer_15[0.6-45.9% std=10.37]
  Expert: layer_16[0.5-19.4% std=4.32]
  Expert: layer_18[1.4-22.7% std=5.26]
  Expert: layer_19[0.3-40.7% std=9.11]
  Expert: layer_20[0.8-37.1% std=8.50]
  Expert: layer_21[0.8-38.3% std=8.88]
Step     103 | Loss: 5.3491 | LR: 6.44e-05 | GradNorm: 0.68 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.4% std=1.08] | layer_5[4.7-7.9% std=0.75] | layer_11[1.7-15.9% std=3.33] | layer_17[2.2-20.5% std=5.41] | layer_22[0.8-46.5% std=10.66]
  Expert: layer_1[4.9-8.4% std=0.73]
  Expert: layer_2[5.7-6.7% std=0.32]
  Expert: layer_3[2.7-13.7% std=2.53]
  Expert: layer_4[4.5-7.6% std=0.95]
  Expert: layer_6[2.6-11.9% std=2.00]
  Expert: layer_7[2.1-17.3% std=3.23]
  Expert: layer_8[1.9-14.1% std=2.65]
  Expert: layer_9[2.8-12.1% std=1.99]
  Expert: layer_10[1.6-18.8% std=3.85]
  Expert: layer_12[0.6-22.4% std=5.91]
  Expert: layer_13[2.2-12.8% std=2.88]
  Expert: layer_14[0.5-35.9% std=8.04]
  Expert: layer_15[0.5-45.9% std=10.37]
  Expert: layer_16[0.5-20.5% std=4.57]
  Expert: layer_18[1.4-22.8% std=5.40]
  Expert: layer_19[0.4-39.0% std=8.69]
  Expert: layer_20[0.9-37.2% std=8.60]
  Expert: layer_21[0.7-38.0% std=8.83]
Step     104 | Loss: 5.3532 | LR: 6.50e-05 | GradNorm: 0.64 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=1.01] | layer_5[4.7-7.6% std=0.68] | layer_11[1.6-16.4% std=3.46] | layer_17[2.3-21.2% std=5.49] | layer_22[0.8-45.9% std=10.50]
  Expert: layer_1[5.1-8.2% std=0.66]
  Expert: layer_2[5.8-6.7% std=0.22]
  Expert: layer_3[2.8-14.3% std=2.69]
  Expert: layer_4[4.2-7.7% std=1.03]
  Expert: layer_6[2.7-12.1% std=2.01]
  Expert: layer_7[2.0-18.0% std=3.46]
  Expert: layer_8[1.8-14.4% std=2.67]
  Expert: layer_9[2.7-12.8% std=2.16]
  Expert: layer_10[1.5-19.7% std=4.02]
  Expert: layer_12[0.6-23.6% std=6.12]
  Expert: layer_13[2.6-13.6% std=2.87]
  Expert: layer_14[0.5-36.2% std=8.08]
  Expert: layer_15[0.8-46.3% std=10.47]
  Expert: layer_16[0.5-20.9% std=4.64]
  Expert: layer_18[1.5-24.2% std=5.61]
  Expert: layer_19[0.5-38.2% std=8.51]
  Expert: layer_20[0.8-37.3% std=8.62]
  Expert: layer_21[0.7-38.0% std=8.88]
Step     105 | Loss: 5.3295 | LR: 6.56e-05 | GradNorm: 0.68 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.3% std=1.02] | layer_5[4.7-7.6% std=0.71] | layer_11[1.7-16.6% std=3.45] | layer_17[2.3-21.1% std=5.16] | layer_22[0.8-44.7% std=10.21]
  Expert: layer_1[5.0-8.3% std=0.68]
  Expert: layer_2[5.5-6.8% std=0.33]
  Expert: layer_3[2.8-14.4% std=2.69]
  Expert: layer_4[4.3-7.5% std=1.00]
  Expert: layer_6[2.7-12.1% std=1.99]
  Expert: layer_7[2.1-18.3% std=3.56]
  Expert: layer_8[1.7-14.2% std=2.65]
  Expert: layer_9[3.1-12.4% std=2.05]
  Expert: layer_10[1.5-19.1% std=3.83]
  Expert: layer_12[0.7-23.3% std=5.98]
  Expert: layer_13[2.2-13.2% std=2.77]
  Expert: layer_14[0.6-36.5% std=8.13]
  Expert: layer_15[0.7-45.9% std=10.36]
  Expert: layer_16[0.5-19.9% std=4.46]
  Expert: layer_18[1.5-22.3% std=5.17]
  Expert: layer_19[0.5-36.1% std=8.00]
  Expert: layer_20[0.9-36.5% std=8.44]
  Expert: layer_21[0.7-37.3% std=8.69]
Step     106 | Loss: 5.3454 | LR: 6.62e-05 | GradNorm: 0.69 | Tok/s: 2594 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=1.00] | layer_5[4.7-7.4% std=0.67] | layer_11[1.6-16.8% std=3.57] | layer_17[2.4-21.5% std=4.98] | layer_22[0.8-43.4% std=9.91]
  Expert: layer_1[5.0-8.2% std=0.66]
  Expert: layer_2[5.7-6.7% std=0.26]
  Expert: layer_3[2.9-14.5% std=2.71]
  Expert: layer_4[4.4-8.0% std=0.98]
  Expert: layer_6[2.7-12.1% std=1.99]
  Expert: layer_7[2.1-18.2% std=3.57]
  Expert: layer_8[1.6-14.4% std=2.74]
  Expert: layer_9[3.0-12.5% std=2.10]
  Expert: layer_10[1.5-19.1% std=3.82]
  Expert: layer_12[0.7-22.9% std=6.01]
  Expert: layer_13[2.4-13.7% std=2.89]
  Expert: layer_14[0.6-37.2% std=8.31]
  Expert: layer_15[0.8-46.1% std=10.42]
  Expert: layer_16[0.4-21.3% std=4.81]
  Expert: layer_18[1.3-22.6% std=5.45]
  Expert: layer_19[0.4-37.2% std=8.26]
  Expert: layer_20[0.9-36.3% std=8.41]
  Expert: layer_21[0.6-37.4% std=8.77]
Step     107 | Loss: 5.3551 | LR: 6.69e-05 | GradNorm: 0.67 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=1.00] | layer_5[4.9-7.4% std=0.61] | layer_11[1.8-17.3% std=3.69] | layer_17[2.0-22.3% std=5.31] | layer_22[0.7-42.8% std=9.83]
  Expert: layer_1[5.1-8.1% std=0.65]
  Expert: layer_2[5.7-6.8% std=0.29]
  Expert: layer_3[2.8-15.0% std=2.81]
  Expert: layer_4[4.5-7.7% std=0.94]
  Expert: layer_6[2.7-12.3% std=2.02]
  Expert: layer_7[2.1-17.7% std=3.42]
  Expert: layer_8[1.6-15.3% std=2.91]
  Expert: layer_9[3.2-12.3% std=2.07]
  Expert: layer_10[1.5-18.6% std=3.76]
  Expert: layer_12[0.7-23.5% std=6.09]
  Expert: layer_13[2.3-13.5% std=2.86]
  Expert: layer_14[0.5-37.8% std=8.43]
  Expert: layer_15[0.6-46.4% std=10.50]
  Expert: layer_16[0.3-20.4% std=4.71]
  Expert: layer_18[1.4-21.7% std=5.63]
  Expert: layer_19[0.5-37.5% std=8.33]
  Expert: layer_20[0.9-36.6% std=8.48]
  Expert: layer_21[0.7-37.9% std=8.88]
Step     108 | Loss: 5.3626 | LR: 6.75e-05 | GradNorm: 0.63 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=1.02] | layer_5[4.9-7.3% std=0.62] | layer_11[2.0-17.8% std=3.74] | layer_17[2.1-20.2% std=4.80] | layer_22[0.6-41.0% std=9.46]
  Expert: layer_1[5.1-8.2% std=0.68]
  Expert: layer_2[6.0-7.0% std=0.28]
  Expert: layer_3[2.9-15.1% std=2.80]
  Expert: layer_4[4.5-8.1% std=0.98]
  Expert: layer_6[2.7-12.2% std=1.98]
  Expert: layer_7[2.2-17.2% std=3.23]
  Expert: layer_8[1.6-15.0% std=2.89]
  Expert: layer_9[3.4-12.0% std=1.95]
  Expert: layer_10[1.6-18.3% std=3.70]
  Expert: layer_12[0.6-24.2% std=6.14]
  Expert: layer_13[2.4-14.2% std=2.89]
  Expert: layer_14[0.5-36.7% std=8.19]
  Expert: layer_15[0.8-46.1% std=10.46]
  Expert: layer_16[0.4-19.9% std=4.71]
  Expert: layer_18[1.6-22.5% std=5.49]
  Expert: layer_19[0.5-36.2% std=7.98]
  Expert: layer_20[0.8-34.3% std=7.93]
  Expert: layer_21[0.6-36.7% std=8.74]
Step     109 | Loss: 5.2729 | LR: 6.81e-05 | GradNorm: 0.52 | Tok/s: 2600 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=1.01] | layer_5[4.9-7.0% std=0.53] | layer_11[2.0-17.9% std=3.80] | layer_17[2.5-19.5% std=4.55] | layer_22[0.8-41.6% std=9.53]
  Expert: layer_1[5.1-8.2% std=0.69]
  Expert: layer_2[5.7-6.6% std=0.22]
  Expert: layer_3[2.9-14.7% std=2.73]
  Expert: layer_4[4.4-7.9% std=0.99]
  Expert: layer_6[2.6-12.2% std=1.99]
  Expert: layer_7[2.2-17.5% std=3.32]
  Expert: layer_8[1.7-14.9% std=2.88]
  Expert: layer_9[3.5-11.3% std=1.79]
  Expert: layer_10[1.7-18.8% std=3.84]
  Expert: layer_12[0.7-23.7% std=5.99]
  Expert: layer_13[2.3-13.9% std=2.84]
  Expert: layer_14[0.5-37.4% std=8.33]
  Expert: layer_15[0.8-46.3% std=10.48]
  Expert: layer_16[0.4-20.7% std=4.66]
  Expert: layer_18[1.5-21.3% std=5.41]
  Expert: layer_19[0.7-35.7% std=7.81]
  Expert: layer_20[1.1-36.8% std=8.41]
  Expert: layer_21[0.7-36.7% std=8.69]
Step     110 | Loss: 5.2732 | LR: 6.88e-05 | GradNorm: 0.61 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=0.99] | layer_5[5.0-6.9% std=0.50] | layer_11[1.9-18.0% std=3.79] | layer_17[2.2-18.7% std=4.39] | layer_22[0.8-41.3% std=9.42]
  Expert: layer_1[5.3-8.1% std=0.63]
  Expert: layer_2[6.0-6.6% std=0.18]
  Expert: layer_3[2.8-14.5% std=2.69]
  Expert: layer_4[4.4-8.2% std=1.03]
  Expert: layer_6[2.5-12.4% std=1.99]
  Expert: layer_7[2.3-17.4% std=3.31]
  Expert: layer_8[1.7-15.3% std=2.96]
  Expert: layer_9[3.4-11.6% std=1.87]
  Expert: layer_10[1.8-19.3% std=3.94]
  Expert: layer_12[0.7-24.2% std=6.05]
  Expert: layer_13[2.3-14.1% std=2.86]
  Expert: layer_14[0.5-38.1% std=8.52]
  Expert: layer_15[0.9-46.4% std=10.50]
  Expert: layer_16[0.4-21.9% std=4.80]
  Expert: layer_18[1.2-21.9% std=5.38]
  Expert: layer_19[0.7-34.8% std=7.60]
  Expert: layer_20[1.4-38.0% std=8.59]
  Expert: layer_21[0.9-36.2% std=8.47]
Step     111 | Loss: 5.2809 | LR: 6.94e-05 | GradNorm: 0.61 | Tok/s: 2590 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=1.05] | layer_5[4.9-7.3% std=0.56] | layer_11[1.9-19.6% std=4.08] | layer_17[2.1-17.9% std=4.37] | layer_22[0.9-39.2% std=8.97]
  Expert: layer_1[5.2-8.3% std=0.65]
  Expert: layer_2[6.0-6.6% std=0.15]
  Expert: layer_3[2.7-14.4% std=2.68]
  Expert: layer_4[4.4-8.3% std=1.07]
  Expert: layer_6[2.5-12.1% std=1.94]
  Expert: layer_7[2.0-17.4% std=3.31]
  Expert: layer_8[1.6-15.2% std=2.96]
  Expert: layer_9[3.3-11.5% std=1.87]
  Expert: layer_10[1.9-18.9% std=3.90]
  Expert: layer_12[0.6-25.1% std=6.19]
  Expert: layer_13[2.4-15.7% std=3.11]
  Expert: layer_14[0.5-38.3% std=8.59]
  Expert: layer_15[0.8-46.3% std=10.51]
  Expert: layer_16[0.3-22.2% std=4.91]
  Expert: layer_18[1.1-23.5% std=5.60]
  Expert: layer_19[0.7-33.9% std=7.43]
  Expert: layer_20[1.2-36.6% std=8.28]
  Expert: layer_21[0.9-34.9% std=8.21]
Step     112 | Loss: 5.2598 | LR: 7.00e-05 | GradNorm: 0.66 | Tok/s: 2600 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.2% std=1.02] | layer_5[4.8-7.1% std=0.57] | layer_11[1.8-19.2% std=3.97] | layer_17[1.5-18.2% std=4.41] | layer_22[0.6-39.4% std=9.06]
  Expert: layer_1[5.3-8.4% std=0.67]
  Expert: layer_2[5.9-6.4% std=0.16]
  Expert: layer_3[2.7-14.2% std=2.69]
  Expert: layer_4[4.5-8.1% std=1.08]
  Expert: layer_6[2.5-12.1% std=1.99]
  Expert: layer_7[1.8-17.8% std=3.40]
  Expert: layer_8[1.6-15.3% std=3.06]
  Expert: layer_9[3.2-10.9% std=1.81]
  Expert: layer_10[1.7-18.7% std=3.90]
  Expert: layer_12[0.7-24.0% std=5.94]
  Expert: layer_13[2.2-14.9% std=3.05]
  Expert: layer_14[0.5-38.2% std=8.55]
  Expert: layer_15[0.6-45.9% std=10.36]
  Expert: layer_16[0.3-21.6% std=4.65]
  Expert: layer_18[0.8-21.6% std=5.39]
  Expert: layer_19[0.7-33.2% std=7.28]
  Expert: layer_20[1.3-36.9% std=8.36]
  Expert: layer_21[0.9-33.3% std=7.81]
Step     113 | Loss: 5.2013 | LR: 7.06e-05 | GradNorm: 0.68 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=1.00] | layer_5[5.0-7.0% std=0.51] | layer_11[1.5-18.9% std=3.88] | layer_17[2.3-18.7% std=4.18] | layer_22[1.1-38.1% std=8.74]
  Expert: layer_1[5.4-8.3% std=0.66]
  Expert: layer_2[5.8-6.5% std=0.21]
  Expert: layer_3[3.0-13.9% std=2.56]
  Expert: layer_4[4.4-8.3% std=1.09]
  Expert: layer_6[2.5-11.9% std=1.91]
  Expert: layer_7[2.2-16.9% std=3.18]
  Expert: layer_8[1.8-15.3% std=2.92]
  Expert: layer_9[3.1-10.9% std=1.80]
  Expert: layer_10[2.1-17.4% std=3.50]
  Expert: layer_12[0.7-23.5% std=5.73]
  Expert: layer_13[2.3-15.3% std=3.00]
  Expert: layer_14[0.6-37.6% std=8.34]
  Expert: layer_15[0.9-45.7% std=10.31]
  Expert: layer_16[0.6-22.6% std=4.84]
  Expert: layer_18[1.7-23.0% std=5.57]
  Expert: layer_19[0.7-33.3% std=7.23]
  Expert: layer_20[1.4-34.8% std=7.88]
  Expert: layer_21[1.2-32.0% std=7.53]
Step     114 | Loss: 5.2789 | LR: 7.12e-05 | GradNorm: 0.70 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=0.98] | layer_5[5.0-7.4% std=0.60] | layer_11[1.4-18.6% std=3.82] | layer_17[1.4-19.8% std=4.43] | layer_22[0.8-40.2% std=9.26]
  Expert: layer_1[5.6-8.4% std=0.64]
  Expert: layer_2[5.7-6.5% std=0.21]
  Expert: layer_3[3.0-13.7% std=2.55]
  Expert: layer_4[4.4-8.4% std=1.08]
  Expert: layer_6[2.4-11.6% std=1.82]
  Expert: layer_7[2.1-17.6% std=3.34]
  Expert: layer_8[1.6-15.7% std=3.06]
  Expert: layer_9[2.8-11.3% std=1.95]
  Expert: layer_10[1.9-18.2% std=3.72]
  Expert: layer_12[0.7-23.9% std=5.88]
  Expert: layer_13[2.3-14.1% std=2.93]
  Expert: layer_14[0.5-38.7% std=8.65]
  Expert: layer_15[0.5-46.0% std=10.41]
  Expert: layer_16[0.4-23.7% std=5.05]
  Expert: layer_18[1.1-21.2% std=5.62]
  Expert: layer_19[0.7-34.0% std=7.46]
  Expert: layer_20[1.4-37.0% std=8.47]
  Expert: layer_21[1.2-33.3% std=7.68]
Step     115 | Loss: 5.1722 | LR: 7.19e-05 | GradNorm: 0.69 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=0.97] | layer_5[5.1-7.5% std=0.62] | layer_11[1.4-18.5% std=3.79] | layer_17[2.2-18.1% std=3.97] | layer_22[0.9-37.1% std=8.70]
  Expert: layer_1[5.6-8.3% std=0.64]
  Expert: layer_2[5.6-6.5% std=0.23]
  Expert: layer_3[3.1-13.9% std=2.56]
  Expert: layer_4[4.3-7.9% std=1.06]
  Expert: layer_6[2.4-11.9% std=1.88]
  Expert: layer_7[2.3-16.5% std=3.02]
  Expert: layer_8[1.7-15.5% std=2.96]
  Expert: layer_9[2.6-11.1% std=1.89]
  Expert: layer_10[2.2-17.4% std=3.43]
  Expert: layer_12[0.7-24.6% std=5.99]
  Expert: layer_13[2.3-14.9% std=2.92]
  Expert: layer_14[0.6-37.1% std=8.23]
  Expert: layer_15[0.7-45.8% std=10.37]
  Expert: layer_16[0.6-23.1% std=5.02]
  Expert: layer_18[1.9-22.0% std=5.51]
  Expert: layer_19[0.8-32.5% std=7.08]
  Expert: layer_20[1.2-33.4% std=7.66]
  Expert: layer_21[1.0-31.1% std=7.31]
Step     116 | Loss: 5.2132 | LR: 7.25e-05 | GradNorm: 0.62 | Tok/s: 2581 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.4% std=1.00] | layer_5[5.2-7.7% std=0.65] | layer_11[1.7-17.6% std=3.62] | layer_17[2.4-17.5% std=3.86] | layer_22[0.9-36.4% std=8.48]
  Expert: layer_1[5.6-8.3% std=0.61]
  Expert: layer_2[6.0-6.8% std=0.22]
  Expert: layer_3[3.1-14.3% std=2.63]
  Expert: layer_4[4.4-7.9% std=1.02]
  Expert: layer_6[2.4-11.9% std=1.85]
  Expert: layer_7[1.9-17.4% std=3.28]
  Expert: layer_8[1.7-15.1% std=2.94]
  Expert: layer_9[2.5-11.9% std=2.06]
  Expert: layer_10[2.0-18.6% std=3.69]
  Expert: layer_12[0.8-24.3% std=6.07]
  Expert: layer_13[2.4-13.9% std=2.75]
  Expert: layer_14[0.6-36.9% std=8.20]
  Expert: layer_15[0.6-45.9% std=10.40]
  Expert: layer_16[0.4-23.6% std=5.12]
  Expert: layer_18[1.4-21.7% std=5.41]
  Expert: layer_19[0.8-32.9% std=7.19]
  Expert: layer_20[1.2-35.1% std=8.00]
  Expert: layer_21[1.2-31.7% std=7.40]
Step     117 | Loss: 5.1996 | LR: 7.31e-05 | GradNorm: 0.63 | Tok/s: 2596 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=1.04] | layer_5[5.4-7.5% std=0.59] | layer_11[1.6-17.9% std=3.67] | layer_17[2.4-17.4% std=3.99] | layer_22[0.9-36.3% std=8.40]
  Expert: layer_1[5.6-8.4% std=0.64]
  Expert: layer_2[5.9-6.7% std=0.23]
  Expert: layer_3[3.1-14.4% std=2.64]
  Expert: layer_4[4.4-7.7% std=1.04]
  Expert: layer_6[2.5-12.2% std=1.92]
  Expert: layer_7[1.8-17.0% std=3.19]
  Expert: layer_8[1.7-15.4% std=3.00]
  Expert: layer_9[2.8-12.2% std=2.04]
  Expert: layer_10[2.1-19.3% std=3.90]
  Expert: layer_12[0.7-24.3% std=6.06]
  Expert: layer_13[2.3-14.0% std=2.75]
  Expert: layer_14[0.6-36.9% std=8.22]
  Expert: layer_15[0.6-45.4% std=10.27]
  Expert: layer_16[0.4-23.6% std=5.14]
  Expert: layer_18[1.8-20.7% std=5.22]
  Expert: layer_19[0.7-32.0% std=7.01]
  Expert: layer_20[1.1-35.7% std=8.12]
  Expert: layer_21[1.1-31.5% std=7.27]
Step     118 | Loss: 5.1665 | LR: 7.38e-05 | GradNorm: 0.64 | Tok/s: 2628 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=0.99] | layer_5[5.2-7.3% std=0.58] | layer_11[1.9-18.3% std=3.70] | layer_17[2.7-17.2% std=3.74] | layer_22[1.0-34.7% std=8.02]
  Expert: layer_1[5.6-8.2% std=0.57]
  Expert: layer_2[5.9-6.6% std=0.18]
  Expert: layer_3[3.3-14.6% std=2.65]
  Expert: layer_4[4.4-7.3% std=0.94]
  Expert: layer_6[2.6-12.6% std=1.98]
  Expert: layer_7[2.2-16.5% std=3.01]
  Expert: layer_8[1.8-15.4% std=2.87]
  Expert: layer_9[2.7-11.8% std=1.97]
  Expert: layer_10[2.2-19.6% std=3.81]
  Expert: layer_12[0.8-24.2% std=6.08]
  Expert: layer_13[2.4-14.1% std=2.65]
  Expert: layer_14[0.6-37.0% std=8.22]
  Expert: layer_15[0.7-44.5% std=10.03]
  Expert: layer_16[0.6-22.4% std=4.82]
  Expert: layer_18[2.1-20.6% std=5.05]
  Expert: layer_19[0.9-32.4% std=7.04]
  Expert: layer_20[1.4-35.6% std=8.07]
  Expert: layer_21[1.2-30.2% std=7.03]
Step     119 | Loss: 5.1917 | LR: 7.44e-05 | GradNorm: 0.61 | Tok/s: 2596 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.3% std=0.98] | layer_5[5.0-7.4% std=0.62] | layer_11[2.2-18.9% std=3.85] | layer_17[2.4-17.6% std=3.75] | layer_22[0.8-35.4% std=8.24]
  Expert: layer_1[5.6-8.5% std=0.63]
  Expert: layer_2[5.7-6.7% std=0.26]
  Expert: layer_3[3.2-14.7% std=2.65]
  Expert: layer_4[4.3-7.2% std=0.94]
  Expert: layer_6[2.5-12.4% std=1.94]
  Expert: layer_7[2.1-16.6% std=3.01]
  Expert: layer_8[1.7-15.3% std=2.90]
  Expert: layer_9[2.9-11.3% std=1.84]
  Expert: layer_10[1.9-20.3% std=4.05]
  Expert: layer_12[0.8-24.3% std=6.18]
  Expert: layer_13[2.3-14.3% std=2.75]
  Expert: layer_14[0.5-37.7% std=8.40]
  Expert: layer_15[0.5-44.4% std=10.00]
  Expert: layer_16[0.5-22.2% std=4.79]
  Expert: layer_18[1.6-19.7% std=4.86]
  Expert: layer_19[0.8-32.2% std=7.01]
  Expert: layer_20[1.5-36.2% std=8.18]
  Expert: layer_21[1.1-30.3% std=7.13]
Step     120 | Loss: 5.1581 | LR: 7.50e-05 | GradNorm: 0.65 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.3% std=0.98] | layer_5[5.3-7.4% std=0.60] | layer_11[1.8-19.2% std=3.95] | layer_17[2.5-16.0% std=3.47] | layer_22[0.8-33.6% std=7.99]
  Expert: layer_1[5.4-8.3% std=0.61]
  Expert: layer_2[5.7-6.8% std=0.29]
  Expert: layer_3[3.1-14.6% std=2.63]
  Expert: layer_4[4.4-7.6% std=0.97]
  Expert: layer_6[2.5-12.5% std=2.00]
  Expert: layer_7[1.7-16.3% std=3.01]
  Expert: layer_8[1.8-15.4% std=2.90]
  Expert: layer_9[3.1-11.4% std=1.75]
  Expert: layer_10[1.8-21.0% std=4.23]
  Expert: layer_12[0.7-24.2% std=6.26]
  Expert: layer_13[2.2-15.9% std=3.05]
  Expert: layer_14[0.5-37.3% std=8.29]
  Expert: layer_15[0.5-44.3% std=10.00]
  Expert: layer_16[0.6-22.0% std=4.79]
  Expert: layer_18[1.9-19.2% std=4.75]
  Expert: layer_19[0.8-31.8% std=6.95]
  Expert: layer_20[1.3-34.0% std=7.66]
  Expert: layer_21[1.3-29.6% std=6.97]
Step     121 | Loss: 5.2117 | LR: 7.56e-05 | GradNorm: 0.64 | Tok/s: 2584 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.95] | layer_5[5.2-7.6% std=0.70] | layer_11[2.0-17.5% std=3.60] | layer_17[2.9-14.2% std=3.06] | layer_22[1.1-34.5% std=7.98]
  Expert: layer_1[5.3-8.4% std=0.64]
  Expert: layer_2[5.4-6.9% std=0.37]
  Expert: layer_3[3.2-14.0% std=2.54]
  Expert: layer_4[4.4-7.6% std=0.95]
  Expert: layer_6[2.6-12.3% std=1.95]
  Expert: layer_7[1.5-17.2% std=3.24]
  Expert: layer_8[1.7-15.3% std=2.97]
  Expert: layer_9[3.1-11.1% std=1.69]
  Expert: layer_10[1.5-22.3% std=4.52]
  Expert: layer_12[0.8-24.2% std=6.41]
  Expert: layer_13[2.2-15.2% std=2.89]
  Expert: layer_14[0.4-37.8% std=8.44]
  Expert: layer_15[0.4-44.4% std=9.97]
  Expert: layer_16[0.5-21.8% std=4.68]
  Expert: layer_18[1.0-17.5% std=4.36]
  Expert: layer_19[0.7-32.4% std=7.06]
  Expert: layer_20[1.7-36.5% std=8.23]
  Expert: layer_21[1.0-30.3% std=7.01]
Step     122 | Loss: 5.1987 | LR: 7.62e-05 | GradNorm: 0.64 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=0.96] | layer_5[5.2-7.7% std=0.75] | layer_11[2.1-18.6% std=3.77] | layer_17[2.7-15.9% std=3.39] | layer_22[1.1-33.7% std=7.93]
  Expert: layer_1[5.4-8.4% std=0.67]
  Expert: layer_2[5.6-6.8% std=0.28]
  Expert: layer_3[3.3-14.3% std=2.55]
  Expert: layer_4[4.4-7.4% std=0.86]
  Expert: layer_6[2.7-12.6% std=1.96]
  Expert: layer_7[1.6-16.6% std=3.04]
  Expert: layer_8[1.7-15.5% std=2.98]
  Expert: layer_9[3.1-11.7% std=1.78]
  Expert: layer_10[1.7-22.1% std=4.41]
  Expert: layer_12[0.8-24.7% std=6.44]
  Expert: layer_13[2.3-14.7% std=2.77]
  Expert: layer_14[0.4-37.8% std=8.41]
  Expert: layer_15[0.5-44.9% std=10.12]
  Expert: layer_16[0.5-23.0% std=5.03]
  Expert: layer_18[1.3-19.6% std=4.91]
  Expert: layer_19[0.7-31.4% std=6.86]
  Expert: layer_20[1.5-34.7% std=7.91]
  Expert: layer_21[1.1-29.8% std=7.13]
Step     123 | Loss: 5.1585 | LR: 7.69e-05 | GradNorm: 0.61 | Tok/s: 2581 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.91] | layer_5[5.3-7.6% std=0.68] | layer_11[2.2-17.7% std=3.52] | layer_17[2.6-17.2% std=3.67] | layer_22[1.2-33.5% std=7.88]
  Expert: layer_1[5.5-8.2% std=0.62]
  Expert: layer_2[5.6-6.8% std=0.26]
  Expert: layer_3[3.4-14.1% std=2.52]
  Expert: layer_4[4.6-7.2% std=0.80]
  Expert: layer_6[2.9-12.6% std=1.95]
  Expert: layer_7[1.6-16.7% std=3.07]
  Expert: layer_8[1.9-15.2% std=2.87]
  Expert: layer_9[3.4-11.4% std=1.71]
  Expert: layer_10[1.8-21.4% std=4.26]
  Expert: layer_12[1.0-24.0% std=6.19]
  Expert: layer_13[2.6-14.3% std=2.66]
  Expert: layer_14[0.4-37.4% std=8.28]
  Expert: layer_15[0.5-44.7% std=10.08]
  Expert: layer_16[0.6-22.0% std=4.75]
  Expert: layer_18[1.5-18.8% std=4.84]
  Expert: layer_19[0.8-29.4% std=6.40]
  Expert: layer_20[1.6-34.9% std=7.94]
  Expert: layer_21[1.3-28.7% std=6.75]
Step     124 | Loss: 5.1046 | LR: 7.75e-05 | GradNorm: 0.62 | Tok/s: 2610 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.93] | layer_5[5.1-7.4% std=0.62] | layer_11[2.1-17.6% std=3.52] | layer_17[2.3-16.8% std=3.61] | layer_22[1.4-32.4% std=7.50]
  Expert: layer_1[5.4-8.3% std=0.67]
  Expert: layer_2[5.7-6.7% std=0.23]
  Expert: layer_3[3.4-14.2% std=2.57]
  Expert: layer_4[4.6-7.6% std=0.81]
  Expert: layer_6[2.8-12.4% std=1.91]
  Expert: layer_7[1.6-17.2% std=3.24]
  Expert: layer_8[1.9-14.9% std=2.81]
  Expert: layer_9[3.3-12.2% std=1.87]
  Expert: layer_10[1.6-21.3% std=4.27]
  Expert: layer_12[0.9-23.7% std=6.08]
  Expert: layer_13[2.7-14.7% std=2.75]
  Expert: layer_14[0.6-37.2% std=8.23]
  Expert: layer_15[0.7-44.5% std=10.04]
  Expert: layer_16[0.7-22.4% std=4.84]
  Expert: layer_18[1.6-19.8% std=4.98]
  Expert: layer_19[0.7-29.0% std=6.31]
  Expert: layer_20[1.2-34.4% std=7.84]
  Expert: layer_21[1.2-27.6% std=6.39]
Step     125 | Loss: 5.0695 | LR: 7.81e-05 | GradNorm: 0.56 | Tok/s: 2610 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.94] | layer_5[5.1-7.2% std=0.56] | layer_11[2.1-17.5% std=3.47] | layer_17[2.6-15.8% std=3.34] | layer_22[1.5-32.3% std=7.29]
  Expert: layer_1[5.3-8.4% std=0.67]
  Expert: layer_2[5.3-6.7% std=0.31]
  Expert: layer_3[3.2-14.3% std=2.54]
  Expert: layer_4[4.5-7.5% std=0.84]
  Expert: layer_6[2.7-12.2% std=1.90]
  Expert: layer_7[1.7-17.5% std=3.31]
  Expert: layer_8[2.0-15.2% std=2.86]
  Expert: layer_9[3.2-12.3% std=1.92]
  Expert: layer_10[1.6-21.0% std=4.20]
  Expert: layer_12[0.9-23.7% std=5.98]
  Expert: layer_13[2.6-13.3% std=2.48]
  Expert: layer_14[0.6-37.4% std=8.25]
  Expert: layer_15[0.7-44.6% std=10.07]
  Expert: layer_16[0.7-21.8% std=4.67]
  Expert: layer_18[1.6-19.4% std=4.95]
  Expert: layer_19[0.6-28.9% std=6.33]
  Expert: layer_20[1.2-35.5% std=8.03]
  Expert: layer_21[1.1-27.3% std=6.27]
Step     126 | Loss: 5.0981 | LR: 7.88e-05 | GradNorm: 0.53 | Tok/s: 2587 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.3% std=0.96] | layer_5[5.3-7.5% std=0.53] | layer_11[1.9-17.8% std=3.60] | layer_17[2.9-15.1% std=3.22] | layer_22[1.2-30.0% std=6.86]
  Expert: layer_1[5.4-8.4% std=0.67]
  Expert: layer_2[5.2-6.8% std=0.34]
  Expert: layer_3[3.2-14.6% std=2.62]
  Expert: layer_4[4.5-7.5% std=0.83]
  Expert: layer_6[2.8-11.7% std=1.79]
  Expert: layer_7[1.8-17.4% std=3.26]
  Expert: layer_8[1.8-14.6% std=2.74]
  Expert: layer_9[3.2-12.8% std=2.06]
  Expert: layer_10[1.7-20.8% std=4.16]
  Expert: layer_12[1.0-24.4% std=6.08]
  Expert: layer_13[2.5-13.5% std=2.58]
  Expert: layer_14[0.6-37.2% std=8.20]
  Expert: layer_15[0.8-44.8% std=10.10]
  Expert: layer_16[0.6-22.8% std=4.94]
  Expert: layer_18[1.6-20.3% std=5.03]
  Expert: layer_19[0.6-28.7% std=6.28]
  Expert: layer_20[1.2-34.2% std=7.73]
  Expert: layer_21[1.1-26.1% std=6.18]
Step     127 | Loss: 5.0972 | LR: 7.94e-05 | GradNorm: 0.61 | Tok/s: 2610 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.93] | layer_5[5.3-7.5% std=0.55] | layer_11[1.9-18.1% std=3.67] | layer_17[3.0-15.1% std=3.22] | layer_22[1.2-30.6% std=7.08]
  Expert: layer_1[5.4-8.2% std=0.63]
  Expert: layer_2[5.3-7.0% std=0.37]
  Expert: layer_3[3.2-14.8% std=2.68]
  Expert: layer_4[4.7-7.6% std=0.80]
  Expert: layer_6[3.1-12.0% std=1.82]
  Expert: layer_7[1.9-17.3% std=3.24]
  Expert: layer_8[1.7-14.7% std=2.72]
  Expert: layer_9[3.1-12.6% std=2.07]
  Expert: layer_10[1.9-20.5% std=4.14]
  Expert: layer_12[1.0-24.3% std=6.09]
  Expert: layer_13[2.5-13.3% std=2.61]
  Expert: layer_14[0.5-37.6% std=8.29]
  Expert: layer_15[0.8-45.3% std=10.24]
  Expert: layer_16[0.5-25.0% std=5.36]
  Expert: layer_18[2.0-19.2% std=4.96]
  Expert: layer_19[0.7-29.3% std=6.45]
  Expert: layer_20[1.0-35.6% std=8.09]
  Expert: layer_21[0.7-26.1% std=6.14]
Step     128 | Loss: 5.0613 | LR: 8.00e-05 | GradNorm: 0.61 | Tok/s: 2610 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.3% std=0.91] | layer_5[5.0-7.3% std=0.58] | layer_11[2.0-18.7% std=3.79] | layer_17[2.9-14.3% std=2.86] | layer_22[1.4-29.9% std=6.82]
  Expert: layer_1[5.4-8.3% std=0.61]
  Expert: layer_2[5.2-7.0% std=0.38]
  Expert: layer_3[3.3-14.8% std=2.64]
  Expert: layer_4[4.7-7.6% std=0.76]
  Expert: layer_6[3.2-12.1% std=1.86]
  Expert: layer_7[2.0-17.2% std=3.20]
  Expert: layer_8[1.7-14.5% std=2.69]
  Expert: layer_9[3.0-12.1% std=2.00]
  Expert: layer_10[2.0-20.2% std=4.06]
  Expert: layer_12[1.0-22.6% std=5.74]
  Expert: layer_13[2.5-13.1% std=2.47]
  Expert: layer_14[0.6-36.8% std=8.07]
  Expert: layer_15[0.7-44.8% std=10.11]
  Expert: layer_16[0.6-23.3% std=5.02]
  Expert: layer_18[2.2-18.5% std=4.31]
  Expert: layer_19[0.7-27.4% std=5.92]
  Expert: layer_20[1.1-32.3% std=7.25]
  Expert: layer_21[0.9-24.8% std=5.80]
Step     129 | Loss: 5.0689 | LR: 8.06e-05 | GradNorm: 0.71 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.90] | layer_5[4.9-7.2% std=0.66] | layer_11[1.9-19.7% std=4.17] | layer_17[2.8-14.8% std=3.12] | layer_22[1.1-30.6% std=7.05]
  Expert: layer_1[5.3-8.3% std=0.61]
  Expert: layer_2[5.3-7.0% std=0.36]
  Expert: layer_3[3.2-14.2% std=2.54]
  Expert: layer_4[5.0-7.8% std=0.74]
  Expert: layer_6[3.2-12.3% std=1.87]
  Expert: layer_7[1.9-17.9% std=3.38]
  Expert: layer_8[1.7-14.8% std=2.82]
  Expert: layer_9[3.0-12.7% std=2.13]
  Expert: layer_10[1.9-22.0% std=4.46]
  Expert: layer_12[0.9-23.1% std=6.02]
  Expert: layer_13[2.2-13.5% std=2.63]
  Expert: layer_14[0.6-37.7% std=8.32]
  Expert: layer_15[0.6-45.4% std=10.24]
  Expert: layer_16[0.4-26.0% std=5.71]
  Expert: layer_18[1.9-18.7% std=4.67]
  Expert: layer_19[0.8-29.3% std=6.37]
  Expert: layer_20[1.2-33.6% std=7.61]
  Expert: layer_21[1.1-25.9% std=6.07]
Step     130 | Loss: 5.0246 | LR: 8.12e-05 | GradNorm: 0.77 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.88] | layer_5[5.1-7.1% std=0.53] | layer_11[2.3-17.6% std=3.61] | layer_17[2.6-14.0% std=2.86] | layer_22[1.5-28.9% std=6.62]
  Expert: layer_1[5.4-8.3% std=0.60]
  Expert: layer_2[5.3-7.0% std=0.38]
  Expert: layer_3[3.2-14.0% std=2.50]
  Expert: layer_4[4.9-7.5% std=0.69]
  Expert: layer_6[3.2-12.5% std=1.91]
  Expert: layer_7[2.0-17.8% std=3.31]
  Expert: layer_8[1.8-14.6% std=2.66]
  Expert: layer_9[3.3-11.6% std=1.82]
  Expert: layer_10[2.1-21.5% std=4.22]
  Expert: layer_12[1.1-22.1% std=5.75]
  Expert: layer_13[2.4-12.2% std=2.34]
  Expert: layer_14[0.7-35.5% std=7.77]
  Expert: layer_15[0.6-44.3% std=9.97]
  Expert: layer_16[0.6-21.9% std=4.71]
  Expert: layer_18[1.9-15.7% std=3.80]
  Expert: layer_19[1.2-27.5% std=5.88]
  Expert: layer_20[1.6-32.2% std=7.15]
  Expert: layer_21[1.3-24.8% std=5.62]
Step     131 | Loss: 5.0283 | LR: 8.19e-05 | GradNorm: 0.82 | Tok/s: 2583 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.89] | layer_5[5.0-6.9% std=0.52] | layer_11[2.1-18.6% std=3.87] | layer_17[2.4-13.8% std=2.99] | layer_22[1.2-26.2% std=6.40]
  Expert: layer_1[5.3-8.1% std=0.56]
  Expert: layer_2[5.4-6.9% std=0.37]
  Expert: layer_3[3.2-14.3% std=2.52]
  Expert: layer_4[4.7-7.9% std=0.77]
  Expert: layer_6[3.0-12.5% std=1.90]
  Expert: layer_7[1.9-17.4% std=3.20]
  Expert: layer_8[1.8-14.6% std=2.67]
  Expert: layer_9[3.0-11.8% std=1.87]
  Expert: layer_10[2.4-21.0% std=4.14]
  Expert: layer_12[1.0-22.9% std=5.79]
  Expert: layer_13[2.7-13.5% std=2.46]
  Expert: layer_14[0.7-34.3% std=7.50]
  Expert: layer_15[0.7-44.3% std=10.00]
  Expert: layer_16[0.3-24.4% std=5.37]
  Expert: layer_18[2.5-19.4% std=4.54]
  Expert: layer_19[1.5-27.3% std=5.85]
  Expert: layer_20[1.1-29.8% std=6.73]
  Expert: layer_21[1.4-24.7% std=5.83]
Step     132 | Loss: 5.0251 | LR: 8.25e-05 | GradNorm: 1.13 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.87] | layer_5[5.0-7.2% std=0.55] | layer_11[2.4-17.2% std=3.66] | layer_17[2.0-14.4% std=3.17] | layer_22[1.2-29.7% std=6.86]
  Expert: layer_1[5.3-8.2% std=0.60]
  Expert: layer_2[5.2-7.4% std=0.47]
  Expert: layer_3[3.0-14.2% std=2.44]
  Expert: layer_4[4.8-7.3% std=0.73]
  Expert: layer_6[3.2-12.4% std=1.86]
  Expert: layer_7[1.8-17.7% std=3.30]
  Expert: layer_8[1.8-14.7% std=2.75]
  Expert: layer_9[3.0-10.9% std=1.77]
  Expert: layer_10[2.0-21.6% std=4.31]
  Expert: layer_12[0.9-22.0% std=5.86]
  Expert: layer_13[2.6-12.4% std=2.28]
  Expert: layer_14[0.7-35.6% std=7.77]
  Expert: layer_15[0.4-43.8% std=9.84]
  Expert: layer_16[0.5-23.7% std=5.08]
  Expert: layer_18[1.9-18.3% std=4.67]
  Expert: layer_19[1.3-27.9% std=6.05]
  Expert: layer_20[1.4-36.2% std=8.12]
  Expert: layer_21[0.8-23.3% std=5.58]
Step     133 | Loss: 5.0793 | LR: 8.31e-05 | GradNorm: 1.15 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.91] | layer_5[5.2-7.6% std=0.61] | layer_11[2.4-17.2% std=3.51] | layer_17[2.9-12.7% std=2.69] | layer_22[1.7-23.4% std=5.72]
  Expert: layer_1[5.4-8.0% std=0.57]
  Expert: layer_2[5.3-7.5% std=0.45]
  Expert: layer_3[3.3-14.1% std=2.37]
  Expert: layer_4[5.0-7.3% std=0.71]
  Expert: layer_6[3.2-12.2% std=1.78]
  Expert: layer_7[1.9-16.4% std=2.93]
  Expert: layer_8[1.8-13.9% std=2.49]
  Expert: layer_9[3.0-10.7% std=1.56]
  Expert: layer_10[2.1-20.3% std=3.92]
  Expert: layer_12[1.0-22.2% std=5.60]
  Expert: layer_13[2.4-14.4% std=2.64]
  Expert: layer_14[1.0-33.4% std=7.26]
  Expert: layer_15[0.6-43.0% std=9.63]
  Expert: layer_16[0.5-22.4% std=5.02]
  Expert: layer_18[2.1-20.6% std=4.67]
  Expert: layer_19[1.6-25.6% std=5.36]
  Expert: layer_20[1.6-26.9% std=5.96]
  Expert: layer_21[1.5-21.9% std=5.34]
Step     134 | Loss: 5.0267 | LR: 8.38e-05 | GradNorm: 0.73 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.91] | layer_5[5.3-7.8% std=0.71] | layer_11[2.4-17.8% std=3.64] | layer_17[2.2-13.0% std=3.11] | layer_22[1.5-26.4% std=6.35]
  Expert: layer_1[5.4-8.0% std=0.57]
  Expert: layer_2[5.2-7.5% std=0.47]
  Expert: layer_3[3.4-14.3% std=2.46]
  Expert: layer_4[4.7-7.3% std=0.73]
  Expert: layer_6[3.4-12.1% std=1.73]
  Expert: layer_7[1.5-16.9% std=3.11]
  Expert: layer_8[1.8-14.9% std=2.73]
  Expert: layer_9[3.4-10.8% std=1.51]
  Expert: layer_10[1.6-20.7% std=4.09]
  Expert: layer_12[0.9-23.7% std=5.98]
  Expert: layer_13[2.6-14.3% std=2.67]
  Expert: layer_14[0.7-35.6% std=7.79]
  Expert: layer_15[0.6-44.0% std=9.86]
  Expert: layer_16[0.6-23.9% std=5.18]
  Expert: layer_18[2.1-20.2% std=4.99]
  Expert: layer_19[1.7-25.6% std=5.48]
  Expert: layer_20[1.3-31.9% std=7.10]
  Expert: layer_21[1.0-22.1% std=5.20]
Step     135 | Loss: 4.9840 | LR: 8.44e-05 | GradNorm: 0.82 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.93] | layer_5[5.2-7.6% std=0.68] | layer_11[2.2-16.2% std=3.31] | layer_17[2.2-14.4% std=3.42] | layer_22[1.3-28.5% std=6.75]
  Expert: layer_1[5.5-8.1% std=0.60]
  Expert: layer_2[5.2-7.4% std=0.48]
  Expert: layer_3[3.3-13.9% std=2.35]
  Expert: layer_4[4.6-7.2% std=0.75]
  Expert: layer_6[3.1-11.6% std=1.70]
  Expert: layer_7[1.4-17.5% std=3.29]
  Expert: layer_8[1.8-14.9% std=2.77]
  Expert: layer_9[3.7-10.6% std=1.49]
  Expert: layer_10[1.5-21.8% std=4.35]
  Expert: layer_12[0.9-23.5% std=6.04]
  Expert: layer_13[2.8-13.2% std=2.57]
  Expert: layer_14[0.6-36.4% std=7.98]
  Expert: layer_15[0.6-43.5% std=9.73]
  Expert: layer_16[0.7-24.4% std=5.18]
  Expert: layer_18[1.7-19.0% std=5.03]
  Expert: layer_19[1.5-26.1% std=5.76]
  Expert: layer_20[1.8-33.4% std=7.44]
  Expert: layer_21[0.6-19.4% std=4.66]
Step     136 | Loss: 5.0334 | LR: 8.50e-05 | GradNorm: 0.81 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.92] | layer_5[5.4-7.4% std=0.62] | layer_11[2.2-16.4% std=3.35] | layer_17[2.4-12.1% std=2.71] | layer_22[1.9-24.0% std=5.68]
  Expert: layer_1[5.4-8.0% std=0.58]
  Expert: layer_2[5.2-7.0% std=0.37]
  Expert: layer_3[3.5-13.9% std=2.35]
  Expert: layer_4[4.4-7.2% std=0.76]
  Expert: layer_6[3.0-11.5% std=1.67]
  Expert: layer_7[1.5-17.0% std=3.10]
  Expert: layer_8[1.9-14.7% std=2.66]
  Expert: layer_9[3.5-10.8% std=1.51]
  Expert: layer_10[1.7-20.6% std=4.02]
  Expert: layer_12[0.9-23.1% std=5.76]
  Expert: layer_13[2.7-14.3% std=2.72]
  Expert: layer_14[0.9-34.6% std=7.51]
  Expert: layer_15[0.8-43.3% std=9.70]
  Expert: layer_16[0.6-23.6% std=5.09]
  Expert: layer_18[1.9-20.3% std=4.81]
  Expert: layer_19[1.5-25.4% std=5.35]
  Expert: layer_20[1.7-27.8% std=6.10]
  Expert: layer_21[1.0-19.4% std=4.61]
Step     137 | Loss: 5.0078 | LR: 8.56e-05 | GradNorm: 0.68 | Tok/s: 2614 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.90] | layer_5[5.6-7.2% std=0.52] | layer_11[2.2-17.4% std=3.58] | layer_17[2.5-12.3% std=2.62] | layer_22[2.1-23.4% std=5.52]
  Expert: layer_1[5.5-8.0% std=0.55]
  Expert: layer_2[5.3-6.7% std=0.34]
  Expert: layer_3[3.5-14.3% std=2.42]
  Expert: layer_4[4.6-7.0% std=0.75]
  Expert: layer_6[3.0-11.6% std=1.68]
  Expert: layer_7[1.6-17.1% std=3.12]
  Expert: layer_8[2.0-15.6% std=2.80]
  Expert: layer_9[3.6-10.0% std=1.33]
  Expert: layer_10[1.7-19.8% std=3.84]
  Expert: layer_12[0.7-23.9% std=5.82]
  Expert: layer_13[2.6-13.9% std=2.59]
  Expert: layer_14[1.0-35.2% std=7.65]
  Expert: layer_15[0.8-44.1% std=9.91]
  Expert: layer_16[0.6-21.5% std=4.65]
  Expert: layer_18[2.2-19.1% std=4.56]
  Expert: layer_19[1.7-23.9% std=5.10]
  Expert: layer_20[1.3-28.9% std=6.38]
  Expert: layer_21[1.2-19.5% std=4.57]
Step     138 | Loss: 4.9097 | LR: 8.63e-05 | GradNorm: 0.69 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=0.92] | layer_5[5.5-7.3% std=0.51] | layer_11[2.2-16.4% std=3.42] | layer_17[2.6-12.9% std=2.67] | layer_22[2.1-24.9% std=5.69]
  Expert: layer_1[5.5-8.1% std=0.55]
  Expert: layer_2[5.5-6.7% std=0.35]
  Expert: layer_3[3.3-13.8% std=2.31]
  Expert: layer_4[4.6-7.4% std=0.78]
  Expert: layer_6[3.0-11.6% std=1.68]
  Expert: layer_7[1.4-18.7% std=3.51]
  Expert: layer_8[1.9-15.0% std=2.70]
  Expert: layer_9[3.6-10.1% std=1.45]
  Expert: layer_10[1.7-20.9% std=4.13]
  Expert: layer_12[0.7-23.7% std=5.85]
  Expert: layer_13[2.6-12.9% std=2.40]
  Expert: layer_14[0.8-36.8% std=8.05]
  Expert: layer_15[0.8-43.8% std=9.82]
  Expert: layer_16[0.7-22.1% std=4.65]
  Expert: layer_18[2.0-16.9% std=4.28]
  Expert: layer_19[1.4-24.0% std=5.25]
  Expert: layer_20[1.3-31.8% std=7.03]
  Expert: layer_21[0.8-19.1% std=4.47]
Step     139 | Loss: 4.9482 | LR: 8.69e-05 | GradNorm: 0.58 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.2% std=0.91] | layer_5[5.4-7.4% std=0.54] | layer_11[2.1-17.3% std=3.55] | layer_17[2.9-12.5% std=2.59] | layer_22[2.0-23.1% std=5.40]
  Expert: layer_1[5.5-8.2% std=0.58]
  Expert: layer_2[5.6-6.7% std=0.32]
  Expert: layer_3[3.3-14.1% std=2.39]
  Expert: layer_4[4.7-7.1% std=0.69]
  Expert: layer_6[2.9-11.6% std=1.70]
  Expert: layer_7[1.3-18.7% std=3.52]
  Expert: layer_8[1.9-14.8% std=2.69]
  Expert: layer_9[3.4-10.9% std=1.61]
  Expert: layer_10[1.8-21.2% std=4.22]
  Expert: layer_12[0.7-23.2% std=5.76]
  Expert: layer_13[2.8-13.1% std=2.42]
  Expert: layer_14[0.8-36.0% std=7.87]
  Expert: layer_15[0.8-43.6% std=9.81]
  Expert: layer_16[0.7-23.5% std=4.94]
  Expert: layer_18[2.1-18.0% std=4.24]
  Expert: layer_19[1.5-23.6% std=5.10]
  Expert: layer_20[1.7-28.3% std=6.20]
  Expert: layer_21[0.9-20.3% std=4.74]
Step     140 | Loss: 5.0088 | LR: 8.75e-05 | GradNorm: 0.75 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.2% std=0.91] | layer_5[5.5-7.5% std=0.60] | layer_11[2.1-18.0% std=3.68] | layer_17[3.0-10.7% std=2.41] | layer_22[1.4-22.4% std=5.29]
  Expert: layer_1[5.5-8.0% std=0.53]
  Expert: layer_2[5.5-7.0% std=0.38]
  Expert: layer_3[3.4-13.8% std=2.34]
  Expert: layer_4[4.6-7.2% std=0.68]
  Expert: layer_6[2.9-11.9% std=1.75]
  Expert: layer_7[1.2-18.5% std=3.48]
  Expert: layer_8[1.9-14.7% std=2.76]
  Expert: layer_9[3.4-11.1% std=1.65]
  Expert: layer_10[1.5-20.9% std=4.22]
  Expert: layer_12[0.8-23.8% std=5.89]
  Expert: layer_13[2.6-13.7% std=2.54]
  Expert: layer_14[0.8-35.6% std=7.78]
  Expert: layer_15[0.5-43.9% std=9.90]
  Expert: layer_16[0.7-22.7% std=4.79]
  Expert: layer_18[2.2-17.0% std=3.91]
  Expert: layer_19[1.3-23.6% std=5.08]
  Expert: layer_20[1.4-28.6% std=6.27]
  Expert: layer_21[1.0-21.0% std=4.84]
Step     141 | Loss: 4.9491 | LR: 8.81e-05 | GradNorm: 0.57 | Tok/s: 2594 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.89] | layer_5[5.5-7.2% std=0.47] | layer_11[2.2-17.1% std=3.52] | layer_17[2.8-11.1% std=2.36] | layer_22[1.7-22.5% std=5.19]
  Expert: layer_1[5.6-7.9% std=0.51]
  Expert: layer_2[5.6-7.0% std=0.37]
  Expert: layer_3[3.2-13.4% std=2.28]
  Expert: layer_4[4.6-7.2% std=0.67]
  Expert: layer_6[3.0-12.2% std=1.80]
  Expert: layer_7[1.3-18.1% std=3.39]
  Expert: layer_8[2.0-14.7% std=2.72]
  Expert: layer_9[3.2-10.9% std=1.64]
  Expert: layer_10[1.7-20.6% std=4.14]
  Expert: layer_12[0.8-22.9% std=5.71]
  Expert: layer_13[2.8-13.1% std=2.35]
  Expert: layer_14[0.8-35.5% std=7.72]
  Expert: layer_15[0.6-43.3% std=9.77]
  Expert: layer_16[0.7-20.9% std=4.38]
  Expert: layer_18[2.3-16.0% std=3.86]
  Expert: layer_19[1.1-23.0% std=4.96]
  Expert: layer_20[1.5-29.5% std=6.45]
  Expert: layer_21[1.2-19.2% std=4.33]
Step     142 | Loss: 4.9315 | LR: 8.88e-05 | GradNorm: 0.67 | Tok/s: 2608 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=0.90] | layer_5[5.5-7.1% std=0.45] | layer_11[2.2-17.0% std=3.56] | layer_17[2.5-11.7% std=2.54] | layer_22[1.6-23.1% std=5.49]
  Expert: layer_1[5.7-8.0% std=0.53]
  Expert: layer_2[5.6-7.0% std=0.41]
  Expert: layer_3[3.1-13.3% std=2.31]
  Expert: layer_4[4.6-7.0% std=0.62]
  Expert: layer_6[3.1-12.1% std=1.79]
  Expert: layer_7[1.3-18.2% std=3.42]
  Expert: layer_8[1.9-14.6% std=2.68]
  Expert: layer_9[3.2-10.8% std=1.68]
  Expert: layer_10[2.1-21.1% std=4.22]
  Expert: layer_12[0.7-22.8% std=5.70]
  Expert: layer_13[2.9-13.2% std=2.38]
  Expert: layer_14[0.8-35.8% std=7.82]
  Expert: layer_15[0.7-42.9% std=9.67]
  Expert: layer_16[0.5-22.7% std=4.75]
  Expert: layer_18[1.7-17.4% std=4.39]
  Expert: layer_19[0.9-23.5% std=5.06]
  Expert: layer_20[1.7-27.2% std=5.99]
  Expert: layer_21[1.1-18.9% std=4.44]
Step     143 | Loss: 5.0031 | LR: 8.94e-05 | GradNorm: 0.66 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.88] | layer_5[5.4-7.3% std=0.54] | layer_11[2.2-17.2% std=3.56] | layer_17[2.5-12.3% std=2.49] | layer_22[1.4-22.7% std=5.60]
  Expert: layer_1[5.6-7.9% std=0.51]
  Expert: layer_2[5.6-7.2% std=0.41]
  Expert: layer_3[3.3-14.0% std=2.45]
  Expert: layer_4[4.7-7.0% std=0.63]
  Expert: layer_6[3.2-12.3% std=1.78]
  Expert: layer_7[1.3-18.0% std=3.38]
  Expert: layer_8[1.9-14.6% std=2.68]
  Expert: layer_9[3.1-10.9% std=1.64]
  Expert: layer_10[2.2-20.2% std=3.92]
  Expert: layer_12[0.9-22.7% std=5.72]
  Expert: layer_13[2.6-13.5% std=2.45]
  Expert: layer_14[1.1-34.9% std=7.60]
  Expert: layer_15[0.7-42.2% std=9.52]
  Expert: layer_16[0.6-23.3% std=4.89]
  Expert: layer_18[1.9-16.4% std=4.11]
  Expert: layer_19[1.2-22.5% std=4.74]
  Expert: layer_20[1.7-26.6% std=5.80]
  Expert: layer_21[1.1-17.9% std=4.48]
Step     144 | Loss: 4.9057 | LR: 9.00e-05 | GradNorm: 0.60 | Tok/s: 2609 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.87] | layer_5[5.3-7.2% std=0.52] | layer_11[2.1-18.2% std=3.76] | layer_17[2.3-12.3% std=2.51] | layer_22[1.6-24.7% std=5.92]
  Expert: layer_1[5.8-8.0% std=0.50]
  Expert: layer_2[5.7-7.1% std=0.38]
  Expert: layer_3[3.3-14.4% std=2.57]
  Expert: layer_4[4.5-7.1% std=0.69]
  Expert: layer_6[3.2-12.1% std=1.73]
  Expert: layer_7[1.3-17.9% std=3.37]
  Expert: layer_8[1.8-15.4% std=2.82]
  Expert: layer_9[3.0-10.6% std=1.70]
  Expert: layer_10[2.1-20.1% std=3.96]
  Expert: layer_12[0.8-23.3% std=5.86]
  Expert: layer_13[2.6-13.4% std=2.42]
  Expert: layer_14[1.0-35.1% std=7.65]
  Expert: layer_15[0.7-43.2% std=9.74]
  Expert: layer_16[0.7-24.0% std=5.08]
  Expert: layer_18[2.3-17.2% std=4.18]
  Expert: layer_19[1.4-23.7% std=4.99]
  Expert: layer_20[1.4-29.5% std=6.45]
  Expert: layer_21[1.1-18.2% std=4.54]
Step     145 | Loss: 4.9250 | LR: 9.06e-05 | GradNorm: 0.61 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.90] | layer_5[5.3-7.2% std=0.57] | layer_11[2.1-17.4% std=3.66] | layer_17[2.1-11.3% std=2.30] | layer_22[1.8-24.1% std=5.59]
  Expert: layer_1[5.7-8.1% std=0.54]
  Expert: layer_2[5.7-7.1% std=0.38]
  Expert: layer_3[3.2-15.1% std=2.69]
  Expert: layer_4[4.3-7.0% std=0.67]
  Expert: layer_6[3.2-11.8% std=1.71]
  Expert: layer_7[1.4-18.7% std=3.52]
  Expert: layer_8[1.6-15.1% std=2.75]
  Expert: layer_9[3.1-10.4% std=1.69]
  Expert: layer_10[2.1-20.6% std=4.04]
  Expert: layer_12[1.0-22.7% std=5.80]
  Expert: layer_13[2.7-12.9% std=2.31]
  Expert: layer_14[1.1-34.9% std=7.59]
  Expert: layer_15[0.7-42.8% std=9.64]
  Expert: layer_16[0.9-21.9% std=4.56]
  Expert: layer_18[2.0-16.1% std=4.05]
  Expert: layer_19[1.7-22.2% std=4.60]
  Expert: layer_20[1.8-28.7% std=6.22]
  Expert: layer_21[1.3-16.2% std=4.03]
Step     146 | Loss: 4.9681 | LR: 9.12e-05 | GradNorm: 0.62 | Tok/s: 2585 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.87] | layer_5[5.0-7.2% std=0.65] | layer_11[2.0-17.2% std=3.68] | layer_17[2.1-11.3% std=2.35] | layer_22[1.9-23.2% std=5.47]
  Expert: layer_1[5.7-8.0% std=0.51]
  Expert: layer_2[5.5-7.4% std=0.46]
  Expert: layer_3[3.2-15.7% std=2.79]
  Expert: layer_4[4.4-7.1% std=0.64]
  Expert: layer_6[3.2-11.9% std=1.75]
  Expert: layer_7[1.3-18.9% std=3.57]
  Expert: layer_8[1.7-14.6% std=2.64]
  Expert: layer_9[3.1-10.4% std=1.66]
  Expert: layer_10[2.1-20.7% std=4.05]
  Expert: layer_12[1.0-23.1% std=5.88]
  Expert: layer_13[2.7-12.9% std=2.30]
  Expert: layer_14[1.0-35.4% std=7.70]
  Expert: layer_15[0.8-43.0% std=9.69]
  Expert: layer_16[1.0-21.6% std=4.49]
  Expert: layer_18[2.1-16.4% std=4.21]
  Expert: layer_19[1.9-21.2% std=4.38]
  Expert: layer_20[1.7-29.3% std=6.40]
  Expert: layer_21[1.6-16.3% std=4.16]
Step     147 | Loss: 4.8817 | LR: 9.19e-05 | GradNorm: 0.73 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.87] | layer_5[5.0-7.2% std=0.65] | layer_11[1.7-18.7% std=3.95] | layer_17[2.5-10.8% std=2.22] | layer_22[1.4-23.0% std=5.55]
  Expert: layer_1[5.5-8.0% std=0.51]
  Expert: layer_2[5.5-7.3% std=0.43]
  Expert: layer_3[3.3-16.0% std=2.89]
  Expert: layer_4[4.1-7.3% std=0.78]
  Expert: layer_6[3.2-11.9% std=1.77]
  Expert: layer_7[1.3-19.0% std=3.59]
  Expert: layer_8[1.8-14.8% std=2.69]
  Expert: layer_9[3.1-10.8% std=1.69]
  Expert: layer_10[2.2-21.1% std=4.19]
  Expert: layer_12[0.8-24.0% std=5.85]
  Expert: layer_13[2.7-13.7% std=2.45]
  Expert: layer_14[0.9-35.5% std=7.75]
  Expert: layer_15[0.8-43.5% std=9.83]
  Expert: layer_16[0.9-22.1% std=4.65]
  Expert: layer_18[2.3-18.0% std=4.52]
  Expert: layer_19[1.9-21.9% std=4.58]
  Expert: layer_20[1.0-28.8% std=6.30]
  Expert: layer_21[1.8-18.5% std=4.53]
Step     148 | Loss: 4.9308 | LR: 9.25e-05 | GradNorm: 0.71 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.87] | layer_5[4.9-7.1% std=0.63] | layer_11[1.9-16.8% std=3.52] | layer_17[2.8-11.3% std=2.05] | layer_22[2.0-21.2% std=4.88]
  Expert: layer_1[5.5-8.0% std=0.54]
  Expert: layer_2[5.4-7.2% std=0.48]
  Expert: layer_3[3.2-15.7% std=2.79]
  Expert: layer_4[4.0-7.1% std=0.87]
  Expert: layer_6[3.3-12.0% std=1.75]
  Expert: layer_7[1.4-18.7% std=3.50]
  Expert: layer_8[1.9-14.3% std=2.48]
  Expert: layer_9[3.1-10.6% std=1.57]
  Expert: layer_10[2.4-21.4% std=4.14]
  Expert: layer_12[1.0-23.1% std=5.61]
  Expert: layer_13[2.4-13.5% std=2.39]
  Expert: layer_14[1.0-34.7% std=7.55]
  Expert: layer_15[0.8-42.1% std=9.43]
  Expert: layer_16[1.1-18.7% std=3.82]
  Expert: layer_18[2.4-15.8% std=3.85]
  Expert: layer_19[2.0-20.3% std=4.11]
  Expert: layer_20[1.4-28.0% std=6.02]
  Expert: layer_21[1.8-15.6% std=3.77]
Step     149 | Loss: 4.8408 | LR: 9.31e-05 | GradNorm: 0.63 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.8% std=0.84] | layer_5[5.0-7.0% std=0.59] | layer_11[2.1-18.1% std=3.79] | layer_17[2.7-12.3% std=2.26] | layer_22[2.2-20.7% std=4.98]
  Expert: layer_1[5.5-8.0% std=0.54]
  Expert: layer_2[5.5-7.2% std=0.46]
  Expert: layer_3[3.1-15.2% std=2.67]
  Expert: layer_4[4.1-7.2% std=0.84]
  Expert: layer_6[3.1-12.0% std=1.78]
  Expert: layer_7[1.7-17.9% std=3.30]
  Expert: layer_8[2.1-15.2% std=2.65]
  Expert: layer_9[3.1-11.2% std=1.64]
  Expert: layer_10[2.5-21.5% std=4.20]
  Expert: layer_12[0.8-23.4% std=5.73]
  Expert: layer_13[2.5-13.8% std=2.41]
  Expert: layer_14[1.0-35.1% std=7.62]
  Expert: layer_15[0.9-42.3% std=9.48]
  Expert: layer_16[1.1-20.4% std=4.20]
  Expert: layer_18[2.5-19.0% std=4.89]
  Expert: layer_19[2.0-20.8% std=4.26]
  Expert: layer_20[1.2-26.2% std=5.68]
  Expert: layer_21[2.1-15.2% std=3.77]
Step     150 | Loss: 4.8959 | LR: 9.38e-05 | GradNorm: 0.61 | Tok/s: 2596 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.90] | layer_5[5.1-7.2% std=0.62] | layer_11[1.8-17.8% std=3.77] | layer_17[2.8-11.0% std=2.16] | layer_22[2.2-21.4% std=4.89]
  Expert: layer_1[5.7-8.0% std=0.53]
  Expert: layer_2[5.5-7.2% std=0.45]
  Expert: layer_3[3.1-15.1% std=2.68]
  Expert: layer_4[4.1-7.2% std=0.88]
  Expert: layer_6[3.1-12.0% std=1.78]
  Expert: layer_7[1.6-18.7% std=3.48]
  Expert: layer_8[1.9-15.1% std=2.73]
  Expert: layer_9[3.1-11.4% std=1.73]
  Expert: layer_10[2.1-22.2% std=4.42]
  Expert: layer_12[0.9-24.5% std=5.86]
  Expert: layer_13[2.1-13.6% std=2.46]
  Expert: layer_14[0.9-36.4% std=7.96]
  Expert: layer_15[1.0-42.3% std=9.45]
  Expert: layer_16[1.0-21.4% std=4.40]
  Expert: layer_18[2.4-17.7% std=4.40]
  Expert: layer_19[2.3-21.3% std=4.46]
  Expert: layer_20[1.3-28.1% std=6.04]
  Expert: layer_21[1.6-15.9% std=3.74]
Step     151 | Loss: 4.8385 | LR: 9.44e-05 | GradNorm: 0.58 | Tok/s: 2579 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.87] | layer_5[5.3-7.0% std=0.49] | layer_11[2.2-17.2% std=3.60] | layer_17[3.3-10.6% std=1.99] | layer_22[2.6-19.0% std=4.43]
  Expert: layer_1[5.7-8.0% std=0.51]
  Expert: layer_2[5.6-7.2% std=0.43]
  Expert: layer_3[3.1-14.6% std=2.54]
  Expert: layer_4[4.1-7.3% std=0.85]
  Expert: layer_6[3.2-11.6% std=1.72]
  Expert: layer_7[1.9-17.9% std=3.26]
  Expert: layer_8[2.2-14.7% std=2.58]
  Expert: layer_9[3.1-11.3% std=1.68]
  Expert: layer_10[2.4-21.4% std=4.19]
  Expert: layer_12[0.9-23.8% std=5.76]
  Expert: layer_13[2.4-13.5% std=2.40]
  Expert: layer_14[1.1-35.2% std=7.66]
  Expert: layer_15[1.1-41.4% std=9.23]
  Expert: layer_16[1.1-21.6% std=4.41]
  Expert: layer_18[2.4-17.2% std=4.13]
  Expert: layer_19[1.9-20.4% std=4.13]
  Expert: layer_20[2.0-24.3% std=5.08]
  Expert: layer_21[2.4-15.2% std=3.54]
Step     152 | Loss: 4.9067 | LR: 9.50e-05 | GradNorm: 0.57 | Tok/s: 2597 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.88] | layer_5[5.2-7.1% std=0.49] | layer_11[2.1-19.1% std=3.90] | layer_17[3.5-10.4% std=1.90] | layer_22[2.2-18.1% std=4.52]
  Expert: layer_1[5.7-8.0% std=0.52]
  Expert: layer_2[5.6-7.2% std=0.40]
  Expert: layer_3[3.4-14.9% std=2.56]
  Expert: layer_4[4.2-7.4% std=0.88]
  Expert: layer_6[3.3-11.7% std=1.74]
  Expert: layer_7[2.1-17.6% std=3.19]
  Expert: layer_8[2.0-15.8% std=2.81]
  Expert: layer_9[3.5-11.2% std=1.64]
  Expert: layer_10[2.3-21.2% std=4.18]
  Expert: layer_12[0.8-24.2% std=5.79]
  Expert: layer_13[2.3-13.6% std=2.38]
  Expert: layer_14[1.0-34.8% std=7.56]
  Expert: layer_15[1.1-42.0% std=9.40]
  Expert: layer_16[0.9-23.1% std=4.80]
  Expert: layer_18[2.1-18.3% std=4.20]
  Expert: layer_19[1.7-19.1% std=3.98]
  Expert: layer_20[2.1-22.1% std=4.63]
  Expert: layer_21[2.2-14.0% std=3.64]
Step     153 | Loss: 4.8504 | LR: 9.56e-05 | GradNorm: 0.63 | Tok/s: 2577 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.84] | layer_5[5.2-7.3% std=0.45] | layer_11[2.2-18.5% std=3.79] | layer_17[3.4-10.2% std=2.01] | layer_22[2.0-20.0% std=4.71]
  Expert: layer_1[5.6-7.9% std=0.52]
  Expert: layer_2[5.7-7.0% std=0.34]
  Expert: layer_3[3.4-14.8% std=2.56]
  Expert: layer_4[4.4-7.4% std=0.89]
  Expert: layer_6[3.2-12.2% std=1.84]
  Expert: layer_7[2.1-18.1% std=3.33]
  Expert: layer_8[2.0-17.0% std=3.12]
  Expert: layer_9[3.4-11.0% std=1.66]
  Expert: layer_10[2.1-21.8% std=4.34]
  Expert: layer_12[0.8-25.7% std=6.14]
  Expert: layer_13[2.1-13.6% std=2.49]
  Expert: layer_14[0.8-36.4% std=7.93]
  Expert: layer_15[1.2-43.6% std=9.78]
  Expert: layer_16[1.0-24.6% std=5.15]
  Expert: layer_18[2.4-18.0% std=4.30]
  Expert: layer_19[1.6-20.9% std=4.37]
  Expert: layer_20[1.9-26.9% std=5.69]
  Expert: layer_21[1.6-14.1% std=3.80]
Step     154 | Loss: 4.8304 | LR: 9.62e-05 | GradNorm: 0.60 | Tok/s: 2586 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.85] | layer_5[5.4-7.1% std=0.46] | layer_11[2.2-17.7% std=3.56] | layer_17[3.4-10.5% std=1.98] | layer_22[1.9-17.5% std=4.24]
  Expert: layer_1[5.5-7.7% std=0.49]
  Expert: layer_2[5.8-7.1% std=0.29]
  Expert: layer_3[3.5-14.8% std=2.54]
  Expert: layer_4[4.4-7.5% std=0.92]
  Expert: layer_6[3.0-12.1% std=1.84]
  Expert: layer_7[2.1-18.0% std=3.30]
  Expert: layer_8[2.0-16.8% std=3.09]
  Expert: layer_9[3.2-10.5% std=1.58]
  Expert: layer_10[2.3-20.5% std=4.02]
  Expert: layer_12[0.9-23.8% std=5.65]
  Expert: layer_13[2.3-14.3% std=2.54]
  Expert: layer_14[0.9-34.1% std=7.38]
  Expert: layer_15[1.1-42.6% std=9.54]
  Expert: layer_16[1.2-23.6% std=4.94]
  Expert: layer_18[2.6-18.4% std=4.03]
  Expert: layer_19[1.8-19.0% std=3.87]
  Expert: layer_20[2.4-21.7% std=4.47]
  Expert: layer_21[1.9-13.5% std=3.58]
Step     155 | Loss: 4.8803 | LR: 9.69e-05 | GradNorm: 0.60 | Tok/s: 2582 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.85] | layer_5[5.3-7.3% std=0.49] | layer_11[2.3-16.6% std=3.34] | layer_17[3.3-10.0% std=2.07] | layer_22[2.3-17.1% std=4.13]
  Expert: layer_1[5.5-7.8% std=0.52]
  Expert: layer_2[5.8-7.3% std=0.30]
  Expert: layer_3[3.5-14.9% std=2.56]
  Expert: layer_4[4.4-7.5% std=0.95]
  Expert: layer_6[3.1-12.1% std=1.78]
  Expert: layer_7[2.2-17.7% std=3.21]
  Expert: layer_8[1.7-16.4% std=3.01]
  Expert: layer_9[3.3-9.9% std=1.43]
  Expert: layer_10[2.6-20.5% std=4.00]
  Expert: layer_12[0.9-22.4% std=5.42]
  Expert: layer_13[2.5-13.8% std=2.40]
  Expert: layer_14[1.1-34.7% std=7.51]
  Expert: layer_15[0.9-42.7% std=9.52]
  Expert: layer_16[1.1-22.3% std=4.59]
  Expert: layer_18[2.1-15.4% std=3.73]
  Expert: layer_19[1.8-18.0% std=3.70]
  Expert: layer_20[2.4-22.7% std=4.68]
  Expert: layer_21[1.9-13.0% std=3.50]
Step     156 | Loss: 4.8437 | LR: 9.75e-05 | GradNorm: 0.66 | Tok/s: 2586 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.86] | layer_5[5.2-7.3% std=0.59] | layer_11[2.3-17.0% std=3.50] | layer_17[3.6-9.8% std=2.07] | layer_22[2.3-17.7% std=4.24]
  Expert: layer_1[5.4-7.7% std=0.50]
  Expert: layer_2[5.5-7.3% std=0.34]
  Expert: layer_3[3.6-14.6% std=2.50]
  Expert: layer_4[4.4-7.6% std=0.94]
  Expert: layer_6[3.0-12.2% std=1.78]
  Expert: layer_7[2.0-18.3% std=3.37]
  Expert: layer_8[1.6-16.4% std=3.07]
  Expert: layer_9[2.9-10.7% std=1.62]
  Expert: layer_10[2.4-21.5% std=4.25]
  Expert: layer_12[0.8-22.4% std=5.50]
  Expert: layer_13[2.9-14.5% std=2.49]
  Expert: layer_14[1.1-35.7% std=7.78]
  Expert: layer_15[0.9-42.9% std=9.58]
  Expert: layer_16[0.9-23.8% std=4.99]
  Expert: layer_18[1.9-16.8% std=4.23]
  Expert: layer_19[2.2-20.1% std=4.09]
  Expert: layer_20[2.7-22.6% std=4.80]
  Expert: layer_21[2.0-13.8% std=3.73]
Step     157 | Loss: 4.8258 | LR: 9.81e-05 | GradNorm: 0.85 | Tok/s: 2607 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.8% std=0.87] | layer_5[5.4-7.5% std=0.60] | layer_11[2.2-17.0% std=3.43] | layer_17[3.9-9.3% std=1.74] | layer_22[2.5-17.5% std=4.03]
  Expert: layer_1[5.4-7.9% std=0.55]
  Expert: layer_2[5.5-7.2% std=0.34]
  Expert: layer_3[3.3-14.6% std=2.51]
  Expert: layer_4[4.6-7.6% std=0.87]
  Expert: layer_6[3.0-12.2% std=1.78]
  Expert: layer_7[1.9-18.2% std=3.36]
  Expert: layer_8[1.8-16.1% std=3.00]
  Expert: layer_9[2.9-10.8% std=1.59]
  Expert: layer_10[2.5-19.7% std=3.77]
  Expert: layer_12[0.9-22.3% std=5.34]
  Expert: layer_13[2.7-14.2% std=2.45]
  Expert: layer_14[1.2-34.6% std=7.50]
  Expert: layer_15[1.0-41.3% std=9.21]
  Expert: layer_16[1.1-20.8% std=4.33]
  Expert: layer_18[2.2-15.6% std=3.64]
  Expert: layer_19[2.9-17.9% std=3.54]
  Expert: layer_20[2.5-23.3% std=4.89]
  Expert: layer_21[2.5-14.4% std=3.60]
Step     158 | Loss: 4.8328 | LR: 9.88e-05 | GradNorm: 1.11 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.87] | layer_5[5.4-7.8% std=0.65] | layer_11[2.4-17.8% std=3.65] | layer_17[3.3-10.1% std=1.93] | layer_22[2.6-15.9% std=3.93]
  Expert: layer_1[5.2-7.9% std=0.57]
  Expert: layer_2[5.5-7.1% std=0.31]
  Expert: layer_3[3.4-14.6% std=2.49]
  Expert: layer_4[4.3-7.4% std=0.83]
  Expert: layer_6[3.0-12.0% std=1.76]
  Expert: layer_7[2.1-18.8% std=3.47]
  Expert: layer_8[1.6-15.1% std=2.78]
  Expert: layer_9[2.7-11.4% std=1.71]
  Expert: layer_10[2.4-19.6% std=3.74]
  Expert: layer_12[1.0-22.4% std=5.42]
  Expert: layer_13[3.1-15.8% std=2.78]
  Expert: layer_14[1.4-32.5% std=7.01]
  Expert: layer_15[1.2-40.1% std=8.91]
  Expert: layer_16[1.1-21.8% std=4.54]
  Expert: layer_18[2.3-17.2% std=3.87]
  Expert: layer_19[2.9-17.9% std=3.48]
  Expert: layer_20[2.6-21.2% std=4.49]
  Expert: layer_21[2.2-12.8% std=3.47]
Step     159 | Loss: 4.8412 | LR: 9.94e-05 | GradNorm: 0.90 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-7.8% std=0.88] | layer_5[5.4-7.4% std=0.56] | layer_11[2.5-17.9% std=3.56] | layer_17[3.4-11.9% std=2.09] | layer_22[1.9-17.4% std=4.12]
  Expert: layer_1[5.4-7.9% std=0.56]
  Expert: layer_2[5.4-7.1% std=0.37]
  Expert: layer_3[3.3-14.9% std=2.57]
  Expert: layer_4[4.5-7.6% std=0.77]
  Expert: layer_6[3.2-12.0% std=1.74]
  Expert: layer_7[2.1-19.2% std=3.57]
  Expert: layer_8[1.5-15.4% std=2.88]
  Expert: layer_9[2.8-11.1% std=1.57]
  Expert: layer_10[2.3-19.4% std=3.70]
  Expert: layer_12[0.8-22.7% std=5.43]
  Expert: layer_13[2.8-14.1% std=2.42]
  Expert: layer_14[1.3-34.3% std=7.42]
  Expert: layer_15[1.1-41.1% std=9.14]
  Expert: layer_16[1.1-19.5% std=4.05]
  Expert: layer_18[2.3-15.8% std=3.75]
  Expert: layer_19[2.9-18.4% std=3.64]
  Expert: layer_20[1.6-27.7% std=5.93]
  Expert: layer_21[2.7-14.0% std=3.60]
Step     160 | Loss: 4.8009 | LR: 1.00e-04 | GradNorm: 0.60 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.87] | layer_5[5.5-7.3% std=0.54] | layer_11[2.5-17.5% std=3.42] | layer_17[2.9-10.3% std=1.96] | layer_22[1.7-16.3% std=3.98]
  Expert: layer_1[5.6-8.0% std=0.55]
  Expert: layer_2[5.4-7.3% std=0.44]
  Expert: layer_3[3.4-14.9% std=2.57]
  Expert: layer_4[4.4-7.7% std=0.81]
  Expert: layer_6[3.3-11.9% std=1.72]
  Expert: layer_7[2.0-19.1% std=3.55]
  Expert: layer_8[1.6-15.8% std=2.93]
  Expert: layer_9[2.9-11.2% std=1.65]
  Expert: layer_10[2.2-19.3% std=3.69]
  Expert: layer_12[0.9-23.2% std=5.49]
  Expert: layer_13[3.0-14.2% std=2.39]
  Expert: layer_14[1.0-33.3% std=7.18]
  Expert: layer_15[1.3-40.9% std=9.11]
  Expert: layer_16[1.2-21.2% std=4.42]
  Expert: layer_18[2.0-16.0% std=3.69]
  Expert: layer_19[2.2-18.1% std=3.62]
  Expert: layer_20[1.6-25.1% std=5.31]
  Expert: layer_21[2.4-14.3% std=3.62]
Step     161 | Loss: 4.7584 | LR: 1.01e-04 | GradNorm: 0.68 | Tok/s: 2585 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.89] | layer_5[5.6-7.1% std=0.44] | layer_11[2.5-16.5% std=3.25] | layer_17[2.9-9.8% std=1.93] | layer_22[2.1-15.2% std=3.65]
  Expert: layer_1[5.5-8.0% std=0.54]
  Expert: layer_2[5.5-7.1% std=0.39]
  Expert: layer_3[3.6-14.9% std=2.55]
  Expert: layer_4[4.4-7.7% std=0.77]
  Expert: layer_6[3.4-11.9% std=1.72]
  Expert: layer_7[1.9-18.5% std=3.40]
  Expert: layer_8[1.7-15.8% std=2.88]
  Expert: layer_9[2.9-11.1% std=1.61]
  Expert: layer_10[2.6-19.3% std=3.64]
  Expert: layer_12[0.9-23.0% std=5.49]
  Expert: layer_13[2.7-14.0% std=2.38]
  Expert: layer_14[1.0-33.2% std=7.19]
  Expert: layer_15[1.4-40.6% std=9.01]
  Expert: layer_16[1.1-21.7% std=4.46]
  Expert: layer_18[2.4-15.2% std=3.55]
  Expert: layer_19[2.1-16.8% std=3.39]
  Expert: layer_20[2.0-24.3% std=5.06]
  Expert: layer_21[2.2-15.0% std=3.55]
Step     162 | Loss: 4.8365 | LR: 1.01e-04 | GradNorm: 0.68 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.87] | layer_5[5.6-7.0% std=0.42] | layer_11[2.3-17.8% std=3.53] | layer_17[2.8-10.6% std=2.02] | layer_22[2.1-15.0% std=3.74]
  Expert: layer_1[5.6-8.0% std=0.56]
  Expert: layer_2[5.6-7.2% std=0.36]
  Expert: layer_3[3.6-15.2% std=2.59]
  Expert: layer_4[4.6-8.1% std=0.80]
  Expert: layer_6[3.6-12.0% std=1.71]
  Expert: layer_7[1.9-18.5% std=3.40]
  Expert: layer_8[1.5-15.7% std=2.91]
  Expert: layer_9[3.3-11.1% std=1.56]
  Expert: layer_10[2.6-20.4% std=3.92]
  Expert: layer_12[1.0-22.2% std=5.36]
  Expert: layer_13[2.9-13.3% std=2.28]
  Expert: layer_14[1.3-34.1% std=7.38]
  Expert: layer_15[1.1-41.2% std=9.19]
  Expert: layer_16[0.9-21.2% std=4.39]
  Expert: layer_18[2.6-16.7% std=4.01]
  Expert: layer_19[2.2-17.7% std=3.63]
  Expert: layer_20[1.7-21.9% std=4.49]
  Expert: layer_21[2.8-13.7% std=3.61]
Step     163 | Loss: 4.7753 | LR: 1.02e-04 | GradNorm: 0.81 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-7.9% std=0.89] | layer_5[5.7-7.1% std=0.42] | layer_11[2.3-16.0% std=3.18] | layer_17[2.9-9.9% std=1.81] | layer_22[2.2-16.1% std=3.64]
  Expert: layer_1[5.6-8.0% std=0.56]
  Expert: layer_2[5.7-7.2% std=0.35]
  Expert: layer_3[3.5-14.8% std=2.52]
  Expert: layer_4[4.2-8.0% std=0.83]
  Expert: layer_6[3.7-11.9% std=1.67]
  Expert: layer_7[1.8-19.3% std=3.60]
  Expert: layer_8[1.5-15.4% std=2.84]
  Expert: layer_9[3.5-11.4% std=1.64]
  Expert: layer_10[2.4-21.7% std=4.23]
  Expert: layer_12[1.1-22.9% std=5.46]
  Expert: layer_13[2.6-12.4% std=2.11]
  Expert: layer_14[1.1-34.6% std=7.54]
  Expert: layer_15[1.2-40.3% std=8.92]
  Expert: layer_16[1.2-20.5% std=4.20]
  Expert: layer_18[2.4-16.4% std=3.64]
  Expert: layer_19[2.3-18.6% std=3.82]
  Expert: layer_20[1.9-24.2% std=5.02]
  Expert: layer_21[2.0-15.6% std=3.46]
Step     164 | Loss: 4.7609 | LR: 1.02e-04 | GradNorm: 0.97 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.89] | layer_5[5.7-7.2% std=0.37] | layer_11[2.2-17.2% std=3.38] | layer_17[2.8-8.9% std=1.77] | layer_22[1.9-16.5% std=3.72]
  Expert: layer_1[5.5-8.0% std=0.58]
  Expert: layer_2[5.9-6.9% std=0.28]
  Expert: layer_3[3.5-14.7% std=2.50]
  Expert: layer_4[4.2-7.8% std=0.83]
  Expert: layer_6[3.4-11.8% std=1.68]
  Expert: layer_7[2.0-18.2% std=3.30]
  Expert: layer_8[1.5-15.6% std=2.86]
  Expert: layer_9[3.5-11.9% std=1.75]
  Expert: layer_10[2.5-21.1% std=4.09]
  Expert: layer_12[1.0-22.6% std=5.32]
  Expert: layer_13[3.0-14.6% std=2.51]
  Expert: layer_14[1.1-31.8% std=6.84]
  Expert: layer_15[1.4-40.2% std=9.04]
  Expert: layer_16[1.0-22.4% std=4.78]
  Expert: layer_18[2.8-16.1% std=3.80]
  Expert: layer_19[2.6-17.1% std=3.34]
  Expert: layer_20[1.4-18.2% std=3.90]
  Expert: layer_21[2.6-12.1% std=3.49]
Step     165 | Loss: 4.7582 | LR: 1.03e-04 | GradNorm: 1.08 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.0% std=0.88] | layer_5[5.7-7.0% std=0.34] | layer_11[2.7-14.5% std=2.89] | layer_17[2.5-10.1% std=1.94] | layer_22[2.4-14.8% std=3.35]
  Expert: layer_1[5.4-8.1% std=0.59]
  Expert: layer_2[5.9-6.8% std=0.25]
  Expert: layer_3[3.5-13.7% std=2.23]
  Expert: layer_4[4.3-7.2% std=0.73]
  Expert: layer_6[3.3-11.8% std=1.69]
  Expert: layer_7[2.1-18.3% std=3.31]
  Expert: layer_8[1.6-15.8% std=2.91]
  Expert: layer_9[3.6-10.7% std=1.48]
  Expert: layer_10[2.3-21.6% std=4.18]
  Expert: layer_12[1.2-22.2% std=5.27]
  Expert: layer_13[2.5-13.0% std=2.37]
  Expert: layer_14[1.2-33.7% std=7.29]
  Expert: layer_15[0.9-39.9% std=8.81]
  Expert: layer_16[1.2-19.5% std=3.98]
  Expert: layer_18[2.5-14.1% std=3.00]
  Expert: layer_19[1.7-15.7% std=3.59]
  Expert: layer_20[2.7-25.3% std=5.20]
  Expert: layer_21[2.3-15.1% std=3.14]
Step     166 | Loss: 4.7111 | LR: 1.04e-04 | GradNorm: 0.69 | Tok/s: 2584 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.1% std=0.87] | layer_5[5.7-6.8% std=0.33] | layer_11[2.7-16.7% std=3.25] | layer_17[2.5-9.9% std=1.94] | layer_22[3.0-14.3% std=3.30]
  Expert: layer_1[5.3-8.1% std=0.61]
  Expert: layer_2[5.8-7.1% std=0.31]
  Expert: layer_3[3.6-13.6% std=2.20]
  Expert: layer_4[4.4-7.4% std=0.75]
  Expert: layer_6[3.4-11.9% std=1.70]
  Expert: layer_7[2.5-17.4% std=3.10]
  Expert: layer_8[1.9-16.7% std=3.02]
  Expert: layer_9[3.3-10.9% std=1.60]
  Expert: layer_10[2.5-20.7% std=3.99]
  Expert: layer_12[0.8-21.7% std=5.08]
  Expert: layer_13[3.1-14.0% std=2.48]
  Expert: layer_14[1.2-31.7% std=6.77]
  Expert: layer_15[1.4-39.7% std=8.80]
  Expert: layer_16[1.3-20.9% std=4.38]
  Expert: layer_18[2.6-14.5% std=3.38]
  Expert: layer_19[2.2-17.8% std=3.54]
  Expert: layer_20[2.3-20.0% std=4.11]
  Expert: layer_21[2.9-12.6% std=3.23]
Step     167 | Loss: 4.7938 | LR: 1.04e-04 | GradNorm: 0.67 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.2% std=0.91] | layer_5[5.7-7.0% std=0.37] | layer_11[2.7-17.3% std=3.35] | layer_17[3.0-9.6% std=1.80] | layer_22[2.1-16.1% std=3.67]
  Expert: layer_1[5.2-8.2% std=0.60]
  Expert: layer_2[5.9-7.1% std=0.28]
  Expert: layer_3[3.6-13.4% std=2.15]
  Expert: layer_4[4.4-7.3% std=0.69]
  Expert: layer_6[3.6-11.6% std=1.63]
  Expert: layer_7[2.2-18.0% std=3.26]
  Expert: layer_8[1.8-16.6% std=3.04]
  Expert: layer_9[3.6-11.0% std=1.64]
  Expert: layer_10[2.2-20.2% std=3.93]
  Expert: layer_12[0.9-22.9% std=5.24]
  Expert: layer_13[3.0-14.9% std=2.59]
  Expert: layer_14[1.2-30.9% std=6.59]
  Expert: layer_15[1.3-39.2% std=8.67]
  Expert: layer_16[1.6-21.8% std=4.52]
  Expert: layer_18[3.2-15.3% std=3.33]
  Expert: layer_19[2.5-19.1% std=3.72]
  Expert: layer_20[1.6-21.9% std=4.56]
  Expert: layer_21[2.4-13.5% std=3.16]
Step     168 | Loss: 4.7945 | LR: 1.05e-04 | GradNorm: 0.76 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.6-8.1% std=0.89] | layer_5[5.4-6.8% std=0.38] | layer_11[2.9-15.8% std=2.99] | layer_17[3.9-9.1% std=1.38] | layer_22[3.0-14.1% std=3.23]
  Expert: layer_1[5.4-8.1% std=0.56]
  Expert: layer_2[5.9-7.1% std=0.28]
  Expert: layer_3[3.7-13.2% std=2.10]
  Expert: layer_4[4.7-7.2% std=0.66]
  Expert: layer_6[3.8-11.7% std=1.59]
  Expert: layer_7[2.3-18.3% std=3.31]
  Expert: layer_8[1.9-16.0% std=2.83]
  Expert: layer_9[3.4-10.7% std=1.62]
  Expert: layer_10[2.3-19.5% std=3.70]
  Expert: layer_12[1.2-22.6% std=5.16]
  Expert: layer_13[3.1-13.2% std=2.25]
  Expert: layer_14[1.2-31.0% std=6.59]
  Expert: layer_15[1.0-38.4% std=8.43]
  Expert: layer_16[1.9-19.3% std=3.88]
  Expert: layer_18[2.9-13.8% std=3.10]
  Expert: layer_19[3.0-17.9% std=3.41]
  Expert: layer_20[2.3-23.1% std=4.77]
  Expert: layer_21[2.8-13.0% std=2.71]
Step     169 | Loss: 4.7242 | LR: 1.06e-04 | GradNorm: 0.65 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.84] | layer_5[5.5-7.0% std=0.36] | layer_11[2.6-16.7% std=3.21] | layer_17[3.7-7.8% std=1.33] | layer_22[2.9-14.5% std=3.19]
  Expert: layer_1[5.3-8.1% std=0.57]
  Expert: layer_2[5.8-6.7% std=0.27]
  Expert: layer_3[3.7-13.6% std=2.19]
  Expert: layer_4[4.9-7.4% std=0.62]
  Expert: layer_6[3.9-11.7% std=1.58]
  Expert: layer_7[2.3-18.3% std=3.31]
  Expert: layer_8[2.1-16.7% std=2.98]
  Expert: layer_9[3.4-11.0% std=1.68]
  Expert: layer_10[2.5-19.7% std=3.69]
  Expert: layer_12[1.0-23.0% std=5.25]
  Expert: layer_13[3.1-13.9% std=2.39]
  Expert: layer_14[1.2-30.5% std=6.45]
  Expert: layer_15[1.3-39.7% std=8.79]
  Expert: layer_16[1.5-20.9% std=4.28]
  Expert: layer_18[3.1-14.6% std=3.26]
  Expert: layer_19[2.9-16.8% std=3.22]
  Expert: layer_20[2.5-20.9% std=4.21]
  Expert: layer_21[3.1-11.7% std=2.84]
Step     170 | Loss: 4.7272 | LR: 1.06e-04 | GradNorm: 0.61 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.84] | layer_5[5.6-7.1% std=0.33] | layer_11[2.5-16.6% std=3.22] | layer_17[3.8-8.0% std=1.28] | layer_22[2.7-13.7% std=3.14]
  Expert: layer_1[5.3-8.0% std=0.57]
  Expert: layer_2[5.8-6.7% std=0.26]
  Expert: layer_3[3.4-13.9% std=2.32]
  Expert: layer_4[5.0-7.5% std=0.62]
  Expert: layer_6[3.9-11.8% std=1.60]
  Expert: layer_7[2.4-19.0% std=3.48]
  Expert: layer_8[2.0-16.6% std=2.99]
  Expert: layer_9[3.3-11.1% std=1.70]
  Expert: layer_10[2.5-20.7% std=3.95]
  Expert: layer_12[1.1-21.9% std=5.29]
  Expert: layer_13[3.0-14.1% std=2.44]
  Expert: layer_14[1.0-31.5% std=6.70]
  Expert: layer_15[1.1-39.3% std=8.68]
  Expert: layer_16[1.3-21.8% std=4.45]
  Expert: layer_18[2.6-14.7% std=3.52]
  Expert: layer_19[2.8-17.8% std=3.46]
  Expert: layer_20[2.8-22.7% std=4.63]
  Expert: layer_21[2.5-13.6% std=3.13]
Step     171 | Loss: 4.7353 | LR: 1.07e-04 | GradNorm: 0.66 | Tok/s: 2558 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.88] | layer_5[5.6-7.1% std=0.38] | layer_11[2.7-16.0% std=3.00] | layer_17[3.7-8.8% std=1.48] | layer_22[2.1-13.0% std=3.06]
  Expert: layer_1[5.3-8.2% std=0.60]
  Expert: layer_2[5.8-6.9% std=0.27]
  Expert: layer_3[3.3-14.3% std=2.42]
  Expert: layer_4[5.2-7.2% std=0.56]
  Expert: layer_6[3.8-11.8% std=1.60]
  Expert: layer_7[2.2-18.9% std=3.44]
  Expert: layer_8[2.0-16.3% std=2.96]
  Expert: layer_9[3.4-10.9% std=1.59]
  Expert: layer_10[2.4-20.3% std=3.86]
  Expert: layer_12[1.3-20.6% std=5.04]
  Expert: layer_13[3.0-13.3% std=2.21]
  Expert: layer_14[1.0-31.2% std=6.61]
  Expert: layer_15[1.0-38.2% std=8.42]
  Expert: layer_16[1.4-20.3% std=4.08]
  Expert: layer_18[2.6-14.0% std=3.14]
  Expert: layer_19[2.8-17.0% std=3.35]
  Expert: layer_20[3.0-22.7% std=4.54]
  Expert: layer_21[1.9-14.8% std=3.08]
Step     172 | Loss: 4.6950 | LR: 1.08e-04 | GradNorm: 0.62 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.84] | layer_5[5.6-7.2% std=0.42] | layer_11[2.5-15.9% std=3.02] | layer_17[3.7-8.8% std=1.51] | layer_22[1.8-13.8% std=3.11]
  Expert: layer_1[5.3-8.2% std=0.58]
  Expert: layer_2[5.8-6.9% std=0.31]
  Expert: layer_3[3.4-14.8% std=2.55]
  Expert: layer_4[5.3-7.1% std=0.54]
  Expert: layer_6[3.8-12.1% std=1.68]
  Expert: layer_7[2.2-18.6% std=3.35]
  Expert: layer_8[2.1-16.0% std=2.81]
  Expert: layer_9[3.3-11.0% std=1.62]
  Expert: layer_10[2.4-20.0% std=3.79]
  Expert: layer_12[1.2-20.4% std=4.97]
  Expert: layer_13[3.3-13.2% std=2.14]
  Expert: layer_14[1.2-30.4% std=6.42]
  Expert: layer_15[1.3-38.5% std=8.51]
  Expert: layer_16[1.3-20.6% std=4.18]
  Expert: layer_18[2.7-15.0% std=3.45]
  Expert: layer_19[3.3-17.4% std=3.29]
  Expert: layer_20[2.7-19.6% std=4.08]
  Expert: layer_21[2.6-12.3% std=2.87]
Step     173 | Loss: 4.7683 | LR: 1.08e-04 | GradNorm: 0.53 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.84] | layer_5[5.5-7.7% std=0.56] | layer_11[2.4-17.5% std=3.36] | layer_17[3.3-9.9% std=1.66] | layer_22[2.1-14.8% std=3.21]
  Expert: layer_1[5.5-8.0% std=0.56]
  Expert: layer_2[5.9-7.0% std=0.27]
  Expert: layer_3[3.4-15.4% std=2.68]
  Expert: layer_4[5.1-7.5% std=0.58]
  Expert: layer_6[3.8-12.0% std=1.65]
  Expert: layer_7[1.9-19.1% std=3.51]
  Expert: layer_8[1.9-15.8% std=2.85]
  Expert: layer_9[3.1-10.5% std=1.57]
  Expert: layer_10[2.2-19.9% std=3.81]
  Expert: layer_12[1.2-22.1% std=5.21]
  Expert: layer_13[3.4-14.2% std=2.38]
  Expert: layer_14[1.3-29.4% std=6.24]
  Expert: layer_15[1.6-38.1% std=8.40]
  Expert: layer_16[1.3-21.9% std=4.44]
  Expert: layer_18[2.9-16.0% std=3.41]
  Expert: layer_19[3.4-16.6% std=3.13]
  Expert: layer_20[2.4-20.5% std=4.18]
  Expert: layer_21[2.8-12.8% std=2.86]
Step     174 | Loss: 4.6709 | LR: 1.09e-04 | GradNorm: 0.59 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.83] | layer_5[5.6-7.4% std=0.47] | layer_11[2.6-16.9% std=3.15] | layer_17[2.9-9.4% std=1.64] | layer_22[2.7-14.4% std=3.00]
  Expert: layer_1[5.5-7.9% std=0.52]
  Expert: layer_2[5.8-6.8% std=0.26]
  Expert: layer_3[3.4-15.0% std=2.60]
  Expert: layer_4[5.0-7.3% std=0.63]
  Expert: layer_6[3.7-12.1% std=1.66]
  Expert: layer_7[2.1-18.9% std=3.44]
  Expert: layer_8[2.2-15.5% std=2.74]
  Expert: layer_9[2.8-9.9% std=1.50]
  Expert: layer_10[2.3-19.8% std=3.74]
  Expert: layer_12[1.2-22.3% std=5.21]
  Expert: layer_13[3.2-12.8% std=2.06]
  Expert: layer_14[1.3-29.4% std=6.17]
  Expert: layer_15[1.7-37.3% std=8.18]
  Expert: layer_16[1.4-20.4% std=4.04]
  Expert: layer_18[2.7-15.0% std=3.12]
  Expert: layer_19[3.5-15.2% std=2.78]
  Expert: layer_20[2.5-21.7% std=4.45]
  Expert: layer_21[2.9-12.5% std=2.78]
Step     175 | Loss: 4.6974 | LR: 1.09e-04 | GradNorm: 0.62 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.86] | layer_5[5.6-7.6% std=0.55] | layer_11[2.6-17.2% std=3.25] | layer_17[2.4-8.5% std=1.66] | layer_22[2.0-14.4% std=3.03]
  Expert: layer_1[5.6-8.0% std=0.55]
  Expert: layer_2[5.9-6.7% std=0.21]
  Expert: layer_3[3.5-14.9% std=2.59]
  Expert: layer_4[4.7-7.5% std=0.72]
  Expert: layer_6[3.6-11.8% std=1.60]
  Expert: layer_7[1.7-19.5% std=3.60]
  Expert: layer_8[2.0-15.6% std=2.84]
  Expert: layer_9[2.9-10.1% std=1.48]
  Expert: layer_10[1.9-20.5% std=3.93]
  Expert: layer_12[1.2-22.0% std=5.12]
  Expert: layer_13[3.0-12.9% std=2.15]
  Expert: layer_14[1.3-29.1% std=6.11]
  Expert: layer_15[1.7-36.8% std=8.05]
  Expert: layer_16[1.0-22.0% std=4.45]
  Expert: layer_18[3.4-15.2% std=2.94]
  Expert: layer_19[3.0-15.2% std=2.98]
  Expert: layer_20[2.0-22.4% std=4.62]
  Expert: layer_21[2.6-12.6% std=2.84]
Step     176 | Loss: 4.7153 | LR: 1.10e-04 | GradNorm: 0.65 | Tok/s: 2586 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.2% std=0.86] | layer_5[5.6-7.4% std=0.47] | layer_11[2.4-16.6% std=3.28] | layer_17[2.4-9.9% std=1.90] | layer_22[2.4-13.1% std=2.90]
  Expert: layer_1[5.6-7.8% std=0.47]
  Expert: layer_2[5.8-6.7% std=0.25]
  Expert: layer_3[3.3-15.1% std=2.64]
  Expert: layer_4[4.7-7.4% std=0.78]
  Expert: layer_6[3.5-12.4% std=1.78]
  Expert: layer_7[1.9-19.9% std=3.73]
  Expert: layer_8[2.0-16.0% std=2.90]
  Expert: layer_9[2.6-10.9% std=1.66]
  Expert: layer_10[2.0-21.9% std=4.24]
  Expert: layer_12[1.1-22.1% std=5.32]
  Expert: layer_13[3.4-12.5% std=1.99]
  Expert: layer_14[1.1-30.1% std=6.34]
  Expert: layer_15[1.1-37.3% std=8.22]
  Expert: layer_16[0.9-21.7% std=4.40]
  Expert: layer_18[2.6-15.2% std=3.46]
  Expert: layer_19[2.5-17.4% std=3.35]
  Expert: layer_20[2.3-21.7% std=4.65]
  Expert: layer_21[2.7-12.2% std=2.66]
Step     177 | Loss: 4.6485 | LR: 1.11e-04 | GradNorm: 0.65 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.84] | layer_5[5.4-7.3% std=0.50] | layer_11[2.5-17.3% std=3.31] | layer_17[2.5-9.6% std=1.72] | layer_22[2.0-14.9% std=3.36]
  Expert: layer_1[5.7-8.0% std=0.53]
  Expert: layer_2[5.7-6.6% std=0.24]
  Expert: layer_3[3.3-15.5% std=2.70]
  Expert: layer_4[4.8-7.3% std=0.71]
  Expert: layer_6[3.2-12.0% std=1.71]
  Expert: layer_7[1.8-20.0% std=3.75]
  Expert: layer_8[1.9-15.5% std=2.82]
  Expert: layer_9[2.9-10.2% std=1.45]
  Expert: layer_10[2.2-21.3% std=4.12]
  Expert: layer_12[1.1-23.2% std=5.39]
  Expert: layer_13[3.5-13.0% std=2.04]
  Expert: layer_14[1.3-27.7% std=5.80]
  Expert: layer_15[1.5-35.1% std=7.64]
  Expert: layer_16[1.2-20.9% std=4.18]
  Expert: layer_18[2.6-16.5% std=3.29]
  Expert: layer_19[3.4-14.4% std=2.76]
  Expert: layer_20[2.3-19.9% std=4.07]
  Expert: layer_21[2.7-13.4% std=2.79]
Step     178 | Loss: 4.7682 | LR: 1.11e-04 | GradNorm: 0.72 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.1% std=0.87] | layer_5[5.0-7.2% std=0.55] | layer_11[2.6-17.1% std=3.39] | layer_17[2.9-8.5% std=1.45] | layer_22[3.2-13.2% std=2.82]
  Expert: layer_1[5.6-8.0% std=0.55]
  Expert: layer_2[5.9-6.7% std=0.25]
  Expert: layer_3[3.3-15.6% std=2.72]
  Expert: layer_4[4.6-7.7% std=0.77]
  Expert: layer_6[2.8-11.9% std=1.74]
  Expert: layer_7[1.9-20.2% std=3.80]
  Expert: layer_8[1.6-15.3% std=2.81]
  Expert: layer_9[2.8-10.7% std=1.58]
  Expert: layer_10[2.4-21.6% std=4.14]
  Expert: layer_12[1.0-22.8% std=5.31]
  Expert: layer_13[3.8-12.3% std=1.93]
  Expert: layer_14[1.4-28.8% std=6.05]
  Expert: layer_15[1.4-34.1% std=7.40]
  Expert: layer_16[1.3-19.5% std=3.87]
  Expert: layer_18[2.7-14.9% std=3.45]
  Expert: layer_19[3.7-15.1% std=2.73]
  Expert: layer_20[2.1-17.9% std=3.68]
  Expert: layer_21[3.0-11.8% std=2.60]
Step     179 | Loss: 4.6907 | LR: 1.12e-04 | GradNorm: 0.82 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.86] | layer_5[5.1-7.2% std=0.52] | layer_11[2.5-17.7% std=3.55] | layer_17[2.8-8.9% std=1.55] | layer_22[1.9-15.0% std=3.16]
  Expert: layer_1[5.7-7.9% std=0.49]
  Expert: layer_2[5.9-6.6% std=0.22]
  Expert: layer_3[3.2-15.6% std=2.71]
  Expert: layer_4[4.4-7.7% std=0.81]
  Expert: layer_6[3.0-11.8% std=1.71]
  Expert: layer_7[1.8-19.4% std=3.61]
  Expert: layer_8[1.9-15.5% std=2.80]
  Expert: layer_9[3.1-10.9% std=1.55]
  Expert: layer_10[2.6-20.5% std=3.91]
  Expert: layer_12[0.9-23.6% std=5.46]
  Expert: layer_13[3.3-12.3% std=1.96]
  Expert: layer_14[1.2-29.2% std=6.15]
  Expert: layer_15[1.4-36.5% std=7.96]
  Expert: layer_16[1.4-20.5% std=4.09]
  Expert: layer_18[3.0-16.9% std=3.49]
  Expert: layer_19[3.6-15.9% std=3.05]
  Expert: layer_20[2.5-22.5% std=4.63]
  Expert: layer_21[2.6-13.9% std=2.86]
Step     180 | Loss: 4.6795 | LR: 1.12e-04 | GradNorm: 0.79 | Tok/s: 2606 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.2% std=0.84] | layer_5[5.6-7.0% std=0.43] | layer_11[2.7-17.3% std=3.41] | layer_17[3.2-8.2% std=1.34] | layer_22[3.4-13.4% std=2.45]
  Expert: layer_1[5.8-7.9% std=0.49]
  Expert: layer_2[5.8-6.7% std=0.25]
  Expert: layer_3[3.5-15.7% std=2.69]
  Expert: layer_4[4.6-7.7% std=0.75]
  Expert: layer_6[3.0-11.8% std=1.68]
  Expert: layer_7[2.3-19.1% std=3.49]
  Expert: layer_8[1.9-14.4% std=2.49]
  Expert: layer_9[3.0-10.9% std=1.60]
  Expert: layer_10[2.7-20.7% std=3.91]
  Expert: layer_12[1.0-20.4% std=4.80]
  Expert: layer_13[3.7-12.5% std=1.97]
  Expert: layer_14[1.3-28.0% std=5.83]
  Expert: layer_15[1.9-34.0% std=7.40]
  Expert: layer_16[1.6-18.5% std=3.73]
  Expert: layer_18[3.2-13.7% std=3.01]
  Expert: layer_19[3.1-14.6% std=2.68]
  Expert: layer_20[2.7-17.1% std=3.48]
  Expert: layer_21[3.2-12.7% std=2.74]
Step     181 | Loss: 4.6927 | LR: 1.13e-04 | GradNorm: 0.57 | Tok/s: 2575 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.84] | layer_5[5.3-7.0% std=0.42] | layer_11[2.5-16.7% std=3.29] | layer_17[3.2-9.0% std=1.49] | layer_22[2.5-15.2% std=2.99]
  Expert: layer_1[5.7-7.9% std=0.48]
  Expert: layer_2[5.8-6.7% std=0.24]
  Expert: layer_3[3.5-15.9% std=2.74]
  Expert: layer_4[4.6-7.6% std=0.74]
  Expert: layer_6[3.1-11.6% std=1.62]
  Expert: layer_7[2.0-19.5% std=3.60]
  Expert: layer_8[1.8-14.9% std=2.61]
  Expert: layer_9[2.9-11.0% std=1.61]
  Expert: layer_10[2.6-21.6% std=4.16]
  Expert: layer_12[0.8-21.5% std=5.12]
  Expert: layer_13[3.3-12.1% std=2.00]
  Expert: layer_14[1.3-29.2% std=6.14]
  Expert: layer_15[1.6-33.8% std=7.33]
  Expert: layer_16[1.6-19.0% std=3.79]
  Expert: layer_18[2.9-14.4% std=3.05]
  Expert: layer_19[3.2-16.6% std=3.16]
  Expert: layer_20[3.0-17.6% std=3.55]
  Expert: layer_21[2.7-12.8% std=2.82]
Step     182 | Loss: 4.6487 | LR: 1.14e-04 | GradNorm: 0.70 | Tok/s: 2605 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.85] | layer_5[5.4-6.8% std=0.42] | layer_11[2.4-17.6% std=3.44] | layer_17[2.8-7.9% std=1.33] | layer_22[2.5-13.7% std=2.83]
  Expert: layer_1[5.6-7.9% std=0.48]
  Expert: layer_2[5.8-6.8% std=0.27]
  Expert: layer_3[3.6-15.9% std=2.69]
  Expert: layer_4[4.5-7.2% std=0.75]
  Expert: layer_6[3.1-11.5% std=1.61]
  Expert: layer_7[2.0-18.8% std=3.45]
  Expert: layer_8[1.8-15.7% std=2.80]
  Expert: layer_9[3.1-10.8% std=1.54]
  Expert: layer_10[2.3-21.2% std=4.10]
  Expert: layer_12[0.8-22.3% std=5.23]
  Expert: layer_13[2.9-12.3% std=2.02]
  Expert: layer_14[1.2-28.6% std=6.02]
  Expert: layer_15[1.6-34.0% std=7.40]
  Expert: layer_16[1.6-19.1% std=3.79]
  Expert: layer_18[2.9-14.4% std=2.85]
  Expert: layer_19[2.5-16.1% std=3.02]
  Expert: layer_20[3.2-19.8% std=3.90]
  Expert: layer_21[2.8-12.6% std=2.70]
Step     183 | Loss: 4.6422 | LR: 1.14e-04 | GradNorm: 0.66 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.84] | layer_5[5.6-6.9% std=0.33] | layer_11[2.5-16.3% std=3.17] | layer_17[3.0-9.1% std=1.43] | layer_22[2.7-12.9% std=2.39]
  Expert: layer_1[5.7-7.7% std=0.43]
  Expert: layer_2[5.8-6.6% std=0.20]
  Expert: layer_3[3.6-15.9% std=2.69]
  Expert: layer_4[4.7-7.0% std=0.69]
  Expert: layer_6[2.9-11.5% std=1.59]
  Expert: layer_7[2.4-18.7% std=3.40]
  Expert: layer_8[1.7-14.9% std=2.58]
  Expert: layer_9[3.0-10.5% std=1.51]
  Expert: layer_10[2.5-21.6% std=4.15]
  Expert: layer_12[0.9-20.1% std=4.91]
  Expert: layer_13[3.3-12.0% std=2.01]
  Expert: layer_14[1.5-28.8% std=6.04]
  Expert: layer_15[1.2-31.4% std=6.79]
  Expert: layer_16[1.9-18.9% std=3.75]
  Expert: layer_18[2.6-14.2% std=2.96]
  Expert: layer_19[2.7-15.4% std=2.85]
  Expert: layer_20[3.3-16.7% std=3.32]
  Expert: layer_21[3.2-12.0% std=2.54]
Step     184 | Loss: 4.5646 | LR: 1.15e-04 | GradNorm: 0.63 | Tok/s: 2601 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.0% std=0.81] | layer_5[5.8-6.7% std=0.29] | layer_11[2.5-16.5% std=3.10] | layer_17[3.0-7.8% std=1.12] | layer_22[2.3-13.2% std=2.77]
  Expert: layer_1[5.7-7.8% std=0.45]
  Expert: layer_2[5.7-6.8% std=0.26]
  Expert: layer_3[3.7-15.3% std=2.54]
  Expert: layer_4[4.6-7.0% std=0.71]
  Expert: layer_6[2.9-11.7% std=1.67]
  Expert: layer_7[2.4-18.0% std=3.26]
  Expert: layer_8[2.0-15.5% std=2.66]
  Expert: layer_9[3.0-9.3% std=1.36]
  Expert: layer_10[2.6-19.8% std=3.72]
  Expert: layer_12[1.1-19.9% std=4.73]
  Expert: layer_13[3.3-12.0% std=1.87]
  Expert: layer_14[1.4-27.4% std=5.65]
  Expert: layer_15[1.3-31.4% std=6.83]
  Expert: layer_16[1.8-17.2% std=3.40]
  Expert: layer_18[3.3-12.3% std=2.50]
  Expert: layer_19[3.3-12.0% std=2.17]
  Expert: layer_20[3.5-16.4% std=3.18]
  Expert: layer_21[2.6-11.0% std=2.47]
Step     185 | Loss: 4.6731 | LR: 1.16e-04 | GradNorm: 0.70 | Tok/s: 2595 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.79] | layer_5[5.7-6.9% std=0.33] | layer_11[2.1-16.1% std=3.19] | layer_17[2.9-8.4% std=1.38] | layer_22[2.1-14.0% std=2.73]
  Expert: layer_1[5.6-7.8% std=0.44]
  Expert: layer_2[5.8-6.8% std=0.26]
  Expert: layer_3[3.7-15.4% std=2.64]
  Expert: layer_4[4.6-7.2% std=0.70]
  Expert: layer_6[2.8-12.1% std=1.75]
  Expert: layer_7[2.4-18.4% std=3.33]
  Expert: layer_8[1.9-15.7% std=2.77]
  Expert: layer_9[2.9-10.0% std=1.47]
  Expert: layer_10[2.2-21.3% std=4.09]
  Expert: layer_12[1.0-21.9% std=5.28]
  Expert: layer_13[3.2-12.6% std=2.05]
  Expert: layer_14[1.4-30.1% std=6.35]
  Expert: layer_15[1.1-33.6% std=7.32]
  Expert: layer_16[1.8-19.0% std=3.80]
  Expert: layer_18[2.5-14.0% std=3.10]
  Expert: layer_19[3.5-13.6% std=2.42]
  Expert: layer_20[3.0-20.3% std=4.11]
  Expert: layer_21[2.3-13.6% std=2.83]
Step     186 | Loss: 4.6214 | LR: 1.16e-04 | GradNorm: 0.87 | Tok/s: 2582 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.83] | layer_5[5.8-6.9% std=0.33] | layer_11[1.9-16.9% std=3.23] | layer_17[3.3-7.9% std=1.26] | layer_22[2.2-14.2% std=2.72]
  Expert: layer_1[5.7-7.9% std=0.46]
  Expert: layer_2[5.8-7.1% std=0.33]
  Expert: layer_3[3.6-15.8% std=2.76]
  Expert: layer_4[4.9-7.1% std=0.61]
  Expert: layer_6[3.0-12.1% std=1.74]
  Expert: layer_7[2.5-17.6% std=3.13]
  Expert: layer_8[1.9-15.1% std=2.66]
  Expert: layer_9[3.2-10.1% std=1.41]
  Expert: layer_10[2.4-19.8% std=3.71]
  Expert: layer_12[1.1-21.9% std=5.03]
  Expert: layer_13[3.7-12.8% std=1.94]
  Expert: layer_14[1.4-28.4% std=5.89]
  Expert: layer_15[1.3-32.2% std=7.03]
  Expert: layer_16[1.5-19.1% std=3.84]
  Expert: layer_18[2.5-14.3% std=2.99]
  Expert: layer_19[3.4-12.0% std=2.10]
  Expert: layer_20[2.6-15.6% std=3.05]
  Expert: layer_21[2.9-13.4% std=2.98]
Step     187 | Loss: 4.6448 | LR: 1.17e-04 | GradNorm: 1.20 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.81] | layer_5[5.8-6.7% std=0.28] | layer_11[2.6-14.5% std=2.83] | layer_17[2.8-8.5% std=1.45] | layer_22[2.9-12.5% std=2.46]
  Expert: layer_1[5.7-7.7% std=0.44]
  Expert: layer_2[5.9-7.1% std=0.29]
  Expert: layer_3[3.5-15.0% std=2.59]
  Expert: layer_4[4.7-7.0% std=0.58]
  Expert: layer_6[3.0-12.0% std=1.72]
  Expert: layer_7[2.7-18.4% std=3.33]
  Expert: layer_8[2.1-15.1% std=2.62]
  Expert: layer_9[3.0-9.5% std=1.29]
  Expert: layer_10[2.7-19.7% std=3.64]
  Expert: layer_12[1.3-22.7% std=5.33]
  Expert: layer_13[3.3-10.8% std=1.70]
  Expert: layer_14[1.4-28.9% std=6.05]
  Expert: layer_15[1.1-30.3% std=6.56]
  Expert: layer_16[2.1-17.0% std=3.23]
  Expert: layer_18[2.7-12.9% std=2.60]
  Expert: layer_19[3.4-11.3% std=2.29]
  Expert: layer_20[3.0-20.7% std=4.09]
  Expert: layer_21[2.6-14.5% std=2.78]
Step     188 | Loss: 4.6512 | LR: 1.17e-04 | GradNorm: 0.71 | Tok/s: 2585 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.82] | layer_5[5.9-6.7% std=0.23] | layer_11[2.2-15.9% std=3.09] | layer_17[3.0-8.7% std=1.28] | layer_22[2.7-14.8% std=2.90]
  Expert: layer_1[5.8-7.8% std=0.45]
  Expert: layer_2[5.9-7.1% std=0.33]
  Expert: layer_3[3.4-14.7% std=2.53]
  Expert: layer_4[4.8-7.0% std=0.59]
  Expert: layer_6[2.8-11.7% std=1.73]
  Expert: layer_7[2.4-17.8% std=3.21]
  Expert: layer_8[1.9-15.5% std=2.74]
  Expert: layer_9[2.9-9.4% std=1.32]
  Expert: layer_10[2.7-18.0% std=3.24]
  Expert: layer_12[1.4-22.7% std=5.22]
  Expert: layer_13[3.9-11.6% std=1.67]
  Expert: layer_14[1.3-27.9% std=5.79]
  Expert: layer_15[1.3-30.1% std=6.56]
  Expert: layer_16[1.6-19.3% std=3.95]
  Expert: layer_18[3.1-13.5% std=2.95]
  Expert: layer_19[4.1-12.2% std=2.07]
  Expert: layer_20[2.3-16.8% std=3.46]
  Expert: layer_21[2.8-11.2% std=2.81]
Step     189 | Loss: 4.5795 | LR: 1.18e-04 | GradNorm: 0.60 | Tok/s: 2554 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.8% std=0.76] | layer_5[5.7-6.7% std=0.28] | layer_11[2.3-16.4% std=3.22] | layer_17[3.0-8.8% std=1.36] | layer_22[2.9-13.5% std=2.54]
  Expert: layer_1[5.8-7.6% std=0.42]
  Expert: layer_2[5.8-7.1% std=0.34]
  Expert: layer_3[3.5-14.6% std=2.51]
  Expert: layer_4[4.8-6.9% std=0.60]
  Expert: layer_6[2.8-12.2% std=1.82]
  Expert: layer_7[2.6-17.7% std=3.18]
  Expert: layer_8[2.0-14.5% std=2.52]
  Expert: layer_9[3.0-8.9% std=1.30]
  Expert: layer_10[2.4-17.9% std=3.24]
  Expert: layer_12[1.3-22.0% std=5.08]
  Expert: layer_13[4.1-11.7% std=1.70]
  Expert: layer_14[1.4-28.9% std=6.05]
  Expert: layer_15[1.6-31.2% std=6.77]
  Expert: layer_16[1.7-19.0% std=3.86]
  Expert: layer_18[2.5-12.9% std=2.87]
  Expert: layer_19[3.5-12.5% std=2.19]
  Expert: layer_20[2.5-16.7% std=3.32]
  Expert: layer_21[2.9-12.9% std=2.76]
Step     190 | Loss: 4.6086 | LR: 1.19e-04 | GradNorm: 0.76 | Tok/s: 2564 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.79] | layer_5[5.7-6.8% std=0.31] | layer_11[2.7-14.6% std=2.92] | layer_17[3.2-8.5% std=1.47] | layer_22[2.4-13.1% std=2.64]
  Expert: layer_1[5.8-7.9% std=0.45]
  Expert: layer_2[5.8-6.9% std=0.30]
  Expert: layer_3[3.6-14.4% std=2.44]
  Expert: layer_4[5.0-6.9% std=0.54]
  Expert: layer_6[3.0-12.1% std=1.78]
  Expert: layer_7[2.1-18.1% std=3.28]
  Expert: layer_8[2.1-13.9% std=2.34]
  Expert: layer_9[3.5-8.7% std=1.11]
  Expert: layer_10[2.3-18.1% std=3.30]
  Expert: layer_12[1.5-20.8% std=5.03]
  Expert: layer_13[3.4-11.1% std=1.73]
  Expert: layer_14[1.3-29.1% std=6.09]
  Expert: layer_15[1.2-30.2% std=6.44]
  Expert: layer_16[2.0-17.7% std=3.42]
  Expert: layer_18[3.0-12.2% std=2.46]
  Expert: layer_19[3.6-12.2% std=2.32]
  Expert: layer_20[2.9-19.2% std=3.83]
  Expert: layer_21[2.6-13.7% std=2.66]
Step     191 | Loss: 4.6355 | LR: 1.19e-04 | GradNorm: 0.60 | Tok/s: 2580 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-7.8% std=0.79] | layer_5[5.7-6.7% std=0.26] | layer_11[2.3-16.3% std=3.21] | layer_17[3.5-8.0% std=1.26] | layer_22[2.2-13.7% std=2.54]
  Expert: layer_1[5.7-7.8% std=0.44]
  Expert: layer_2[5.8-7.2% std=0.37]
  Expert: layer_3[3.6-15.0% std=2.58]
  Expert: layer_4[5.0-7.3% std=0.57]
  Expert: layer_6[3.1-12.7% std=1.87]
  Expert: layer_7[2.0-18.4% std=3.40]
  Expert: layer_8[2.1-15.3% std=2.64]
  Expert: layer_9[3.5-9.0% std=1.19]
  Expert: layer_10[2.4-17.8% std=3.25]
  Expert: layer_12[1.2-20.6% std=4.94]
  Expert: layer_13[3.9-12.5% std=1.90]
  Expert: layer_14[1.3-27.9% std=5.77]
  Expert: layer_15[1.2-30.2% std=6.50]
  Expert: layer_16[1.5-19.1% std=3.83]
  Expert: layer_18[3.2-13.2% std=2.78]
  Expert: layer_19[3.7-11.6% std=2.04]
  Expert: layer_20[2.5-16.4% std=3.45]
  Expert: layer_21[3.0-11.4% std=2.56]
Step     192 | Loss: 4.5817 | LR: 1.20e-04 | GradNorm: 0.69 | Tok/s: 2601 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.76] | layer_5[5.7-6.8% std=0.30] | layer_11[2.1-17.2% std=3.41] | layer_17[3.8-7.9% std=1.20] | layer_22[2.8-12.1% std=2.39]
  Expert: layer_1[5.7-7.7% std=0.43]
  Expert: layer_2[5.8-7.0% std=0.34]
  Expert: layer_3[3.7-15.3% std=2.60]
  Expert: layer_4[4.8-7.4% std=0.59]
  Expert: layer_6[3.2-12.8% std=1.89]
  Expert: layer_7[1.9-19.0% std=3.55]
  Expert: layer_8[2.2-15.3% std=2.68]
  Expert: layer_9[3.6-8.8% std=1.17]
  Expert: layer_10[2.3-18.9% std=3.53]
  Expert: layer_12[1.4-21.1% std=4.97]
  Expert: layer_13[3.6-12.7% std=2.04]
  Expert: layer_14[1.3-30.0% std=6.39]
  Expert: layer_15[1.6-32.0% std=6.91]
  Expert: layer_16[1.7-20.2% std=4.00]
  Expert: layer_18[2.6-16.3% std=3.22]
  Expert: layer_19[2.8-12.2% std=2.27]
  Expert: layer_20[2.3-16.9% std=3.47]
  Expert: layer_21[3.0-14.9% std=2.86]
Step     193 | Loss: 4.6553 | LR: 1.21e-04 | GradNorm: 0.72 | Tok/s: 2599 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.79] | layer_5[5.5-7.0% std=0.32] | layer_11[2.2-14.4% std=3.09] | layer_17[3.9-9.2% std=1.45] | layer_22[2.8-12.5% std=2.25]
  Expert: layer_1[5.5-7.9% std=0.50]
  Expert: layer_2[5.9-7.0% std=0.31]
  Expert: layer_3[3.5-15.1% std=2.56]
  Expert: layer_4[5.3-7.6% std=0.61]
  Expert: layer_6[2.9-12.3% std=1.81]
  Expert: layer_7[1.5-19.7% std=3.69]
  Expert: layer_8[1.9-15.5% std=2.82]
  Expert: layer_9[3.5-9.1% std=1.18]
  Expert: layer_10[1.9-20.1% std=3.82]
  Expert: layer_12[1.3-21.9% std=5.28]
  Expert: layer_13[3.5-12.8% std=2.13]
  Expert: layer_14[1.4-30.5% std=6.44]
  Expert: layer_15[0.8-31.1% std=6.63]
  Expert: layer_16[1.7-18.6% std=3.64]
  Expert: layer_18[2.6-13.7% std=2.86]
  Expert: layer_19[4.2-13.6% std=2.28]
  Expert: layer_20[2.2-18.3% std=3.78]
  Expert: layer_21[3.0-14.1% std=2.73]
Step     194 | Loss: 4.5535 | LR: 1.21e-04 | GradNorm: 0.58 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-7.9% std=0.78] | layer_5[5.7-6.9% std=0.33] | layer_11[2.1-15.4% std=3.04] | layer_17[4.5-8.3% std=1.07] | layer_22[2.2-13.1% std=2.52]
  Expert: layer_1[5.5-7.9% std=0.49]
  Expert: layer_2[6.0-6.9% std=0.25]
  Expert: layer_3[3.5-14.9% std=2.49]
  Expert: layer_4[5.1-7.5% std=0.61]
  Expert: layer_6[3.0-12.6% std=1.84]
  Expert: layer_7[1.9-18.5% std=3.38]
  Expert: layer_8[2.1-15.4% std=2.80]
  Expert: layer_9[3.5-9.1% std=1.26]
  Expert: layer_10[2.0-18.6% std=3.42]
  Expert: layer_12[1.4-21.8% std=5.07]
  Expert: layer_13[3.8-11.7% std=1.76]
  Expert: layer_14[1.6-28.1% std=5.85]
  Expert: layer_15[1.3-31.0% std=6.66]
  Expert: layer_16[1.8-18.1% std=3.51]
  Expert: layer_18[3.2-13.6% std=2.66]
  Expert: layer_19[3.5-12.4% std=2.26]
  Expert: layer_20[2.5-18.6% std=3.78]
  Expert: layer_21[2.7-12.9% std=2.63]
Step     195 | Loss: 4.5748 | LR: 1.22e-04 | GradNorm: 0.67 | Tok/s: 2599 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.78] | layer_5[5.6-7.2% std=0.42] | layer_11[2.0-15.6% std=3.01] | layer_17[3.6-8.3% std=1.23] | layer_22[2.5-12.7% std=2.28]
  Expert: layer_1[5.6-7.9% std=0.49]
  Expert: layer_2[5.9-6.9% std=0.28]
  Expert: layer_3[3.6-15.2% std=2.55]
  Expert: layer_4[5.2-7.3% std=0.56]
  Expert: layer_6[2.9-12.3% std=1.78]
  Expert: layer_7[2.2-18.8% std=3.45]
  Expert: layer_8[2.0-15.6% std=2.88]
  Expert: layer_9[3.6-9.8% std=1.33]
  Expert: layer_10[1.9-19.7% std=3.71]
  Expert: layer_12[1.5-22.3% std=5.14]
  Expert: layer_13[4.1-11.5% std=1.73]
  Expert: layer_14[1.7-28.5% std=5.92]
  Expert: layer_15[1.8-29.5% std=6.30]
  Expert: layer_16[1.8-19.1% std=3.73]
  Expert: layer_18[2.8-14.7% std=2.96]
  Expert: layer_19[3.7-13.9% std=2.43]
  Expert: layer_20[2.8-15.6% std=3.07]
  Expert: layer_21[3.2-12.4% std=2.40]
Step     196 | Loss: 4.5665 | LR: 1.22e-04 | GradNorm: 0.70 | Tok/s: 2577 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.78] | layer_5[5.5-7.2% std=0.40] | layer_11[2.3-15.6% std=3.01] | layer_17[3.7-8.2% std=1.03] | layer_22[2.7-13.6% std=2.69]
  Expert: layer_1[5.5-7.8% std=0.47]
  Expert: layer_2[5.9-6.9% std=0.30]
  Expert: layer_3[3.7-15.4% std=2.61]
  Expert: layer_4[4.8-7.1% std=0.57]
  Expert: layer_6[3.3-12.6% std=1.83]
  Expert: layer_7[2.3-18.8% std=3.42]
  Expert: layer_8[2.0-16.3% std=3.04]
  Expert: layer_9[3.7-9.5% std=1.25]
  Expert: layer_10[1.9-19.4% std=3.66]
  Expert: layer_12[1.5-23.5% std=5.43]
  Expert: layer_13[3.5-12.0% std=1.91]
  Expert: layer_14[1.3-27.5% std=5.72]
  Expert: layer_15[1.6-30.6% std=6.53]
  Expert: layer_16[1.7-19.4% std=3.80]
  Expert: layer_18[2.8-15.3% std=2.96]
  Expert: layer_19[3.3-12.8% std=2.16]
  Expert: layer_20[2.9-19.5% std=3.85]
  Expert: layer_21[3.1-12.4% std=2.44]
Step     197 | Loss: 4.6402 | LR: 1.23e-04 | GradNorm: 0.77 | Tok/s: 2604 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-7.9% std=0.77] | layer_5[5.3-7.3% std=0.51] | layer_11[2.3-14.6% std=2.88] | layer_17[3.6-9.4% std=1.34] | layer_22[3.0-13.3% std=2.47]
  Expert: layer_1[5.7-7.8% std=0.45]
  Expert: layer_2[5.8-6.9% std=0.29]
  Expert: layer_3[3.7-15.7% std=2.67]
  Expert: layer_4[5.0-6.9% std=0.52]
  Expert: layer_6[3.2-12.3% std=1.78]
  Expert: layer_7[2.4-19.0% std=3.46]
  Expert: layer_8[2.0-16.3% std=3.00]
  Expert: layer_9[3.6-10.2% std=1.39]
  Expert: layer_10[2.0-20.2% std=3.83]
  Expert: layer_12[1.3-22.4% std=5.21]
  Expert: layer_13[3.5-12.1% std=1.90]
  Expert: layer_14[1.4-26.8% std=5.55]
  Expert: layer_15[1.5-28.6% std=5.98]
  Expert: layer_16[1.7-18.9% std=3.80]
  Expert: layer_18[2.8-12.9% std=2.81]
  Expert: layer_19[4.0-14.1% std=2.29]
  Expert: layer_20[3.2-15.2% std=3.00]
  Expert: layer_21[3.2-10.8% std=2.41]
Step     198 | Loss: 4.5833 | LR: 1.24e-04 | GradNorm: 0.78 | Tok/s: 2599 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.77] | layer_5[5.4-7.0% std=0.42] | layer_11[2.3-16.8% std=3.25] | layer_17[3.9-7.8% std=1.05] | layer_22[2.4-12.8% std=2.48]
  Expert: layer_1[5.6-7.7% std=0.42]
  Expert: layer_2[5.8-6.8% std=0.29]
  Expert: layer_3[3.7-15.3% std=2.63]
  Expert: layer_4[4.9-7.0% std=0.53]
  Expert: layer_6[3.2-12.2% std=1.75]
  Expert: layer_7[2.6-18.7% std=3.41]
  Expert: layer_8[2.0-15.6% std=2.94]
  Expert: layer_9[3.5-10.0% std=1.44]
  Expert: layer_10[1.9-19.9% std=3.79]
  Expert: layer_12[1.4-21.2% std=4.93]
  Expert: layer_13[3.7-11.4% std=1.69]
  Expert: layer_14[1.3-28.7% std=5.98]
  Expert: layer_15[2.2-30.8% std=6.60]
  Expert: layer_16[1.2-20.6% std=4.11]
  Expert: layer_18[2.6-17.6% std=3.49]
  Expert: layer_19[3.3-14.6% std=2.64]
  Expert: layer_20[2.7-18.5% std=3.59]
  Expert: layer_21[3.0-12.5% std=2.61]
Step     199 | Loss: 4.5880 | LR: 1.24e-04 | GradNorm: 0.69 | Tok/s: 2601 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.81] | layer_5[5.4-6.9% std=0.42] | layer_11[2.4-15.4% std=2.98] | layer_17[3.7-8.5% std=1.17] | layer_22[3.1-11.6% std=2.07]
  Expert: layer_1[5.8-7.9% std=0.45]
  Expert: layer_2[5.8-6.7% std=0.26]
  Expert: layer_3[3.8-15.1% std=2.55]
  Expert: layer_4[4.9-7.0% std=0.51]
  Expert: layer_6[3.1-11.9% std=1.68]
  Expert: layer_7[2.6-19.0% std=3.44]
  Expert: layer_8[2.0-15.5% std=2.86]
  Expert: layer_9[3.6-9.7% std=1.28]
  Expert: layer_10[2.1-19.3% std=3.66]
  Expert: layer_12[1.5-20.8% std=4.70]
  Expert: layer_13[3.5-11.8% std=1.80]
  Expert: layer_14[1.2-25.7% std=5.28]
  Expert: layer_15[2.4-27.4% std=5.68]
  Expert: layer_16[1.7-18.6% std=3.69]
  Expert: layer_18[2.9-12.3% std=2.62]
  Expert: layer_19[3.5-11.8% std=1.86]
  Expert: layer_20[2.9-16.2% std=3.05]
  Expert: layer_21[3.4-11.8% std=2.19]
Step     200 | Loss: 4.5974 | LR: 1.25e-04 | GradNorm: 0.91 | Tok/s: 2596 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.82] | layer_5[5.2-7.0% std=0.51] | layer_11[2.6-15.3% std=2.92] | layer_17[4.8-9.7% std=1.20] | layer_22[4.0-12.6% std=2.22]
  Expert: layer_1[5.6-8.0% std=0.49]
  Expert: layer_2[5.9-6.8% std=0.22]
  Expert: layer_3[3.6-15.5% std=2.65]
  Expert: layer_4[5.0-7.0% std=0.55]
  Expert: layer_6[2.9-11.9% std=1.69]
  Expert: layer_7[2.1-20.1% std=3.75]
  Expert: layer_8[1.8-15.9% std=2.93]
  Expert: layer_9[3.4-9.9% std=1.34]
  Expert: layer_10[2.0-20.3% std=3.93]
  Expert: layer_12[1.4-21.9% std=5.13]
  Expert: layer_13[3.4-11.8% std=1.90]
  Expert: layer_14[1.3-28.1% std=5.86]
  Expert: layer_15[1.6-29.2% std=6.14]
  Expert: layer_16[1.8-18.0% std=3.62]
  Expert: layer_18[2.5-13.6% std=3.20]
  Expert: layer_19[3.8-13.8% std=2.56]
  Expert: layer_20[3.0-15.0% std=3.13]
  Expert: layer_21[3.1-11.1% std=2.16]
Step     201 | Loss: 4.5480 | LR: 1.26e-04 | GradNorm: 1.09 | Tok/s: 2583 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.80] | layer_5[5.3-7.2% std=0.54] | layer_11[2.4-16.1% std=3.14] | layer_17[3.6-8.9% std=1.47] | layer_22[2.1-13.4% std=3.02]
  Expert: layer_1[5.5-7.9% std=0.49]
  Expert: layer_2[5.9-6.8% std=0.26]
  Expert: layer_3[3.6-15.1% std=2.59]
  Expert: layer_4[4.9-7.2% std=0.57]
  Expert: layer_6[3.0-11.8% std=1.66]
  Expert: layer_7[1.9-19.1% std=3.54]
  Expert: layer_8[2.0-16.1% std=3.01]
  Expert: layer_9[3.7-8.8% std=1.11]
  Expert: layer_10[2.0-18.1% std=3.48]
  Expert: layer_12[1.4-22.7% std=5.08]
  Expert: layer_13[3.2-13.5% std=2.14]
  Expert: layer_14[1.2-25.7% std=5.30]
  Expert: layer_15[1.9-28.6% std=5.99]
  Expert: layer_16[2.1-19.6% std=3.85]
  Expert: layer_18[3.2-15.9% std=2.92]
  Expert: layer_19[3.5-10.2% std=1.88]
  Expert: layer_20[2.1-18.0% std=3.50]
  Expert: layer_21[2.7-12.9% std=2.69]
Step     202 | Loss: 4.5966 | LR: 1.26e-04 | GradNorm: 1.08 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.80] | layer_5[5.5-7.1% std=0.54] | layer_11[2.5-15.8% std=3.03] | layer_17[4.7-9.2% std=0.98] | layer_22[3.8-10.5% std=1.90]
  Expert: layer_1[5.7-7.9% std=0.48]
  Expert: layer_2[5.9-6.7% std=0.23]
  Expert: layer_3[3.4-15.4% std=2.68]
  Expert: layer_4[5.0-7.1% std=0.53]
  Expert: layer_6[2.8-12.2% std=1.77]
  Expert: layer_7[2.3-19.1% std=3.50]
  Expert: layer_8[1.9-16.2% std=3.00]
  Expert: layer_9[3.3-9.2% std=1.24]
  Expert: layer_10[2.2-18.5% std=3.48]
  Expert: layer_12[1.2-21.2% std=4.84]
  Expert: layer_13[3.6-11.8% std=1.85]
  Expert: layer_14[1.2-28.0% std=5.80]
  Expert: layer_15[1.4-28.5% std=6.07]
  Expert: layer_16[1.9-16.4% std=3.27]
  Expert: layer_18[2.6-14.3% std=3.25]
  Expert: layer_19[3.8-13.3% std=2.24]
  Expert: layer_20[2.8-14.8% std=3.03]
  Expert: layer_21[3.7-11.3% std=2.30]
Step     203 | Loss: 4.5489 | LR: 1.27e-04 | GradNorm: 0.90 | Tok/s: 2603 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.7-8.2% std=0.84] | layer_5[5.3-7.2% std=0.53] | layer_11[2.5-14.7% std=2.82] | layer_17[3.9-8.2% std=1.19] | layer_22[2.3-13.0% std=2.29]
  Expert: layer_1[5.5-7.8% std=0.47]
  Expert: layer_2[5.7-6.7% std=0.26]
  Expert: layer_3[3.6-15.4% std=2.68]
  Expert: layer_4[5.0-7.2% std=0.54]
  Expert: layer_6[2.9-11.9% std=1.69]
  Expert: layer_7[2.4-19.3% std=3.55]
  Expert: layer_8[2.1-15.1% std=2.75]
  Expert: layer_9[3.4-9.2% std=1.21]
  Expert: layer_10[2.2-18.3% std=3.44]
  Expert: layer_12[1.5-21.3% std=4.79]
  Expert: layer_13[3.4-12.1% std=2.07]
  Expert: layer_14[1.7-25.7% std=5.26]
  Expert: layer_15[1.8-26.7% std=5.51]
  Expert: layer_16[2.3-18.6% std=3.55]
  Expert: layer_18[2.9-15.8% std=3.00]
  Expert: layer_19[3.7-10.9% std=2.01]
  Expert: layer_20[3.0-13.9% std=2.71]
  Expert: layer_21[3.4-14.2% std=2.78]
Step     204 | Loss: 4.5457 | LR: 1.28e-04 | GradNorm: 0.66 | Tok/s: 2601 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.81] | layer_5[5.3-7.2% std=0.54] | layer_11[2.8-14.2% std=2.80] | layer_17[3.4-8.6% std=1.23] | layer_22[3.0-12.1% std=2.34]
  Expert: layer_1[5.6-7.8% std=0.46]
  Expert: layer_2[5.8-6.8% std=0.28]
  Expert: layer_3[3.7-14.9% std=2.54]
  Expert: layer_4[5.0-7.1% std=0.54]
  Expert: layer_6[2.9-12.1% std=1.74]
  Expert: layer_7[2.3-18.9% std=3.46]
  Expert: layer_8[2.1-16.0% std=2.90]
  Expert: layer_9[3.3-9.5% std=1.28]
  Expert: layer_10[2.1-17.8% std=3.29]
  Expert: layer_12[1.4-20.9% std=4.82]
  Expert: layer_13[3.6-12.6% std=2.10]
  Expert: layer_14[1.9-25.1% std=5.09]
  Expert: layer_15[1.6-26.0% std=5.39]
  Expert: layer_16[2.0-17.4% std=3.36]
  Expert: layer_18[3.2-14.1% std=2.77]
  Expert: layer_19[3.8-10.4% std=1.59]
  Expert: layer_20[3.3-16.4% std=3.21]
  Expert: layer_21[3.5-12.0% std=2.28]
Step     205 | Loss: 4.5371 | LR: 1.28e-04 | GradNorm: 0.69 | Tok/s: 2599 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.81] | layer_5[5.2-7.0% std=0.50] | layer_11[2.8-14.3% std=2.71] | layer_17[3.9-8.5% std=1.04] | layer_22[3.4-11.1% std=2.26]
  Expert: layer_1[5.5-7.8% std=0.46]
  Expert: layer_2[5.8-6.9% std=0.28]
  Expert: layer_3[3.6-14.7% std=2.49]
  Expert: layer_4[5.0-7.2% std=0.52]
  Expert: layer_6[3.0-12.2% std=1.74]
  Expert: layer_7[2.3-18.1% std=3.30]
  Expert: layer_8[2.2-15.6% std=2.78]
  Expert: layer_9[3.4-9.8% std=1.38]
  Expert: layer_10[2.2-17.4% std=3.19]
  Expert: layer_12[1.4-19.9% std=4.58]
  Expert: layer_13[4.1-12.5% std=1.94]
  Expert: layer_14[1.9-24.4% std=4.89]
  Expert: layer_15[2.1-24.8% std=5.22]
  Expert: layer_16[2.1-16.7% std=3.28]
  Expert: layer_18[2.5-13.1% std=2.79]
  Expert: layer_19[3.9-10.6% std=1.44]
  Expert: layer_20[3.0-15.8% std=3.04]
  Expert: layer_21[3.8-11.6% std=2.17]
Step     206 | Loss: 4.5590 | LR: 1.29e-04 | GradNorm: 0.67 | Tok/s: 2583 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.82] | layer_5[5.4-6.8% std=0.44] | layer_11[3.0-14.5% std=2.73] | layer_17[3.9-8.1% std=0.99] | layer_22[2.4-12.3% std=2.06]
  Expert: layer_1[5.4-7.8% std=0.47]
  Expert: layer_2[5.8-7.0% std=0.28]
  Expert: layer_3[3.8-14.8% std=2.48]
  Expert: layer_4[4.7-7.3% std=0.52]
  Expert: layer_6[3.0-11.8% std=1.66]
  Expert: layer_7[2.2-18.9% std=3.45]
  Expert: layer_8[2.1-14.7% std=2.60]
  Expert: layer_9[3.3-9.7% std=1.32]
  Expert: layer_10[1.9-17.5% std=3.25]
  Expert: layer_12[1.3-19.1% std=4.49]
  Expert: layer_13[4.1-12.0% std=1.90]
  Expert: layer_14[2.1-25.3% std=5.19]
  Expert: layer_15[2.1-23.2% std=4.75]
  Expert: layer_16[2.3-19.2% std=3.74]
  Expert: layer_18[2.9-15.6% std=2.99]
  Expert: layer_19[4.2-11.7% std=1.72]
  Expert: layer_20[2.9-14.8% std=2.80]
  Expert: layer_21[3.2-14.1% std=2.58]
Step     207 | Loss: 4.5468 | LR: 1.29e-04 | GradNorm: 0.61 | Tok/s: 2601 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.80] | layer_5[5.3-6.7% std=0.41] | layer_11[3.1-14.6% std=2.70] | layer_17[3.9-8.2% std=0.92] | layer_22[2.5-12.0% std=2.08]
  Expert: layer_1[5.4-7.7% std=0.44]
  Expert: layer_2[5.9-6.9% std=0.25]
  Expert: layer_3[3.9-14.7% std=2.41]
  Expert: layer_4[5.0-7.0% std=0.47]
  Expert: layer_6[3.1-11.9% std=1.69]
  Expert: layer_7[2.3-18.6% std=3.39]
  Expert: layer_8[2.2-15.1% std=2.61]
  Expert: layer_9[3.2-9.2% std=1.24]
  Expert: layer_10[2.0-16.6% std=3.02]
  Expert: layer_12[1.4-18.9% std=4.38]
  Expert: layer_13[4.0-11.4% std=1.73]
  Expert: layer_14[2.0-23.9% std=4.85]
  Expert: layer_15[2.0-22.3% std=4.52]
  Expert: layer_16[2.7-17.2% std=3.21]
  Expert: layer_18[3.1-13.1% std=2.35]
  Expert: layer_19[4.4-10.0% std=1.51]
  Expert: layer_20[2.8-15.7% std=3.08]
  Expert: layer_21[3.2-13.0% std=2.40]
Step     208 | Loss: 4.4756 | LR: 1.30e-04 | GradNorm: 0.64 | Tok/s: 2600 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.80] | layer_5[5.3-6.7% std=0.38] | layer_11[3.0-14.9% std=2.85] | layer_17[3.7-7.9% std=0.94] | layer_22[3.1-12.2% std=2.20]
  Expert: layer_1[5.5-7.8% std=0.46]
  Expert: layer_2[5.8-7.1% std=0.29]
  Expert: layer_3[3.7-14.8% std=2.46]
  Expert: layer_4[4.9-7.0% std=0.47]
  Expert: layer_6[3.0-12.2% std=1.75]
  Expert: layer_7[2.3-18.8% std=3.47]
  Expert: layer_8[2.1-16.1% std=2.85]
  Expert: layer_9[3.0-9.2% std=1.33]
  Expert: layer_10[2.3-15.8% std=2.83]
  Expert: layer_12[1.3-19.3% std=4.42]
  Expert: layer_13[4.0-11.6% std=1.71]
  Expert: layer_14[2.0-22.0% std=4.37]
  Expert: layer_15[1.9-23.6% std=4.87]
  Expert: layer_16[2.2-16.9% std=3.16]
  Expert: layer_18[3.1-11.5% std=2.31]
  Expert: layer_19[4.0-9.0% std=1.16]
  Expert: layer_20[3.0-15.2% std=3.08]
  Expert: layer_21[3.4-11.0% std=1.90]
Step     209 | Loss: 4.5307 | LR: 1.31e-04 | GradNorm: 0.67 | Tok/s: 2598 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.78] | layer_5[5.3-6.6% std=0.34] | layer_11[2.9-14.5% std=2.87] | layer_17[3.5-9.3% std=1.21] | layer_22[2.8-11.4% std=1.98]
  Expert: layer_1[5.4-7.7% std=0.46]
  Expert: layer_2[6.0-7.0% std=0.24]
  Expert: layer_3[3.8-15.1% std=2.53]
  Expert: layer_4[4.7-7.0% std=0.56]
  Expert: layer_6[2.9-12.5% std=1.82]
  Expert: layer_7[2.0-19.9% std=3.72]
  Expert: layer_8[1.9-16.1% std=2.88]
  Expert: layer_9[2.8-9.3% std=1.43]
  Expert: layer_10[2.0-17.1% std=3.13]
  Expert: layer_12[1.1-19.5% std=4.69]
  Expert: layer_13[3.9-10.5% std=1.62]
  Expert: layer_14[1.9-25.8% std=5.26]
  Expert: layer_15[1.4-24.6% std=5.09]
  Expert: layer_16[2.2-17.1% std=3.29]
  Expert: layer_18[2.9-13.1% std=2.83]
  Expert: layer_19[4.7-11.4% std=1.65]
  Expert: layer_20[2.9-15.8% std=3.19]
  Expert: layer_21[3.4-12.8% std=2.30]
Step     210 | Loss: 4.5594 | LR: 1.31e-04 | GradNorm: 0.62 | Tok/s: 2505 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.81] | layer_5[5.5-6.8% std=0.27] | layer_11[2.8-14.4% std=2.90] | layer_17[3.6-8.5% std=1.14] | layer_22[2.2-13.6% std=2.55]
  Expert: layer_1[5.4-7.8% std=0.48]
  Expert: layer_2[5.8-7.0% std=0.26]
  Expert: layer_3[3.9-15.0% std=2.52]
  Expert: layer_4[4.7-7.1% std=0.61]
  Expert: layer_6[3.0-12.0% std=1.69]
  Expert: layer_7[1.9-19.5% std=3.63]
  Expert: layer_8[2.0-15.4% std=2.73]
  Expert: layer_9[2.9-9.3% std=1.40]
  Expert: layer_10[2.2-17.0% std=3.10]
  Expert: layer_12[1.1-19.3% std=4.52]
  Expert: layer_13[3.6-12.3% std=1.98]
  Expert: layer_14[2.1-24.8% std=5.03]
  Expert: layer_15[1.6-23.9% std=4.88]
  Expert: layer_16[2.0-17.9% std=3.49]
  Expert: layer_18[3.3-13.4% std=2.58]
  Expert: layer_19[4.2-9.9% std=1.41]
  Expert: layer_20[2.6-14.8% std=3.03]
  Expert: layer_21[3.3-13.1% std=2.38]
Step     211 | Loss: 4.5132 | LR: 1.32e-04 | GradNorm: 0.71 | Tok/s: 2576 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.2% std=0.81] | layer_5[5.5-6.8% std=0.31] | layer_11[2.8-15.5% std=2.96] | layer_17[3.7-7.6% std=1.04] | layer_22[2.2-13.4% std=2.54]
  Expert: layer_1[5.5-7.8% std=0.46]
  Expert: layer_2[5.7-6.9% std=0.24]
  Expert: layer_3[4.0-14.9% std=2.49]
  Expert: layer_4[4.8-7.4% std=0.66]
  Expert: layer_6[3.2-12.4% std=1.76]
  Expert: layer_7[1.9-20.1% std=3.79]
  Expert: layer_8[2.0-15.1% std=2.65]
  Expert: layer_9[3.0-8.4% std=1.28]
  Expert: layer_10[2.3-17.6% std=3.21]
  Expert: layer_12[1.4-19.3% std=4.41]
  Expert: layer_13[3.3-11.8% std=1.94]
  Expert: layer_14[2.0-24.7% std=4.98]
  Expert: layer_15[1.7-24.0% std=4.92]
  Expert: layer_16[2.4-17.1% std=3.25]
  Expert: layer_18[3.4-12.1% std=2.14]
  Expert: layer_19[3.2-9.2% std=1.58]
  Expert: layer_20[2.8-14.6% std=2.95]
  Expert: layer_21[3.3-14.5% std=2.61]
Step     212 | Loss: 4.5132 | LR: 1.32e-04 | GradNorm: 0.86 | Tok/s: 2554 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.81] | layer_5[5.5-6.7% std=0.28] | layer_11[2.9-14.7% std=3.03] | layer_17[3.6-9.9% std=1.35] | layer_22[3.0-11.8% std=2.04]
  Expert: layer_1[5.6-7.8% std=0.46]
  Expert: layer_2[5.8-6.8% std=0.25]
  Expert: layer_3[3.7-14.7% std=2.48]
  Expert: layer_4[4.9-7.8% std=0.74]
  Expert: layer_6[3.0-12.2% std=1.75]
  Expert: layer_7[1.7-20.6% std=3.90]
  Expert: layer_8[1.9-16.2% std=2.92]
  Expert: layer_9[2.8-9.2% std=1.34]
  Expert: layer_10[2.2-19.0% std=3.50]
  Expert: layer_12[1.3-19.4% std=4.59]
  Expert: layer_13[3.7-11.4% std=1.86]
  Expert: layer_14[2.1-25.9% std=5.30]
  Expert: layer_15[1.5-23.8% std=4.88]
  Expert: layer_16[2.2-18.3% std=3.60]
  Expert: layer_18[2.9-12.7% std=2.83]
  Expert: layer_19[4.3-12.0% std=1.75]
  Expert: layer_20[3.0-13.6% std=2.90]
  Expert: layer_21[3.8-12.2% std=2.10]
Step     213 | Loss: 4.4986 | LR: 1.33e-04 | GradNorm: 0.85 | Tok/s: 2469 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.77] | layer_5[5.2-6.8% std=0.37] | layer_11[2.9-15.9% std=3.04] | layer_17[3.6-8.4% std=1.21] | layer_22[3.1-11.3% std=2.19]
  Expert: layer_1[5.5-7.6% std=0.40]
  Expert: layer_2[5.7-6.8% std=0.24]
  Expert: layer_3[3.7-14.4% std=2.36]
  Expert: layer_4[5.2-7.9% std=0.64]
  Expert: layer_6[3.2-12.2% std=1.72]
  Expert: layer_7[1.8-19.8% std=3.72]
  Expert: layer_8[1.9-16.1% std=2.88]
  Expert: layer_9[3.2-8.2% std=1.16]
  Expert: layer_10[2.6-17.4% std=3.10]
  Expert: layer_12[1.8-19.1% std=4.22]
  Expert: layer_13[3.5-11.0% std=1.76]
  Expert: layer_14[2.1-21.2% std=4.29]
  Expert: layer_15[2.0-23.2% std=4.77]
  Expert: layer_16[2.5-15.9% std=2.91]
  Expert: layer_18[3.2-11.5% std=2.07]
  Expert: layer_19[2.7-8.0% std=1.40]
  Expert: layer_20[2.8-17.6% std=3.33]
  Expert: layer_21[3.7-13.5% std=2.28]
Step     214 | Loss: 4.5133 | LR: 1.34e-04 | GradNorm: 0.84 | Tok/s: 2473 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.79] | layer_5[5.3-6.9% std=0.39] | layer_11[2.6-15.6% std=3.00] | layer_17[3.6-8.3% std=1.16] | layer_22[1.8-15.1% std=2.89]
  Expert: layer_1[5.5-7.5% std=0.39]
  Expert: layer_2[5.8-6.7% std=0.20]
  Expert: layer_3[3.4-14.8% std=2.44]
  Expert: layer_4[5.4-7.6% std=0.54]
  Expert: layer_6[3.2-11.9% std=1.66]
  Expert: layer_7[1.9-19.2% std=3.55]
  Expert: layer_8[1.9-15.2% std=2.68]
  Expert: layer_9[3.3-8.7% std=1.18]
  Expert: layer_10[2.8-17.4% std=3.09]
  Expert: layer_12[1.4-18.4% std=4.23]
  Expert: layer_13[4.1-11.1% std=1.65]
  Expert: layer_14[2.3-20.7% std=3.97]
  Expert: layer_15[2.5-21.0% std=4.37]
  Expert: layer_16[2.1-17.7% std=3.38]
  Expert: layer_18[3.2-12.9% std=2.60]
  Expert: layer_19[3.8-9.6% std=1.63]
  Expert: layer_20[2.5-12.7% std=2.54]
  Expert: layer_21[2.6-13.3% std=2.65]
Step     215 | Loss: 4.4632 | LR: 1.34e-04 | GradNorm: 0.84 | Tok/s: 2461 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.78] | layer_5[5.4-6.7% std=0.34] | layer_11[3.0-13.7% std=2.76] | layer_17[3.3-9.0% std=1.25] | layer_22[2.6-10.8% std=2.00]
  Expert: layer_1[5.7-7.7% std=0.41]
  Expert: layer_2[5.9-6.7% std=0.21]
  Expert: layer_3[3.6-14.6% std=2.35]
  Expert: layer_4[5.2-7.3% std=0.56]
  Expert: layer_6[3.2-11.6% std=1.59]
  Expert: layer_7[1.9-19.0% std=3.49]
  Expert: layer_8[2.0-15.8% std=2.78]
  Expert: layer_9[3.1-8.5% std=1.17]
  Expert: layer_10[2.6-18.0% std=3.21]
  Expert: layer_12[1.4-18.6% std=4.34]
  Expert: layer_13[3.8-10.5% std=1.71]
  Expert: layer_14[2.2-22.5% std=4.46]
  Expert: layer_15[1.5-19.8% std=3.92]
  Expert: layer_16[2.3-16.7% std=3.06]
  Expert: layer_18[3.8-11.0% std=2.13]
  Expert: layer_19[4.6-9.8% std=1.58]
  Expert: layer_20[3.2-17.7% std=3.41]
  Expert: layer_21[3.5-13.0% std=2.18]
Step     216 | Loss: 4.4756 | LR: 1.35e-04 | GradNorm: 0.75 | Tok/s: 2577 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.1% std=0.78] | layer_5[5.7-7.0% std=0.36] | layer_11[2.9-17.0% std=3.20] | layer_17[3.7-7.6% std=0.87] | layer_22[2.0-12.9% std=2.48]
  Expert: layer_1[5.8-7.7% std=0.40]
  Expert: layer_2[5.8-6.8% std=0.22]
  Expert: layer_3[3.8-14.9% std=2.43]
  Expert: layer_4[5.5-7.2% std=0.46]
  Expert: layer_6[3.3-11.8% std=1.63]
  Expert: layer_7[1.9-18.4% std=3.35]
  Expert: layer_8[2.0-15.4% std=2.75]
  Expert: layer_9[3.1-8.4% std=1.28]
  Expert: layer_10[2.5-17.3% std=3.07]
  Expert: layer_12[1.5-19.2% std=4.35]
  Expert: layer_13[4.0-10.3% std=1.60]
  Expert: layer_14[2.3-22.4% std=4.46]
  Expert: layer_15[2.4-20.7% std=4.31]
  Expert: layer_16[2.1-17.5% std=3.24]
  Expert: layer_18[2.7-13.6% std=2.35]
  Expert: layer_19[3.6-9.6% std=1.57]
  Expert: layer_20[3.0-15.2% std=2.87]
  Expert: layer_21[3.3-12.6% std=2.18]
Step     217 | Loss: 4.4355 | LR: 1.36e-04 | GradNorm: 0.67 | Tok/s: 2556 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.8-8.0% std=0.78] | layer_5[5.9-7.1% std=0.33] | layer_11[2.7-15.5% std=2.95] | layer_17[3.9-9.1% std=1.15] | layer_22[2.9-12.1% std=2.24]
  Expert: layer_1[5.7-7.7% std=0.40]
  Expert: layer_2[5.8-7.0% std=0.27]
  Expert: layer_3[3.8-14.6% std=2.36]
  Expert: layer_4[5.6-7.1% std=0.40]
  Expert: layer_6[3.2-11.5% std=1.57]
  Expert: layer_7[2.0-18.1% std=3.24]
  Expert: layer_8[2.2-15.1% std=2.63]
  Expert: layer_9[3.1-8.5% std=1.34]
  Expert: layer_10[2.4-17.8% std=3.16]
  Expert: layer_12[1.7-19.5% std=4.42]
  Expert: layer_13[4.3-11.2% std=1.67]
  Expert: layer_14[2.2-21.8% std=4.28]
  Expert: layer_15[2.4-18.5% std=3.69]
  Expert: layer_16[2.5-16.2% std=2.88]
  Expert: layer_18[3.3-12.1% std=2.27]
  Expert: layer_19[4.6-8.6% std=1.12]
  Expert: layer_20[3.3-14.0% std=2.54]
  Expert: layer_21[3.6-11.8% std=1.81]
Step     218 | Loss: 4.5329 | LR: 1.36e-04 | GradNorm: 0.70 | Tok/s: 2552 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.80] | layer_5[5.9-6.7% std=0.30] | layer_11[2.5-16.5% std=3.23] | layer_17[4.1-8.4% std=1.06] | layer_22[2.4-12.4% std=2.37]
  Expert: layer_1[5.8-7.8% std=0.42]
  Expert: layer_2[5.8-6.8% std=0.27]
  Expert: layer_3[3.5-14.6% std=2.42]
  Expert: layer_4[5.5-7.1% std=0.46]
  Expert: layer_6[3.0-11.6% std=1.63]
  Expert: layer_7[1.7-18.9% std=3.44]
  Expert: layer_8[2.1-15.5% std=2.77]
  Expert: layer_9[2.9-8.6% std=1.38]
  Expert: layer_10[2.2-19.0% std=3.47]
  Expert: layer_12[1.4-20.3% std=4.66]
  Expert: layer_13[3.8-11.7% std=1.77]
  Expert: layer_14[1.8-23.9% std=4.78]
  Expert: layer_15[1.8-20.2% std=4.18]
  Expert: layer_16[2.2-16.5% std=3.05]
  Expert: layer_18[3.3-14.8% std=2.75]
  Expert: layer_19[4.8-8.8% std=1.27]
  Expert: layer_20[2.9-16.2% std=3.13]
  Expert: layer_21[3.3-11.1% std=1.98]
Step     219 | Loss: 4.4758 | LR: 1.37e-04 | GradNorm: 0.74 | Tok/s: 2561 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.77] | layer_5[5.8-7.0% std=0.30] | layer_11[2.8-15.5% std=2.94] | layer_17[3.8-8.3% std=1.20] | layer_22[2.3-12.7% std=2.49]
  Expert: layer_1[5.8-7.7% std=0.41]
  Expert: layer_2[5.8-6.7% std=0.25]
  Expert: layer_3[3.7-13.9% std=2.24]
  Expert: layer_4[5.3-7.0% std=0.45]
  Expert: layer_6[2.8-11.3% std=1.60]
  Expert: layer_7[1.8-18.6% std=3.38]
  Expert: layer_8[2.3-15.7% std=2.77]
  Expert: layer_9[2.8-8.4% std=1.32]
  Expert: layer_10[2.1-18.8% std=3.49]
  Expert: layer_12[1.5-21.4% std=4.86]
  Expert: layer_13[3.2-11.1% std=1.66]
  Expert: layer_14[1.6-23.3% std=4.65]
  Expert: layer_15[1.5-20.8% std=4.19]
  Expert: layer_16[2.6-15.3% std=2.84]
  Expert: layer_18[3.6-12.6% std=2.19]
  Expert: layer_19[4.4-8.5% std=1.27]
  Expert: layer_20[3.0-16.4% std=3.08]
  Expert: layer_21[2.7-11.4% std=1.94]
Step     220 | Loss: 4.4441 | LR: 1.38e-04 | GradNorm: 0.83 | Tok/s: 2569 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.79] | layer_5[5.4-6.8% std=0.40] | layer_11[2.9-17.2% std=3.28] | layer_17[3.5-8.9% std=1.21] | layer_22[2.7-13.5% std=2.45]
  Expert: layer_1[5.7-7.8% std=0.44]
  Expert: layer_2[5.8-6.6% std=0.23]
  Expert: layer_3[3.9-14.3% std=2.33]
  Expert: layer_4[5.2-7.4% std=0.53]
  Expert: layer_6[2.9-11.5% std=1.63]
  Expert: layer_7[1.9-18.4% std=3.38]
  Expert: layer_8[2.1-15.3% std=2.74]
  Expert: layer_9[2.7-8.6% std=1.37]
  Expert: layer_10[2.0-19.1% std=3.60]
  Expert: layer_12[1.5-21.1% std=4.68]
  Expert: layer_13[3.3-11.3% std=1.87]
  Expert: layer_14[1.6-23.6% std=4.80]
  Expert: layer_15[2.3-24.1% std=5.00]
  Expert: layer_16[2.1-17.0% std=3.17]
  Expert: layer_18[2.9-12.5% std=2.42]
  Expert: layer_19[4.1-8.8% std=1.47]
  Expert: layer_20[2.9-15.3% std=2.96]
  Expert: layer_21[3.5-12.0% std=1.91]
Step     221 | Loss: 4.4355 | LR: 1.38e-04 | GradNorm: 0.74 | Tok/s: 2575 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.79] | layer_5[5.7-7.0% std=0.33] | layer_11[3.1-14.8% std=2.74] | layer_17[3.0-8.7% std=1.40] | layer_22[2.7-11.9% std=2.37]
  Expert: layer_1[5.7-7.8% std=0.44]
  Expert: layer_2[5.8-6.8% std=0.26]
  Expert: layer_3[3.6-14.2% std=2.30]
  Expert: layer_4[5.1-7.9% std=0.59]
  Expert: layer_6[2.9-11.2% std=1.57]
  Expert: layer_7[2.0-18.2% std=3.29]
  Expert: layer_8[2.3-14.5% std=2.50]
  Expert: layer_9[2.9-8.3% std=1.24]
  Expert: layer_10[2.1-19.2% std=3.58]
  Expert: layer_12[1.7-19.9% std=4.49]
  Expert: layer_13[3.3-9.7% std=1.48]
  Expert: layer_14[1.5-22.5% std=4.49]
  Expert: layer_15[1.8-20.3% std=3.98]
  Expert: layer_16[2.5-15.4% std=2.80]
  Expert: layer_18[3.2-11.3% std=1.90]
  Expert: layer_19[4.6-9.1% std=1.27]
  Expert: layer_20[2.9-16.4% std=3.13]
  Expert: layer_21[3.5-13.1% std=2.09]
Step     222 | Loss: 4.4565 | LR: 1.39e-04 | GradNorm: 0.58 | Tok/s: 2583 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.77] | layer_5[5.7-7.2% std=0.38] | layer_11[2.9-16.4% std=3.02] | layer_17[3.7-8.0% std=1.07] | layer_22[2.3-12.7% std=2.41]
  Expert: layer_1[5.7-7.7% std=0.42]
  Expert: layer_2[5.8-6.8% std=0.28]
  Expert: layer_3[3.6-14.6% std=2.39]
  Expert: layer_4[5.3-7.6% std=0.51]
  Expert: layer_6[2.8-11.5% std=1.63]
  Expert: layer_7[2.1-18.1% std=3.28]
  Expert: layer_8[2.2-14.3% std=2.49]
  Expert: layer_9[2.9-8.8% std=1.27]
  Expert: layer_10[2.4-18.1% std=3.27]
  Expert: layer_12[1.4-20.4% std=4.53]
  Expert: layer_13[3.6-11.0% std=1.60]
  Expert: layer_14[1.8-21.7% std=4.22]
  Expert: layer_15[2.4-20.4% std=4.08]
  Expert: layer_16[2.4-15.5% std=2.87]
  Expert: layer_18[3.7-12.1% std=1.93]
  Expert: layer_19[4.6-7.4% std=0.96]
  Expert: layer_20[2.5-14.8% std=2.74]
  Expert: layer_21[3.8-11.4% std=1.92]
Step     223 | Loss: 4.4454 | LR: 1.39e-04 | GradNorm: 0.60 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.75] | layer_5[5.6-7.5% std=0.42] | layer_11[2.9-16.6% std=3.02] | layer_17[3.9-7.6% std=1.02] | layer_22[3.4-13.0% std=2.16]
  Expert: layer_1[5.6-7.7% std=0.41]
  Expert: layer_2[5.8-6.9% std=0.29]
  Expert: layer_3[3.6-14.7% std=2.42]
  Expert: layer_4[5.2-7.5% std=0.51]
  Expert: layer_6[2.8-11.6% std=1.67]
  Expert: layer_7[2.0-19.1% std=3.51]
  Expert: layer_8[2.0-14.2% std=2.47]
  Expert: layer_9[2.9-8.6% std=1.31]
  Expert: layer_10[2.2-18.4% std=3.35]
  Expert: layer_12[1.5-20.6% std=4.62]
  Expert: layer_13[3.6-11.2% std=1.77]
  Expert: layer_14[2.1-23.5% std=4.64]
  Expert: layer_15[2.5-20.7% std=4.16]
  Expert: layer_16[2.1-16.2% std=2.95]
  Expert: layer_18[3.6-12.8% std=2.07]
  Expert: layer_19[4.2-8.7% std=1.27]
  Expert: layer_20[2.7-12.8% std=2.39]
  Expert: layer_21[3.8-11.7% std=1.95]
Step     224 | Loss: 4.3930 | LR: 1.40e-04 | GradNorm: 0.54 | Tok/s: 2553 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.75] | layer_5[5.4-7.4% std=0.44] | layer_11[3.1-15.8% std=2.88] | layer_17[3.8-8.0% std=1.15] | layer_22[3.1-11.4% std=2.03]
  Expert: layer_1[5.7-7.6% std=0.39]
  Expert: layer_2[5.8-7.0% std=0.27]
  Expert: layer_3[3.9-14.5% std=2.37]
  Expert: layer_4[5.3-7.7% std=0.51]
  Expert: layer_6[2.9-11.6% std=1.64]
  Expert: layer_7[2.0-18.8% std=3.45]
  Expert: layer_8[2.1-14.4% std=2.45]
  Expert: layer_9[2.9-8.8% std=1.32]
  Expert: layer_10[2.5-17.7% std=3.17]
  Expert: layer_12[1.5-19.7% std=4.37]
  Expert: layer_13[3.8-10.6% std=1.69]
  Expert: layer_14[2.0-22.2% std=4.34]
  Expert: layer_15[2.6-19.2% std=3.84]
  Expert: layer_16[2.1-15.0% std=2.67]
  Expert: layer_18[3.8-11.1% std=1.80]
  Expert: layer_19[4.0-8.1% std=1.08]
  Expert: layer_20[2.9-13.4% std=2.48]
  Expert: layer_21[3.6-11.6% std=1.89]
Step     225 | Loss: 4.4089 | LR: 1.41e-04 | GradNorm: 0.59 | Tok/s: 2549 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.77] | layer_5[5.4-6.9% std=0.39] | layer_11[2.8-15.3% std=2.86] | layer_17[3.6-8.0% std=1.15] | layer_22[2.7-11.7% std=2.16]
  Expert: layer_1[5.7-7.6% std=0.40]
  Expert: layer_2[5.9-7.0% std=0.25]
  Expert: layer_3[3.8-14.5% std=2.38]
  Expert: layer_4[5.3-7.6% std=0.48]
  Expert: layer_6[2.7-11.4% std=1.65]
  Expert: layer_7[1.9-18.9% std=3.47]
  Expert: layer_8[2.0-14.8% std=2.56]
  Expert: layer_9[3.0-9.2% std=1.29]
  Expert: layer_10[2.5-18.0% std=3.24]
  Expert: layer_12[1.4-19.7% std=4.44]
  Expert: layer_13[3.9-11.0% std=1.73]
  Expert: layer_14[2.1-21.8% std=4.25]
  Expert: layer_15[2.4-18.7% std=3.75]
  Expert: layer_16[2.1-16.7% std=3.07]
  Expert: layer_18[3.7-11.7% std=1.99]
  Expert: layer_19[4.0-8.9% std=1.22]
  Expert: layer_20[2.6-15.5% std=2.93]
  Expert: layer_21[3.5-11.0% std=1.87]
Step     226 | Loss: 4.4135 | LR: 1.41e-04 | GradNorm: 0.61 | Tok/s: 2578 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.77] | layer_5[5.5-7.0% std=0.41] | layer_11[2.9-15.3% std=2.76] | layer_17[4.1-7.3% std=0.93] | layer_22[2.9-10.7% std=2.00]
  Expert: layer_1[5.8-7.6% std=0.38]
  Expert: layer_2[5.9-6.7% std=0.24]
  Expert: layer_3[3.8-14.3% std=2.32]
  Expert: layer_4[5.2-7.3% std=0.46]
  Expert: layer_6[2.8-11.3% std=1.59]
  Expert: layer_7[1.9-18.7% std=3.43]
  Expert: layer_8[1.8-14.4% std=2.48]
  Expert: layer_9[3.2-9.0% std=1.27]
  Expert: layer_10[2.5-17.9% std=3.23]
  Expert: layer_12[1.5-18.8% std=4.28]
  Expert: layer_13[4.0-10.3% std=1.67]
  Expert: layer_14[2.3-23.4% std=4.60]
  Expert: layer_15[2.5-19.1% std=3.86]
  Expert: layer_16[2.0-16.1% std=2.98]
  Expert: layer_18[3.1-12.3% std=2.15]
  Expert: layer_19[4.5-10.1% std=1.47]
  Expert: layer_20[2.9-15.1% std=2.77]
  Expert: layer_21[3.5-10.9% std=1.71]
Step     227 | Loss: 4.3175 | LR: 1.42e-04 | GradNorm: 0.52 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.8% std=0.70] | layer_5[5.5-7.0% std=0.43] | layer_11[3.2-15.1% std=2.66] | layer_17[3.7-8.1% std=1.03] | layer_22[2.5-11.6% std=2.43]
  Expert: layer_1[5.7-7.4% std=0.38]
  Expert: layer_2[5.8-6.7% std=0.25]
  Expert: layer_3[3.9-13.9% std=2.20]
  Expert: layer_4[5.6-7.2% std=0.44]
  Expert: layer_6[3.2-11.4% std=1.59]
  Expert: layer_7[2.3-17.8% std=3.25]
  Expert: layer_8[2.2-15.0% std=2.53]
  Expert: layer_9[3.1-9.5% std=1.39]
  Expert: layer_10[2.8-16.6% std=2.87]
  Expert: layer_12[1.6-19.1% std=4.20]
  Expert: layer_13[4.1-10.5% std=1.68]
  Expert: layer_14[2.3-19.8% std=3.82]
  Expert: layer_15[2.9-17.1% std=3.60]
  Expert: layer_16[2.6-15.7% std=2.90]
  Expert: layer_18[2.7-10.6% std=1.99]
  Expert: layer_19[4.6-8.0% std=1.02]
  Expert: layer_20[3.1-13.9% std=2.56]
  Expert: layer_21[3.5-9.9% std=1.73]
Step     228 | Loss: 4.3620 | LR: 1.42e-04 | GradNorm: 0.54 | Tok/s: 2594 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.77] | layer_5[5.7-6.8% std=0.35] | layer_11[2.8-14.0% std=2.55] | layer_17[4.1-8.2% std=1.02] | layer_22[2.8-12.5% std=2.41]
  Expert: layer_1[5.7-7.7% std=0.41]
  Expert: layer_2[5.8-6.9% std=0.28]
  Expert: layer_3[4.0-14.4% std=2.30]
  Expert: layer_4[5.5-6.8% std=0.36]
  Expert: layer_6[3.0-11.3% std=1.58]
  Expert: layer_7[2.0-19.1% std=3.57]
  Expert: layer_8[1.8-15.2% std=2.65]
  Expert: layer_9[3.3-9.1% std=1.28]
  Expert: layer_10[2.6-17.8% std=3.19]
  Expert: layer_12[1.7-20.1% std=4.41]
  Expert: layer_13[3.8-10.7% std=1.67]
  Expert: layer_14[2.1-21.3% std=4.13]
  Expert: layer_15[2.5-16.6% std=3.22]
  Expert: layer_16[2.3-16.6% std=3.09]
  Expert: layer_18[3.2-11.4% std=1.89]
  Expert: layer_19[4.6-8.8% std=1.15]
  Expert: layer_20[3.4-14.3% std=2.55]
  Expert: layer_21[3.6-11.2% std=1.83]
Step     229 | Loss: 4.3056 | LR: 1.43e-04 | GradNorm: 0.57 | Tok/s: 2596 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.75] | layer_5[5.6-6.7% std=0.33] | layer_11[3.1-13.8% std=2.54] | layer_17[4.2-7.4% std=0.83] | layer_22[3.0-10.7% std=2.04]
  Expert: layer_1[5.7-7.6% std=0.40]
  Expert: layer_2[5.6-6.8% std=0.26]
  Expert: layer_3[3.9-14.4% std=2.30]
  Expert: layer_4[5.5-6.8% std=0.40]
  Expert: layer_6[3.0-11.5% std=1.61]
  Expert: layer_7[2.1-18.7% std=3.45]
  Expert: layer_8[2.1-15.1% std=2.61]
  Expert: layer_9[3.1-8.8% std=1.29]
  Expert: layer_10[2.6-17.5% std=3.10]
  Expert: layer_12[1.6-19.1% std=4.14]
  Expert: layer_13[4.0-9.8% std=1.53]
  Expert: layer_14[2.2-21.0% std=4.12]
  Expert: layer_15[3.2-15.1% std=2.90]
  Expert: layer_16[2.2-16.1% std=2.98]
  Expert: layer_18[3.6-10.8% std=1.82]
  Expert: layer_19[4.6-8.4% std=1.07]
  Expert: layer_20[3.4-13.3% std=2.27]
  Expert: layer_21[3.3-10.2% std=1.69]
Step     230 | Loss: 4.3959 | LR: 1.44e-04 | GradNorm: 0.62 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.75] | layer_5[5.5-6.9% std=0.31] | layer_11[2.9-13.5% std=2.48] | layer_17[3.6-8.0% std=1.03] | layer_22[2.3-11.9% std=2.54]
  Expert: layer_1[5.7-7.8% std=0.45]
  Expert: layer_2[5.7-7.0% std=0.30]
  Expert: layer_3[3.7-14.5% std=2.36]
  Expert: layer_4[5.6-6.9% std=0.35]
  Expert: layer_6[2.9-11.7% std=1.67]
  Expert: layer_7[1.9-19.3% std=3.60]
  Expert: layer_8[2.1-15.6% std=2.74]
  Expert: layer_9[3.2-8.8% std=1.18]
  Expert: layer_10[2.4-18.1% std=3.29]
  Expert: layer_12[1.7-19.5% std=4.38]
  Expert: layer_13[3.5-9.4% std=1.54]
  Expert: layer_14[2.1-21.2% std=4.13]
  Expert: layer_15[2.1-14.6% std=2.71]
  Expert: layer_16[2.0-16.2% std=2.96]
  Expert: layer_18[2.9-10.8% std=1.83]
  Expert: layer_19[4.4-9.3% std=1.23]
  Expert: layer_20[3.2-15.3% std=2.79]
  Expert: layer_21[3.1-10.7% std=1.86]
Step     231 | Loss: 4.3319 | LR: 1.44e-04 | GradNorm: 0.69 | Tok/s: 2581 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.77] | layer_5[5.7-7.2% std=0.37] | layer_11[2.9-15.3% std=2.71] | layer_17[4.3-8.4% std=0.98] | layer_22[2.2-11.2% std=2.29]
  Expert: layer_1[5.6-7.7% std=0.43]
  Expert: layer_2[5.7-7.0% std=0.27]
  Expert: layer_3[3.7-15.1% std=2.53]
  Expert: layer_4[5.7-6.9% std=0.36]
  Expert: layer_6[2.9-12.0% std=1.69]
  Expert: layer_7[2.1-19.1% std=3.52]
  Expert: layer_8[2.0-15.3% std=2.65]
  Expert: layer_9[3.3-9.1% std=1.29]
  Expert: layer_10[2.7-17.8% std=3.20]
  Expert: layer_12[1.6-18.2% std=3.99]
  Expert: layer_13[3.9-10.0% std=1.61]
  Expert: layer_14[2.6-20.7% std=3.98]
  Expert: layer_15[3.0-13.5% std=2.80]
  Expert: layer_16[2.2-15.5% std=2.91]
  Expert: layer_18[3.5-11.6% std=2.07]
  Expert: layer_19[4.4-9.4% std=1.39]
  Expert: layer_20[3.4-11.6% std=2.04]
  Expert: layer_21[3.2-10.4% std=1.87]
Step     232 | Loss: 4.3705 | LR: 1.45e-04 | GradNorm: 0.90 | Tok/s: 2594 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.75] | layer_5[5.7-6.9% std=0.38] | layer_11[2.9-13.8% std=2.59] | layer_17[3.3-9.6% std=1.39] | layer_22[2.7-12.5% std=2.50]
  Expert: layer_1[5.6-7.7% std=0.44]
  Expert: layer_2[5.7-7.0% std=0.26]
  Expert: layer_3[3.8-15.4% std=2.56]
  Expert: layer_4[5.4-7.1% std=0.40]
  Expert: layer_6[2.8-12.2% std=1.76]
  Expert: layer_7[1.8-19.7% std=3.73]
  Expert: layer_8[1.9-16.5% std=2.97]
  Expert: layer_9[3.4-9.4% std=1.24]
  Expert: layer_10[2.3-18.7% std=3.46]
  Expert: layer_12[1.7-19.6% std=4.36]
  Expert: layer_13[3.1-10.9% std=1.91]
  Expert: layer_14[2.1-19.9% std=3.91]
  Expert: layer_15[2.1-13.5% std=2.64]
  Expert: layer_16[2.5-16.7% std=3.08]
  Expert: layer_18[3.5-11.2% std=1.81]
  Expert: layer_19[4.6-8.8% std=1.35]
  Expert: layer_20[3.7-15.1% std=2.57]
  Expert: layer_21[3.4-11.8% std=1.82]
Step     233 | Loss: 4.3796 | LR: 1.46e-04 | GradNorm: 1.03 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.74] | layer_5[5.8-6.8% std=0.27] | layer_11[3.2-17.4% std=3.12] | layer_17[4.3-9.6% std=1.46] | layer_22[2.5-12.0% std=2.37]
  Expert: layer_1[5.8-7.7% std=0.42]
  Expert: layer_2[5.9-6.8% std=0.22]
  Expert: layer_3[4.1-15.3% std=2.51]
  Expert: layer_4[5.6-7.0% std=0.38]
  Expert: layer_6[2.8-12.5% std=1.81]
  Expert: layer_7[2.1-19.0% std=3.51]
  Expert: layer_8[2.1-15.4% std=2.71]
  Expert: layer_9[3.4-9.1% std=1.35]
  Expert: layer_10[2.8-17.7% std=3.20]
  Expert: layer_12[1.4-18.6% std=4.11]
  Expert: layer_13[4.2-9.7% std=1.61]
  Expert: layer_14[3.0-21.6% std=4.11]
  Expert: layer_15[2.8-14.2% std=3.30]
  Expert: layer_16[2.6-14.4% std=2.72]
  Expert: layer_18[2.7-11.9% std=2.51]
  Expert: layer_19[4.3-9.2% std=1.52]
  Expert: layer_20[3.8-9.8% std=1.98]
  Expert: layer_21[3.2-11.7% std=2.32]
Step     234 | Loss: 4.3983 | LR: 1.46e-04 | GradNorm: 0.73 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.76] | layer_5[5.8-6.7% std=0.26] | layer_11[3.0-15.2% std=2.75] | layer_17[4.3-7.9% std=0.97] | layer_22[2.3-11.1% std=2.08]
  Expert: layer_1[5.7-7.7% std=0.42]
  Expert: layer_2[5.9-6.8% std=0.24]
  Expert: layer_3[4.0-15.1% std=2.51]
  Expert: layer_4[5.6-6.7% std=0.31]
  Expert: layer_6[2.8-12.5% std=1.81]
  Expert: layer_7[2.1-18.5% std=3.40]
  Expert: layer_8[2.1-14.8% std=2.69]
  Expert: layer_9[3.3-8.7% std=1.23]
  Expert: layer_10[2.7-17.0% std=3.01]
  Expert: layer_12[1.6-18.8% std=4.01]
  Expert: layer_13[3.8-11.3% std=1.87]
  Expert: layer_14[2.8-19.5% std=3.73]
  Expert: layer_15[2.9-14.3% std=2.74]
  Expert: layer_16[2.4-15.3% std=2.75]
  Expert: layer_18[3.9-10.9% std=1.81]
  Expert: layer_19[4.1-8.3% std=1.28]
  Expert: layer_20[3.3-14.5% std=2.54]
  Expert: layer_21[3.5-13.2% std=2.10]
Step     235 | Loss: 4.3278 | LR: 1.47e-04 | GradNorm: 0.60 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.75] | layer_5[5.9-6.8% std=0.27] | layer_11[2.8-12.5% std=2.31] | layer_17[3.6-8.4% std=1.20] | layer_22[2.4-10.9% std=1.95]
  Expert: layer_1[5.8-7.8% std=0.42]
  Expert: layer_2[5.8-6.9% std=0.27]
  Expert: layer_3[3.8-14.5% std=2.37]
  Expert: layer_4[5.7-6.9% std=0.27]
  Expert: layer_6[2.8-12.0% std=1.73]
  Expert: layer_7[2.1-17.9% std=3.24]
  Expert: layer_8[2.4-14.5% std=2.53]
  Expert: layer_9[3.4-9.2% std=1.22]
  Expert: layer_10[2.7-16.6% std=2.91]
  Expert: layer_12[1.8-18.6% std=4.04]
  Expert: layer_13[3.7-10.7% std=1.62]
  Expert: layer_14[2.6-20.2% std=3.85]
  Expert: layer_15[2.5-14.6% std=2.66]
  Expert: layer_16[2.6-16.1% std=2.90]
  Expert: layer_18[3.9-11.8% std=1.89]
  Expert: layer_19[4.2-8.3% std=1.10]
  Expert: layer_20[2.9-13.3% std=2.48]
  Expert: layer_21[3.5-13.1% std=2.01]
Step     236 | Loss: 4.3430 | LR: 1.48e-04 | GradNorm: 0.68 | Tok/s: 2583 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.78] | layer_5[5.8-6.9% std=0.25] | layer_11[2.9-13.8% std=2.36] | layer_17[3.5-7.6% std=1.03] | layer_22[2.5-11.1% std=2.11]
  Expert: layer_1[5.8-8.0% std=0.46]
  Expert: layer_2[5.8-6.8% std=0.29]
  Expert: layer_3[3.9-14.3% std=2.35]
  Expert: layer_4[5.8-6.8% std=0.25]
  Expert: layer_6[2.9-11.8% std=1.66]
  Expert: layer_7[2.1-17.8% std=3.20]
  Expert: layer_8[2.3-13.9% std=2.47]
  Expert: layer_9[3.5-9.2% std=1.22]
  Expert: layer_10[2.9-15.3% std=2.64]
  Expert: layer_12[1.6-18.2% std=3.85]
  Expert: layer_13[4.0-9.5% std=1.37]
  Expert: layer_14[2.6-21.2% std=4.05]
  Expert: layer_15[2.9-15.4% std=3.04]
  Expert: layer_16[2.4-15.1% std=2.72]
  Expert: layer_18[3.4-13.1% std=2.33]
  Expert: layer_19[4.4-9.1% std=1.39]
  Expert: layer_20[3.2-12.9% std=2.28]
  Expert: layer_21[3.2-12.2% std=2.09]
Step     237 | Loss: 4.3449 | LR: 1.48e-04 | GradNorm: 0.79 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.76] | layer_5[5.9-6.7% std=0.22] | layer_11[2.8-12.8% std=2.11] | layer_17[3.4-8.7% std=1.33] | layer_22[1.9-10.1% std=2.01]
  Expert: layer_1[5.7-7.8% std=0.45]
  Expert: layer_2[5.8-6.8% std=0.26]
  Expert: layer_3[3.9-13.8% std=2.17]
  Expert: layer_4[5.8-6.9% std=0.30]
  Expert: layer_6[2.9-11.2% std=1.54]
  Expert: layer_7[2.3-17.6% std=3.17]
  Expert: layer_8[2.4-14.3% std=2.50]
  Expert: layer_9[3.5-8.7% std=1.15]
  Expert: layer_10[3.1-14.9% std=2.58]
  Expert: layer_12[1.7-18.4% std=3.75]
  Expert: layer_13[4.3-9.5% std=1.21]
  Expert: layer_14[2.5-18.3% std=3.35]
  Expert: layer_15[2.9-12.0% std=2.54]
  Expert: layer_16[3.1-13.2% std=2.44]
  Expert: layer_18[3.2-9.6% std=1.83]
  Expert: layer_19[4.6-8.0% std=1.01]
  Expert: layer_20[2.8-10.0% std=1.89]
  Expert: layer_21[3.9-9.3% std=1.44]
Step     238 | Loss: 4.3824 | LR: 1.49e-04 | GradNorm: 0.76 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.74] | layer_5[5.8-6.9% std=0.27] | layer_11[2.9-13.1% std=2.35] | layer_17[2.9-8.5% std=1.34] | layer_22[2.2-11.7% std=2.41]
  Expert: layer_1[5.6-7.8% std=0.44]
  Expert: layer_2[5.9-6.6% std=0.21]
  Expert: layer_3[4.0-14.1% std=2.22]
  Expert: layer_4[5.7-7.1% std=0.36]
  Expert: layer_6[3.0-11.5% std=1.60]
  Expert: layer_7[2.1-18.6% std=3.40]
  Expert: layer_8[2.1-14.2% std=2.57]
  Expert: layer_9[3.6-8.4% std=1.03]
  Expert: layer_10[2.4-15.4% std=2.69]
  Expert: layer_12[1.9-19.6% std=4.19]
  Expert: layer_13[3.6-9.7% std=1.43]
  Expert: layer_14[2.2-20.0% std=3.86]
  Expert: layer_15[3.8-14.4% std=2.56]
  Expert: layer_16[2.8-16.6% std=2.93]
  Expert: layer_18[3.4-12.4% std=2.09]
  Expert: layer_19[4.5-8.3% std=1.06]
  Expert: layer_20[2.7-13.8% std=2.58]
  Expert: layer_21[3.4-11.2% std=1.70]
Step     239 | Loss: 4.3239 | LR: 1.49e-04 | GradNorm: 0.68 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.1% std=0.76] | layer_5[5.8-6.7% std=0.28] | layer_11[2.8-11.6% std=2.09] | layer_17[3.8-8.2% std=1.04] | layer_22[1.7-9.2% std=1.73]
  Expert: layer_1[5.6-7.8% std=0.45]
  Expert: layer_2[6.0-6.8% std=0.20]
  Expert: layer_3[4.2-13.7% std=2.10]
  Expert: layer_4[5.6-7.1% std=0.43]
  Expert: layer_6[2.9-11.1% std=1.54]
  Expert: layer_7[2.2-18.1% std=3.31]
  Expert: layer_8[2.4-14.5% std=2.52]
  Expert: layer_9[3.8-8.4% std=0.99]
  Expert: layer_10[2.7-15.7% std=2.70]
  Expert: layer_12[1.8-17.7% std=3.79]
  Expert: layer_13[4.1-10.1% std=1.42]
  Expert: layer_14[2.9-19.6% std=3.70]
  Expert: layer_15[2.6-12.5% std=2.23]
  Expert: layer_16[2.7-14.3% std=2.63]
  Expert: layer_18[3.5-10.1% std=1.65]
  Expert: layer_19[5.2-7.6% std=0.76]
  Expert: layer_20[2.8-11.2% std=2.15]
  Expert: layer_21[3.8-8.8% std=1.04]
Step     240 | Loss: 4.3435 | LR: 1.50e-04 | GradNorm: 0.60 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.76] | layer_5[5.7-6.8% std=0.28] | layer_11[2.6-13.2% std=2.25] | layer_17[4.1-8.0% std=0.94] | layer_22[2.0-11.2% std=2.17]
  Expert: layer_1[5.7-7.7% std=0.42]
  Expert: layer_2[6.0-6.6% std=0.18]
  Expert: layer_3[4.3-13.5% std=2.04]
  Expert: layer_4[5.5-7.2% std=0.39]
  Expert: layer_6[3.0-11.4% std=1.58]
  Expert: layer_7[2.4-18.0% std=3.23]
  Expert: layer_8[2.3-14.0% std=2.45]
  Expert: layer_9[4.0-8.4% std=0.95]
  Expert: layer_10[2.7-15.6% std=2.66]
  Expert: layer_12[1.7-18.0% std=3.80]
  Expert: layer_13[4.3-9.3% std=1.38]
  Expert: layer_14[2.8-18.6% std=3.43]
  Expert: layer_15[3.5-12.0% std=2.21]
  Expert: layer_16[2.7-14.6% std=2.64]
  Expert: layer_18[4.0-10.8% std=1.71]
  Expert: layer_19[4.6-8.6% std=0.99]
  Expert: layer_20[2.6-11.6% std=2.17]
  Expert: layer_21[3.7-11.0% std=1.77]
Step     241 | Loss: 4.3411 | LR: 1.51e-04 | GradNorm: 0.62 | Tok/s: 2577 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.1% std=0.76] | layer_5[5.8-6.7% std=0.25] | layer_11[3.0-14.0% std=2.36] | layer_17[4.5-8.5% std=1.14] | layer_22[1.8-11.2% std=2.12]
  Expert: layer_1[5.5-7.8% std=0.45]
  Expert: layer_2[6.0-6.5% std=0.16]
  Expert: layer_3[4.2-13.7% std=2.11]
  Expert: layer_4[5.3-7.2% std=0.46]
  Expert: layer_6[2.9-11.6% std=1.65]
  Expert: layer_7[2.5-18.9% std=3.43]
  Expert: layer_8[2.5-14.1% std=2.55]
  Expert: layer_9[3.7-8.4% std=1.02]
  Expert: layer_10[2.5-15.5% std=2.65]
  Expert: layer_12[1.8-18.8% std=3.97]
  Expert: layer_13[4.5-8.8% std=1.26]
  Expert: layer_14[2.7-19.3% std=3.60]
  Expert: layer_15[3.5-11.8% std=2.39]
  Expert: layer_16[2.8-14.6% std=2.62]
  Expert: layer_18[3.5-10.8% std=1.95]
  Expert: layer_19[3.7-8.7% std=1.25]
  Expert: layer_20[3.1-12.2% std=2.24]
  Expert: layer_21[4.0-9.9% std=1.56]
Step     242 | Loss: 4.2922 | LR: 1.51e-04 | GradNorm: 0.64 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.1% std=0.75] | layer_5[5.8-6.8% std=0.28] | layer_11[2.9-12.4% std=2.15] | layer_17[4.3-8.1% std=1.20] | layer_22[1.8-11.1% std=2.20]
  Expert: layer_1[5.5-7.8% std=0.44]
  Expert: layer_2[6.0-6.5% std=0.16]
  Expert: layer_3[4.2-13.6% std=2.09]
  Expert: layer_4[5.4-7.2% std=0.47]
  Expert: layer_6[2.7-11.5% std=1.66]
  Expert: layer_7[2.4-18.9% std=3.43]
  Expert: layer_8[2.5-15.0% std=2.61]
  Expert: layer_9[3.6-8.9% std=1.04]
  Expert: layer_10[2.3-16.4% std=2.85]
  Expert: layer_12[1.7-19.9% std=4.22]
  Expert: layer_13[4.4-9.0% std=1.40]
  Expert: layer_14[2.4-18.5% std=3.42]
  Expert: layer_15[3.5-11.0% std=1.88]
  Expert: layer_16[3.1-15.6% std=2.84]
  Expert: layer_18[3.6-11.2% std=1.86]
  Expert: layer_19[4.3-10.0% std=1.37]
  Expert: layer_20[2.9-10.0% std=1.90]
  Expert: layer_21[4.4-9.6% std=1.28]
Step     243 | Loss: 4.3577 | LR: 1.52e-04 | GradNorm: 0.58 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.76] | layer_5[5.8-6.8% std=0.25] | layer_11[3.0-13.2% std=2.20] | layer_17[4.7-8.8% std=1.06] | layer_22[2.0-10.1% std=2.25]
  Expert: layer_1[5.6-7.7% std=0.42]
  Expert: layer_2[5.8-6.7% std=0.23]
  Expert: layer_3[3.9-13.8% std=2.17]
  Expert: layer_4[5.2-7.2% std=0.52]
  Expert: layer_6[2.8-11.7% std=1.70]
  Expert: layer_7[2.4-18.6% std=3.37]
  Expert: layer_8[2.4-15.5% std=2.73]
  Expert: layer_9[3.6-8.7% std=1.02]
  Expert: layer_10[2.4-16.0% std=2.77]
  Expert: layer_12[1.8-19.0% std=3.99]
  Expert: layer_13[4.5-10.5% std=1.41]
  Expert: layer_14[2.6-17.9% std=3.27]
  Expert: layer_15[2.9-12.0% std=2.22]
  Expert: layer_16[2.4-14.9% std=2.67]
  Expert: layer_18[3.8-11.2% std=1.84]
  Expert: layer_19[3.7-9.0% std=1.32]
  Expert: layer_20[2.6-12.7% std=2.17]
  Expert: layer_21[3.8-9.6% std=1.38]
Step     244 | Loss: 4.2915 | LR: 1.52e-04 | GradNorm: 0.64 | Tok/s: 2590 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.73] | layer_5[5.3-7.0% std=0.43] | layer_11[2.6-13.6% std=2.28] | layer_17[3.9-8.5% std=1.14] | layer_22[2.3-11.6% std=2.30]
  Expert: layer_1[5.5-7.6% std=0.41]
  Expert: layer_2[5.8-6.7% std=0.26]
  Expert: layer_3[4.0-13.6% std=2.09]
  Expert: layer_4[5.5-7.3% std=0.54]
  Expert: layer_6[2.8-11.9% std=1.72]
  Expert: layer_7[2.3-19.0% std=3.48]
  Expert: layer_8[2.5-15.1% std=2.61]
  Expert: layer_9[3.6-9.1% std=1.17]
  Expert: layer_10[2.5-16.5% std=2.87]
  Expert: layer_12[1.8-18.9% std=4.00]
  Expert: layer_13[4.4-10.4% std=1.37]
  Expert: layer_14[2.8-17.3% std=3.13]
  Expert: layer_15[3.7-11.5% std=2.32]
  Expert: layer_16[2.6-17.3% std=3.28]
  Expert: layer_18[3.6-11.3% std=2.02]
  Expert: layer_19[3.7-8.2% std=1.35]
  Expert: layer_20[2.1-12.0% std=2.27]
  Expert: layer_21[3.8-10.7% std=1.64]
Step     245 | Loss: 4.3529 | LR: 1.53e-04 | GradNorm: 0.98 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.76] | layer_5[5.4-7.1% std=0.42] | layer_11[2.9-11.7% std=1.92] | layer_17[4.1-8.2% std=1.07] | layer_22[2.7-10.0% std=1.93]
  Expert: layer_1[5.6-7.7% std=0.41]
  Expert: layer_2[5.7-6.7% std=0.32]
  Expert: layer_3[3.7-13.9% std=2.19]
  Expert: layer_4[5.6-7.2% std=0.52]
  Expert: layer_6[2.8-11.7% std=1.68]
  Expert: layer_7[2.1-19.5% std=3.58]
  Expert: layer_8[2.2-15.1% std=2.62]
  Expert: layer_9[3.5-9.2% std=1.15]
  Expert: layer_10[2.2-17.8% std=3.21]
  Expert: layer_12[1.9-19.0% std=4.17]
  Expert: layer_13[4.4-8.7% std=1.36]
  Expert: layer_14[2.7-18.9% std=3.59]
  Expert: layer_15[3.1-11.5% std=2.02]
  Expert: layer_16[2.3-14.9% std=2.67]
  Expert: layer_18[3.5-11.2% std=1.89]
  Expert: layer_19[4.7-10.0% std=1.39]
  Expert: layer_20[3.0-12.1% std=2.17]
  Expert: layer_21[4.3-10.6% std=1.61]
Step     246 | Loss: 4.3547 | LR: 1.54e-04 | GradNorm: 1.25 | Tok/s: 2570 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.77] | layer_5[5.5-6.8% std=0.38] | layer_11[2.5-13.8% std=2.41] | layer_17[4.2-9.6% std=1.56] | layer_22[2.0-12.3% std=2.54]
  Expert: layer_1[5.5-7.7% std=0.45]
  Expert: layer_2[5.6-6.8% std=0.34]
  Expert: layer_3[3.4-14.6% std=2.41]
  Expert: layer_4[4.9-7.0% std=0.59]
  Expert: layer_6[2.7-11.7% std=1.74]
  Expert: layer_7[2.2-17.8% std=3.23]
  Expert: layer_8[2.1-15.6% std=2.81]
  Expert: layer_9[3.3-9.1% std=1.23]
  Expert: layer_10[2.4-15.4% std=2.73]
  Expert: layer_12[1.6-19.1% std=4.05]
  Expert: layer_13[4.8-10.9% std=1.48]
  Expert: layer_14[2.7-16.3% std=3.13]
  Expert: layer_15[2.4-12.9% std=2.88]
  Expert: layer_16[2.1-16.5% std=3.14]
  Expert: layer_18[3.6-11.3% std=2.02]
  Expert: layer_19[3.8-9.1% std=1.61]
  Expert: layer_20[2.3-9.9% std=2.31]
  Expert: layer_21[3.2-9.6% std=1.71]
Step     247 | Loss: 4.2698 | LR: 1.54e-04 | GradNorm: 0.60 | Tok/s: 2591 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.1% std=0.75] | layer_5[5.3-7.1% std=0.38] | layer_11[3.1-12.6% std=1.99] | layer_17[4.1-8.1% std=1.06] | layer_22[2.5-11.6% std=1.97]
  Expert: layer_1[5.6-7.7% std=0.43]
  Expert: layer_2[5.8-6.6% std=0.29]
  Expert: layer_3[3.7-14.3% std=2.30]
  Expert: layer_4[5.3-7.1% std=0.49]
  Expert: layer_6[2.9-11.4% std=1.65]
  Expert: layer_7[2.3-17.6% std=3.11]
  Expert: layer_8[2.2-14.1% std=2.37]
  Expert: layer_9[3.6-8.1% std=1.00]
  Expert: layer_10[2.6-15.3% std=2.64]
  Expert: layer_12[2.0-17.9% std=3.69]
  Expert: layer_13[4.3-9.6% std=1.45]
  Expert: layer_14[2.4-17.3% std=3.13]
  Expert: layer_15[2.9-11.7% std=1.99]
  Expert: layer_16[3.9-13.7% std=2.15]
  Expert: layer_18[3.6-10.9% std=1.63]
  Expert: layer_19[4.0-8.4% std=1.22]
  Expert: layer_20[2.8-10.0% std=1.83]
  Expert: layer_21[3.4-11.2% std=1.59]
Step     248 | Loss: 4.2941 | LR: 1.55e-04 | GradNorm: 0.76 | Tok/s: 2590 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.75] | layer_5[5.4-7.1% std=0.38] | layer_11[3.1-12.0% std=1.88] | layer_17[3.2-8.5% std=1.18] | layer_22[1.9-10.7% std=2.32]
  Expert: layer_1[5.5-7.7% std=0.42]
  Expert: layer_2[5.8-6.8% std=0.29]
  Expert: layer_3[3.8-13.7% std=2.15]
  Expert: layer_4[5.2-7.1% std=0.47]
  Expert: layer_6[3.0-11.2% std=1.56]
  Expert: layer_7[2.0-18.0% std=3.21]
  Expert: layer_8[2.3-14.0% std=2.33]
  Expert: layer_9[3.8-7.8% std=0.91]
  Expert: layer_10[2.4-15.5% std=2.66]
  Expert: layer_12[2.2-16.8% std=3.49]
  Expert: layer_13[4.1-9.4% std=1.46]
  Expert: layer_14[2.7-16.8% std=3.02]
  Expert: layer_15[3.7-10.5% std=1.72]
  Expert: layer_16[3.6-13.4% std=2.07]
  Expert: layer_18[4.0-11.3% std=1.61]
  Expert: layer_19[4.8-8.4% std=0.99]
  Expert: layer_20[3.0-12.3% std=2.09]
  Expert: layer_21[3.1-11.5% std=1.80]
Step     249 | Loss: 4.2840 | LR: 1.56e-04 | GradNorm: 0.64 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.73] | layer_5[5.6-8.0% std=0.56] | layer_11[2.5-13.0% std=2.15] | layer_17[4.2-9.8% std=1.18] | layer_22[1.9-10.4% std=2.16]
  Expert: layer_1[5.7-7.8% std=0.44]
  Expert: layer_2[5.7-6.8% std=0.34]
  Expert: layer_3[3.9-14.3% std=2.28]
  Expert: layer_4[5.6-7.1% std=0.43]
  Expert: layer_6[2.9-11.6% std=1.67]
  Expert: layer_7[2.1-18.2% std=3.27]
  Expert: layer_8[2.2-14.1% std=2.44]
  Expert: layer_9[3.5-8.8% std=1.31]
  Expert: layer_10[2.5-16.1% std=2.80]
  Expert: layer_12[1.8-15.6% std=3.35]
  Expert: layer_13[4.2-10.0% std=1.54]
  Expert: layer_14[2.8-17.3% std=3.06]
  Expert: layer_15[3.7-12.0% std=2.46]
  Expert: layer_16[3.0-14.4% std=2.59]
  Expert: layer_18[3.6-11.1% std=1.94]
  Expert: layer_19[4.6-8.3% std=1.26]
  Expert: layer_20[3.2-11.1% std=2.17]
  Expert: layer_21[3.2-10.4% std=1.55]
Step     250 | Loss: 4.3057 | LR: 1.56e-04 | GradNorm: 0.57 | Tok/s: 2593 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.74] | layer_5[5.4-7.8% std=0.53] | layer_11[2.5-12.5% std=2.06] | layer_17[4.3-8.9% std=1.20] | layer_22[2.1-11.6% std=2.27]
  Expert: layer_1[5.6-7.7% std=0.43]
  Expert: layer_2[5.7-6.8% std=0.31]
  Expert: layer_3[4.0-14.5% std=2.31]
  Expert: layer_4[5.8-7.0% std=0.35]
  Expert: layer_6[3.1-11.5% std=1.61]
  Expert: layer_7[1.9-18.5% std=3.37]
  Expert: layer_8[2.1-13.5% std=2.36]
  Expert: layer_9[3.8-8.7% std=1.24]
  Expert: layer_10[2.6-16.3% std=2.86]
  Expert: layer_12[2.0-16.7% std=3.48]
  Expert: layer_13[4.1-10.2% std=1.47]
  Expert: layer_14[2.7-17.1% std=3.09]
  Expert: layer_15[3.7-11.1% std=2.19]
  Expert: layer_16[3.1-15.1% std=2.62]
  Expert: layer_18[3.8-11.9% std=1.81]
  Expert: layer_19[4.3-7.9% std=1.09]
  Expert: layer_20[3.8-12.2% std=2.04]
  Expert: layer_21[3.1-11.8% std=1.83]
Step     251 | Loss: 4.2772 | LR: 1.57e-04 | GradNorm: 0.59 | Tok/s: 2573 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.72] | layer_5[5.5-7.6% std=0.50] | layer_11[2.8-10.6% std=1.69] | layer_17[4.5-8.4% std=1.02] | layer_22[2.3-8.6% std=1.62]
  Expert: layer_1[5.7-7.7% std=0.42]
  Expert: layer_2[5.8-6.8% std=0.29]
  Expert: layer_3[4.0-14.1% std=2.21]
  Expert: layer_4[5.6-6.8% std=0.35]
  Expert: layer_6[3.1-11.1% std=1.56]
  Expert: layer_7[1.9-19.0% std=3.46]
  Expert: layer_8[2.0-13.3% std=2.24]
  Expert: layer_9[3.9-8.7% std=1.05]
  Expert: layer_10[2.4-17.6% std=3.15]
  Expert: layer_12[2.1-16.4% std=3.48]
  Expert: layer_13[4.5-9.5% std=1.31]
  Expert: layer_14[2.7-18.2% std=3.30]
  Expert: layer_15[3.1-10.0% std=1.84]
  Expert: layer_16[3.0-13.2% std=2.12]
  Expert: layer_18[4.2-9.6% std=1.43]
  Expert: layer_19[4.8-8.7% std=1.16]
  Expert: layer_20[3.6-11.9% std=1.88]
  Expert: layer_21[3.6-11.3% std=1.57]
Step     252 | Loss: 4.2555 | LR: 1.58e-04 | GradNorm: 0.69 | Tok/s: 2587 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.72] | layer_5[5.6-7.7% std=0.53] | layer_11[2.6-13.0% std=2.14] | layer_17[4.3-7.8% std=0.98] | layer_22[2.4-9.9% std=1.91]
  Expert: layer_1[5.7-7.8% std=0.43]
  Expert: layer_2[5.9-6.8% std=0.25]
  Expert: layer_3[4.2-14.2% std=2.20]
  Expert: layer_4[5.5-6.9% std=0.34]
  Expert: layer_6[3.1-11.0% std=1.53]
  Expert: layer_7[2.0-19.0% std=3.45]
  Expert: layer_8[2.0-13.1% std=2.22]
  Expert: layer_9[3.9-9.1% std=1.20]
  Expert: layer_10[2.5-17.2% std=3.04]
  Expert: layer_12[2.1-16.7% std=3.44]
  Expert: layer_13[3.8-9.1% std=1.24]
  Expert: layer_14[3.0-19.2% std=3.51]
  Expert: layer_15[3.3-10.5% std=2.16]
  Expert: layer_16[3.0-14.5% std=2.54]
  Expert: layer_18[3.3-10.9% std=1.80]
  Expert: layer_19[4.4-9.2% std=1.12]
  Expert: layer_20[4.3-9.4% std=1.48]
  Expert: layer_21[3.5-11.8% std=1.87]
Step     253 | Loss: 4.2182 | LR: 1.58e-04 | GradNorm: 0.70 | Tok/s: 2585 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.1% std=0.76] | layer_5[5.5-7.4% std=0.48] | layer_11[2.6-12.4% std=2.16] | layer_17[4.7-8.0% std=1.05] | layer_22[1.7-11.2% std=2.21]
  Expert: layer_1[5.6-7.8% std=0.44]
  Expert: layer_2[5.8-6.7% std=0.28]
  Expert: layer_3[3.7-14.0% std=2.17]
  Expert: layer_4[5.4-7.0% std=0.38]
  Expert: layer_6[3.1-10.8% std=1.50]
  Expert: layer_7[2.0-18.6% std=3.39]
  Expert: layer_8[2.0-13.8% std=2.34]
  Expert: layer_9[4.0-8.9% std=1.13]
  Expert: layer_10[2.4-16.9% std=2.99]
  Expert: layer_12[2.0-17.0% std=3.58]
  Expert: layer_13[4.8-8.5% std=1.04]
  Expert: layer_14[2.9-17.7% std=3.15]
  Expert: layer_15[2.9-10.8% std=2.15]
  Expert: layer_16[3.0-13.7% std=2.35]
  Expert: layer_18[3.6-8.6% std=1.36]
  Expert: layer_19[4.0-8.6% std=1.34]
  Expert: layer_20[4.0-11.8% std=1.99]
  Expert: layer_21[3.6-10.3% std=1.89]
Step     254 | Loss: 4.1936 | LR: 1.59e-04 | GradNorm: 0.68 | Tok/s: 2581 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.73] | layer_5[5.4-6.9% std=0.43] | layer_11[2.7-12.9% std=2.24] | layer_17[4.7-7.9% std=0.95] | layer_22[2.5-10.8% std=1.97]
  Expert: layer_1[5.9-7.6% std=0.39]
  Expert: layer_2[5.9-6.8% std=0.27]
  Expert: layer_3[3.8-14.6% std=2.27]
  Expert: layer_4[5.7-6.9% std=0.32]
  Expert: layer_6[3.3-10.8% std=1.46]
  Expert: layer_7[2.3-18.2% std=3.26]
  Expert: layer_8[2.1-14.3% std=2.43]
  Expert: layer_9[3.7-8.4% std=1.16]
  Expert: layer_10[2.6-16.4% std=2.86]
  Expert: layer_12[2.3-17.2% std=3.49]
  Expert: layer_13[4.3-9.2% std=1.24]
  Expert: layer_14[2.6-16.9% std=3.04]
  Expert: layer_15[3.1-9.7% std=1.80]
  Expert: layer_16[2.9-14.8% std=2.65]
  Expert: layer_18[4.7-9.4% std=1.13]
  Expert: layer_19[4.4-7.9% std=1.00]
  Expert: layer_20[3.8-10.5% std=1.69]
  Expert: layer_21[2.9-10.5% std=1.67]
Step     255 | Loss: 4.2960 | LR: 1.59e-04 | GradNorm: 0.54 | Tok/s: 2575 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.75] | layer_5[5.5-7.0% std=0.46] | layer_11[2.8-12.4% std=2.18] | layer_17[4.5-8.5% std=1.09] | layer_22[1.7-10.1% std=2.09]
  Expert: layer_1[5.7-7.7% std=0.43]
  Expert: layer_2[5.7-7.0% std=0.35]
  Expert: layer_3[3.8-14.7% std=2.31]
  Expert: layer_4[5.5-6.9% std=0.34]
  Expert: layer_6[3.2-10.8% std=1.46]
  Expert: layer_7[2.0-18.8% std=3.40]
  Expert: layer_8[2.0-14.1% std=2.41]
  Expert: layer_9[3.8-8.3% std=1.07]
  Expert: layer_10[2.6-17.0% std=2.97]
  Expert: layer_12[2.1-17.7% std=3.68]
  Expert: layer_13[4.3-9.3% std=1.33]
  Expert: layer_14[2.7-16.9% std=3.01]
  Expert: layer_15[3.2-9.1% std=1.65]
  Expert: layer_16[3.2-14.3% std=2.46]
  Expert: layer_18[4.0-9.4% std=1.28]
  Expert: layer_19[4.8-8.3% std=1.13]
  Expert: layer_20[3.4-12.3% std=2.12]
  Expert: layer_21[3.6-9.9% std=1.29]
Step     256 | Loss: 4.2064 | LR: 1.60e-04 | GradNorm: 0.62 | Tok/s: 2569 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.73] | layer_5[5.3-6.9% std=0.47] | layer_11[2.9-13.2% std=2.25] | layer_17[4.4-8.4% std=1.01] | layer_22[1.5-10.6% std=2.24]
  Expert: layer_1[5.8-7.7% std=0.42]
  Expert: layer_2[5.8-7.2% std=0.37]
  Expert: layer_3[3.8-14.5% std=2.25]
  Expert: layer_4[5.6-6.8% std=0.37]
  Expert: layer_6[3.3-11.1% std=1.50]
  Expert: layer_7[1.9-18.2% std=3.27]
  Expert: layer_8[2.2-13.9% std=2.33]
  Expert: layer_9[3.8-8.2% std=1.06]
  Expert: layer_10[2.9-16.2% std=2.76]
  Expert: layer_12[2.1-17.4% std=3.58]
  Expert: layer_13[4.3-8.8% std=1.20]
  Expert: layer_14[2.6-16.0% std=2.76]
  Expert: layer_15[3.4-9.5% std=1.72]
  Expert: layer_16[3.3-13.3% std=2.22]
  Expert: layer_18[4.4-9.3% std=1.26]
  Expert: layer_19[4.5-8.6% std=1.00]
  Expert: layer_20[3.2-10.4% std=1.86]
  Expert: layer_21[4.1-10.1% std=1.43]
Step     257 | Loss: 4.2106 | LR: 1.61e-04 | GradNorm: 0.68 | Tok/s: 2560 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.73] | layer_5[5.4-7.2% std=0.52] | layer_11[2.8-13.2% std=2.32] | layer_17[4.8-8.2% std=1.06] | layer_22[2.3-10.3% std=2.02]
  Expert: layer_1[5.7-7.7% std=0.41]
  Expert: layer_2[5.8-7.2% std=0.34]
  Expert: layer_3[3.8-14.8% std=2.33]
  Expert: layer_4[5.7-6.8% std=0.34]
  Expert: layer_6[3.2-11.3% std=1.56]
  Expert: layer_7[2.0-18.5% std=3.35]
  Expert: layer_8[2.1-13.7% std=2.33]
  Expert: layer_9[3.6-8.2% std=1.18]
  Expert: layer_10[2.7-16.3% std=2.83]
  Expert: layer_12[1.9-17.1% std=3.65]
  Expert: layer_13[4.4-9.6% std=1.50]
  Expert: layer_14[2.5-18.0% std=3.27]
  Expert: layer_15[3.6-10.0% std=1.88]
  Expert: layer_16[2.8-14.2% std=2.52]
  Expert: layer_18[4.1-9.9% std=1.48]
  Expert: layer_19[4.1-9.3% std=1.26]
  Expert: layer_20[4.1-9.2% std=1.77]
  Expert: layer_21[3.9-9.9% std=1.39]
Step     258 | Loss: 4.1967 | LR: 1.61e-04 | GradNorm: 0.79 | Tok/s: 2561 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.73] | layer_5[5.3-7.0% std=0.48] | layer_11[2.7-13.6% std=2.40] | layer_17[3.8-8.4% std=1.18] | layer_22[1.4-12.2% std=2.47]
  Expert: layer_1[5.6-7.7% std=0.41]
  Expert: layer_2[5.8-7.2% std=0.35]
  Expert: layer_3[3.9-14.7% std=2.30]
  Expert: layer_4[5.5-6.7% std=0.36]
  Expert: layer_6[3.4-11.3% std=1.50]
  Expert: layer_7[1.9-18.3% std=3.34]
  Expert: layer_8[2.2-14.5% std=2.48]
  Expert: layer_9[3.8-8.4% std=1.08]
  Expert: layer_10[2.5-16.0% std=2.78]
  Expert: layer_12[1.8-18.0% std=3.74]
  Expert: layer_13[4.5-9.9% std=1.43]
  Expert: layer_14[2.2-14.9% std=2.63]
  Expert: layer_15[4.0-9.5% std=1.57]
  Expert: layer_16[3.1-14.0% std=2.38]
  Expert: layer_18[5.2-8.9% std=0.99]
  Expert: layer_19[3.7-7.8% std=0.94]
  Expert: layer_20[3.4-12.8% std=2.23]
  Expert: layer_21[4.0-10.4% std=1.60]
Step     259 | Loss: 4.2303 | LR: 1.62e-04 | GradNorm: 0.91 | Tok/s: 2560 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.72] | layer_5[5.3-6.9% std=0.46] | layer_11[3.0-12.2% std=2.12] | layer_17[4.6-8.6% std=1.29] | layer_22[2.2-10.8% std=2.04]
  Expert: layer_1[5.6-7.6% std=0.40]
  Expert: layer_2[5.9-7.1% std=0.31]
  Expert: layer_3[3.7-14.5% std=2.26]
  Expert: layer_4[5.5-6.7% std=0.38]
  Expert: layer_6[3.3-11.2% std=1.51]
  Expert: layer_7[2.0-18.3% std=3.30]
  Expert: layer_8[2.3-14.5% std=2.49]
  Expert: layer_9[3.7-8.2% std=1.07]
  Expert: layer_10[2.8-17.1% std=3.01]
  Expert: layer_12[2.2-16.4% std=3.51]
  Expert: layer_13[4.4-8.7% std=1.28]
  Expert: layer_14[1.9-16.8% std=3.13]
  Expert: layer_15[3.0-9.0% std=1.52]
  Expert: layer_16[3.4-14.7% std=2.62]
  Expert: layer_18[3.6-10.2% std=1.57]
  Expert: layer_19[4.1-10.2% std=1.55]
  Expert: layer_20[3.2-10.3% std=1.98]
  Expert: layer_21[4.0-11.1% std=1.75]
Step     260 | Loss: 4.1710 | LR: 1.62e-04 | GradNorm: 0.75 | Tok/s: 2544 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.73] | layer_5[5.4-7.0% std=0.43] | layer_11[3.2-13.5% std=2.25] | layer_17[3.8-8.0% std=1.21] | layer_22[1.4-9.5% std=1.88]
  Expert: layer_1[5.6-7.7% std=0.41]
  Expert: layer_2[5.9-7.1% std=0.30]
  Expert: layer_3[4.1-14.0% std=2.12]
  Expert: layer_4[5.4-6.9% std=0.39]
  Expert: layer_6[3.7-10.8% std=1.37]
  Expert: layer_7[2.3-17.6% std=3.10]
  Expert: layer_8[2.3-13.4% std=2.27]
  Expert: layer_9[3.8-8.4% std=1.01]
  Expert: layer_10[2.8-15.1% std=2.49]
  Expert: layer_12[2.1-15.0% std=3.00]
  Expert: layer_13[4.4-8.7% std=1.20]
  Expert: layer_14[2.2-14.1% std=2.49]
  Expert: layer_15[3.9-9.3% std=1.45]
  Expert: layer_16[3.3-12.6% std=2.00]
  Expert: layer_18[4.8-8.1% std=1.03]
  Expert: layer_19[4.8-8.0% std=1.00]
  Expert: layer_20[4.1-11.9% std=1.71]
  Expert: layer_21[4.0-9.4% std=1.31]
Step     261 | Loss: 4.1872 | LR: 1.63e-04 | GradNorm: 0.59 | Tok/s: 2577 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.1% std=0.74] | layer_5[5.6-7.2% std=0.44] | layer_11[3.0-13.5% std=2.41] | layer_17[4.2-7.8% std=0.89] | layer_22[1.7-10.4% std=2.06]
  Expert: layer_1[5.5-7.7% std=0.42]
  Expert: layer_2[5.8-6.9% std=0.30]
  Expert: layer_3[4.4-14.3% std=2.15]
  Expert: layer_4[5.4-6.9% std=0.37]
  Expert: layer_6[3.2-10.8% std=1.41]
  Expert: layer_7[2.3-17.8% std=3.17]
  Expert: layer_8[2.1-13.6% std=2.31]
  Expert: layer_9[3.4-8.4% std=1.11]
  Expert: layer_10[2.7-15.4% std=2.59]
  Expert: layer_12[2.2-16.5% std=3.48]
  Expert: layer_13[4.6-9.3% std=1.28]
  Expert: layer_14[2.2-15.1% std=2.66]
  Expert: layer_15[3.7-9.4% std=1.40]
  Expert: layer_16[2.7-14.6% std=2.58]
  Expert: layer_18[4.9-9.9% std=1.22]
  Expert: layer_19[4.1-10.1% std=1.27]
  Expert: layer_20[3.8-11.7% std=1.85]
  Expert: layer_21[3.6-10.4% std=1.44]
Step     262 | Loss: 4.2325 | LR: 1.64e-04 | GradNorm: 0.65 | Tok/s: 2592 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.73] | layer_5[5.3-6.9% std=0.43] | layer_11[3.4-13.5% std=2.32] | layer_17[5.0-8.1% std=0.91] | layer_22[2.5-9.8% std=1.67]
  Expert: layer_1[5.7-7.6% std=0.38]
  Expert: layer_2[5.8-6.7% std=0.23]
  Expert: layer_3[4.4-14.4% std=2.21]
  Expert: layer_4[5.4-6.8% std=0.38]
  Expert: layer_6[3.2-11.3% std=1.51]
  Expert: layer_7[2.3-18.4% std=3.30]
  Expert: layer_8[2.0-13.4% std=2.30]
  Expert: layer_9[3.3-8.3% std=1.16]
  Expert: layer_10[2.7-16.7% std=2.92]
  Expert: layer_12[2.3-16.5% std=3.59]
  Expert: layer_13[4.6-9.1% std=1.34]
  Expert: layer_14[2.0-17.7% std=3.28]
  Expert: layer_15[4.0-9.6% std=1.70]
  Expert: layer_16[2.8-14.4% std=2.61]
  Expert: layer_18[4.1-10.8% std=1.65]
  Expert: layer_19[4.3-9.7% std=1.50]
  Expert: layer_20[3.2-11.0% std=1.99]
  Expert: layer_21[3.4-11.5% std=1.65]
Step     263 | Loss: 4.2092 | LR: 1.64e-04 | GradNorm: 0.55 | Tok/s: 2550 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.73] | layer_5[5.4-6.8% std=0.43] | layer_11[3.3-14.3% std=2.44] | layer_17[5.0-7.6% std=0.82] | layer_22[1.3-10.3% std=2.08]
  Expert: layer_1[5.6-7.6% std=0.39]
  Expert: layer_2[5.8-6.7% std=0.26]
  Expert: layer_3[4.3-14.0% std=2.11]
  Expert: layer_4[5.4-6.9% std=0.40]
  Expert: layer_6[3.3-11.1% std=1.45]
  Expert: layer_7[2.3-17.5% std=3.09]
  Expert: layer_8[2.1-13.5% std=2.28]
  Expert: layer_9[3.4-8.4% std=1.09]
  Expert: layer_10[2.9-15.0% std=2.48]
  Expert: layer_12[2.3-16.7% std=3.35]
  Expert: layer_13[4.5-9.8% std=1.24]
  Expert: layer_14[2.5-15.3% std=2.66]
  Expert: layer_15[4.1-11.1% std=1.77]
  Expert: layer_16[2.7-13.7% std=2.38]
  Expert: layer_18[4.4-9.5% std=1.47]
  Expert: layer_19[4.4-7.6% std=0.85]
  Expert: layer_20[3.1-10.9% std=1.80]
  Expert: layer_21[3.8-10.4% std=1.49]
Step     264 | Loss: 4.1465 | LR: 1.65e-04 | GradNorm: 0.51 | Tok/s: 2560 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.1% std=0.75] | layer_5[5.5-6.9% std=0.37] | layer_11[3.8-12.5% std=2.04] | layer_17[4.8-8.2% std=0.96] | layer_22[1.5-8.9% std=1.78]
  Expert: layer_1[5.6-7.6% std=0.40]
  Expert: layer_2[5.7-6.7% std=0.25]
  Expert: layer_3[4.5-13.6% std=2.01]
  Expert: layer_4[5.5-6.9% std=0.37]
  Expert: layer_6[3.3-10.8% std=1.37]
  Expert: layer_7[2.7-16.7% std=2.88]
  Expert: layer_8[2.3-12.9% std=2.12]
  Expert: layer_9[3.4-8.6% std=1.07]
  Expert: layer_10[3.1-14.1% std=2.27]
  Expert: layer_12[2.5-15.4% std=3.06]
  Expert: layer_13[4.8-9.3% std=1.11]
  Expert: layer_14[3.0-14.5% std=2.43]
  Expert: layer_15[4.0-9.4% std=1.44]
  Expert: layer_16[3.5-11.8% std=1.82]
  Expert: layer_18[5.2-7.8% std=0.88]
  Expert: layer_19[4.9-7.6% std=0.80]
  Expert: layer_20[3.3-12.0% std=1.84]
  Expert: layer_21[3.7-9.3% std=1.18]
Step     265 | Loss: 4.2107 | LR: 1.66e-04 | GradNorm: 0.49 | Tok/s: 2551 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.73] | layer_5[5.5-6.9% std=0.39] | layer_11[3.8-11.7% std=1.88] | layer_17[5.1-8.0% std=0.89] | layer_22[1.9-9.7% std=1.65]
  Expert: layer_1[5.6-7.5% std=0.39]
  Expert: layer_2[5.6-6.6% std=0.24]
  Expert: layer_3[4.5-13.8% std=2.08]
  Expert: layer_4[5.6-6.8% std=0.33]
  Expert: layer_6[3.4-10.8% std=1.37]
  Expert: layer_7[2.8-17.4% std=3.03]
  Expert: layer_8[2.2-12.7% std=2.12]
  Expert: layer_9[3.5-8.6% std=1.05]
  Expert: layer_10[3.1-14.9% std=2.48]
  Expert: layer_12[2.4-16.2% std=3.34]
  Expert: layer_13[4.0-9.2% std=1.22]
  Expert: layer_14[2.9-16.7% std=3.02]
  Expert: layer_15[4.0-9.1% std=1.44]
  Expert: layer_16[3.4-13.4% std=2.22]
  Expert: layer_18[4.8-9.9% std=1.31]
  Expert: layer_19[4.9-8.0% std=1.07]
  Expert: layer_20[3.4-11.5% std=1.87]
  Expert: layer_21[3.9-9.7% std=1.31]
Step     266 | Loss: 4.1996 | LR: 1.66e-04 | GradNorm: 0.48 | Tok/s: 2562 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.71] | layer_5[5.6-7.1% std=0.39] | layer_11[3.5-11.7% std=1.88] | layer_17[4.6-8.0% std=0.82] | layer_22[1.8-10.2% std=1.81]
  Expert: layer_1[5.6-7.5% std=0.38]
  Expert: layer_2[5.7-6.7% std=0.22]
  Expert: layer_3[4.6-14.2% std=2.16]
  Expert: layer_4[5.6-6.7% std=0.31]
  Expert: layer_6[3.6-10.7% std=1.33]
  Expert: layer_7[2.8-17.1% std=3.01]
  Expert: layer_8[2.4-13.3% std=2.23]
  Expert: layer_9[3.7-8.5% std=0.97]
  Expert: layer_10[3.2-14.9% std=2.45]
  Expert: layer_12[2.4-16.9% std=3.36]
  Expert: layer_13[4.3-8.8% std=1.09]
  Expert: layer_14[2.7-16.1% std=2.84]
  Expert: layer_15[4.0-9.9% std=1.46]
  Expert: layer_16[3.7-13.6% std=2.19]
  Expert: layer_18[4.4-9.2% std=1.16]
  Expert: layer_19[4.6-8.6% std=1.17]
  Expert: layer_20[3.4-10.6% std=1.85]
  Expert: layer_21[3.5-9.9% std=1.42]
Step     267 | Loss: 4.1707 | LR: 1.67e-04 | GradNorm: 0.42 | Tok/s: 2548 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.71] | layer_5[5.5-7.1% std=0.41] | layer_11[3.7-11.5% std=1.80] | layer_17[4.7-7.9% std=0.92] | layer_22[1.8-9.5% std=1.61]
  Expert: layer_1[5.7-7.6% std=0.40]
  Expert: layer_2[5.7-6.5% std=0.19]
  Expert: layer_3[4.7-14.3% std=2.19]
  Expert: layer_4[5.6-6.7% std=0.27]
  Expert: layer_6[3.5-10.8% std=1.37]
  Expert: layer_7[2.8-17.2% std=3.03]
  Expert: layer_8[2.3-13.3% std=2.21]
  Expert: layer_9[3.9-8.2% std=0.90]
  Expert: layer_10[3.0-15.6% std=2.62]
  Expert: layer_12[2.6-16.7% std=3.30]
  Expert: layer_13[4.3-8.8% std=1.13]
  Expert: layer_14[3.2-16.0% std=2.83]
  Expert: layer_15[4.1-9.6% std=1.51]
  Expert: layer_16[3.9-12.9% std=2.00]
  Expert: layer_18[3.7-8.2% std=1.19]
  Expert: layer_19[4.6-9.2% std=1.18]
  Expert: layer_20[3.4-10.7% std=1.78]
  Expert: layer_21[4.2-9.3% std=1.12]
Step     268 | Loss: 4.1532 | LR: 1.68e-04 | GradNorm: 0.47 | Tok/s: 2539 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.72] | layer_5[5.8-7.0% std=0.35] | layer_11[3.8-11.0% std=1.75] | layer_17[4.4-7.8% std=0.91] | layer_22[2.1-8.8% std=1.50]
  Expert: layer_1[5.7-7.5% std=0.37]
  Expert: layer_2[5.8-6.5% std=0.19]
  Expert: layer_3[4.7-14.2% std=2.18]
  Expert: layer_4[5.2-6.6% std=0.35]
  Expert: layer_6[3.4-10.8% std=1.40]
  Expert: layer_7[2.9-16.9% std=2.92]
  Expert: layer_8[2.4-13.5% std=2.20]
  Expert: layer_9[4.1-8.1% std=0.86]
  Expert: layer_10[3.2-15.3% std=2.55]
  Expert: layer_12[2.4-16.4% std=3.27]
  Expert: layer_13[3.9-9.4% std=1.27]
  Expert: layer_14[3.2-15.0% std=2.55]
  Expert: layer_15[4.1-9.5% std=1.45]
  Expert: layer_16[4.0-12.2% std=1.92]
  Expert: layer_18[4.4-8.1% std=0.99]
  Expert: layer_19[4.5-8.4% std=0.96]
  Expert: layer_20[3.7-11.6% std=1.83]
  Expert: layer_21[4.0-8.3% std=1.02]
Step     269 | Loss: 4.1553 | LR: 1.68e-04 | GradNorm: 0.48 | Tok/s: 2539 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.71] | layer_5[5.8-6.9% std=0.31] | layer_11[3.5-11.2% std=1.90] | layer_17[4.8-7.6% std=0.86] | layer_22[2.4-8.5% std=1.47]
  Expert: layer_1[5.6-7.6% std=0.40]
  Expert: layer_2[5.7-6.5% std=0.22]
  Expert: layer_3[4.4-14.8% std=2.32]
  Expert: layer_4[5.2-6.8% std=0.39]
  Expert: layer_6[3.5-11.1% std=1.45]
  Expert: layer_7[2.6-17.2% std=3.03]
  Expert: layer_8[2.6-14.0% std=2.33]
  Expert: layer_9[4.0-8.3% std=0.95]
  Expert: layer_10[3.4-15.8% std=2.62]
  Expert: layer_12[2.3-15.8% std=3.13]
  Expert: layer_13[3.8-9.4% std=1.33]
  Expert: layer_14[3.4-14.9% std=2.56]
  Expert: layer_15[4.1-8.9% std=1.36]
  Expert: layer_16[4.1-13.1% std=2.05]
  Expert: layer_18[4.2-8.5% std=1.01]
  Expert: layer_19[5.0-8.4% std=0.84]
  Expert: layer_20[3.7-10.6% std=1.62]
  Expert: layer_21[4.2-8.5% std=0.98]
Step     270 | Loss: 4.2098 | LR: 1.69e-04 | GradNorm: 0.61 | Tok/s: 2544 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.72] | layer_5[5.7-7.0% std=0.38] | layer_11[3.4-12.4% std=2.15] | layer_17[4.8-8.0% std=0.99] | layer_22[1.5-10.1% std=1.77]
  Expert: layer_1[5.7-7.5% std=0.38]
  Expert: layer_2[5.8-6.5% std=0.18]
  Expert: layer_3[4.4-15.1% std=2.44]
  Expert: layer_4[5.4-6.8% std=0.37]
  Expert: layer_6[3.4-11.2% std=1.49]
  Expert: layer_7[2.5-17.1% std=2.98]
  Expert: layer_8[2.5-14.6% std=2.53]
  Expert: layer_9[3.7-8.7% std=1.12]
  Expert: layer_10[3.3-15.8% std=2.64]
  Expert: layer_12[2.2-15.7% std=3.16]
  Expert: layer_13[3.9-9.5% std=1.34]
  Expert: layer_14[3.5-15.5% std=2.67]
  Expert: layer_15[3.9-10.3% std=1.69]
  Expert: layer_16[3.5-13.4% std=2.16]
  Expert: layer_18[4.1-9.0% std=1.30]
  Expert: layer_19[4.0-9.1% std=1.32]
  Expert: layer_20[3.8-11.6% std=1.81]
  Expert: layer_21[3.9-8.4% std=1.24]
Step     271 | Loss: 4.1671 | LR: 1.69e-04 | GradNorm: 0.70 | Tok/s: 2586 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.73] | layer_5[5.7-6.9% std=0.30] | layer_11[3.2-11.9% std=2.08] | layer_17[5.1-8.5% std=0.89] | layer_22[1.9-10.1% std=1.79]
  Expert: layer_1[5.7-7.8% std=0.42]
  Expert: layer_2[5.7-6.5% std=0.20]
  Expert: layer_3[4.4-14.6% std=2.30]
  Expert: layer_4[5.6-6.9% std=0.38]
  Expert: layer_6[3.3-10.8% std=1.42]
  Expert: layer_7[2.3-17.3% std=3.04]
  Expert: layer_8[2.3-14.6% std=2.48]
  Expert: layer_9[3.8-8.2% std=0.91]
  Expert: layer_10[3.1-16.7% std=2.84]
  Expert: layer_12[2.3-15.3% std=2.98]
  Expert: layer_13[4.0-9.3% std=1.37]
  Expert: layer_14[3.6-14.2% std=2.35]
  Expert: layer_15[3.8-8.8% std=1.29]
  Expert: layer_16[3.4-13.0% std=2.17]
  Expert: layer_18[3.9-8.1% std=1.03]
  Expert: layer_19[4.6-8.2% std=0.93]
  Expert: layer_20[3.3-9.6% std=1.50]
  Expert: layer_21[4.4-9.9% std=1.21]
Step     272 | Loss: 4.1544 | LR: 1.70e-04 | GradNorm: 0.80 | Tok/s: 2555 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.70] | layer_5[5.6-7.2% std=0.39] | layer_11[3.4-12.3% std=2.25] | layer_17[4.7-8.3% std=1.06] | layer_22[1.9-10.0% std=1.82]
  Expert: layer_1[5.8-7.5% std=0.36]
  Expert: layer_2[5.7-6.5% std=0.20]
  Expert: layer_3[4.3-14.5% std=2.28]
  Expert: layer_4[5.5-7.1% std=0.36]
  Expert: layer_6[3.3-11.5% std=1.55]
  Expert: layer_7[2.2-16.7% std=2.90]
  Expert: layer_8[2.6-15.6% std=2.77]
  Expert: layer_9[3.6-8.3% std=0.96]
  Expert: layer_10[2.9-15.0% std=2.45]
  Expert: layer_12[2.3-15.0% std=2.96]
  Expert: layer_13[4.0-8.7% std=1.26]
  Expert: layer_14[2.6-13.6% std=2.37]
  Expert: layer_15[4.0-9.3% std=1.44]
  Expert: layer_16[3.1-13.1% std=2.18]
  Expert: layer_18[4.3-8.7% std=1.11]
  Expert: layer_19[4.4-8.7% std=1.30]
  Expert: layer_20[3.9-13.2% std=2.03]
  Expert: layer_21[3.7-9.1% std=1.56]
Step     273 | Loss: 4.1365 | LR: 1.71e-04 | GradNorm: 1.02 | Tok/s: 2548 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.72] | layer_5[5.6-7.0% std=0.44] | layer_11[3.1-11.4% std=2.02] | layer_17[4.3-8.6% std=1.14] | layer_22[1.2-9.6% std=2.01]
  Expert: layer_1[5.9-7.7% std=0.39]
  Expert: layer_2[5.7-6.5% std=0.18]
  Expert: layer_3[4.1-14.1% std=2.16]
  Expert: layer_4[5.5-7.3% std=0.42]
  Expert: layer_6[3.2-11.2% std=1.53]
  Expert: layer_7[2.4-16.4% std=2.83]
  Expert: layer_8[2.7-15.5% std=2.64]
  Expert: layer_9[3.6-8.1% std=0.94]
  Expert: layer_10[3.4-15.3% std=2.55]
  Expert: layer_12[2.7-14.6% std=2.88]
  Expert: layer_13[4.5-8.9% std=1.14]
  Expert: layer_14[3.8-12.7% std=2.00]
  Expert: layer_15[2.9-12.3% std=2.00]
  Expert: layer_16[2.9-12.3% std=2.27]
  Expert: layer_18[4.1-8.2% std=1.21]
  Expert: layer_19[3.8-8.8% std=1.39]
  Expert: layer_20[2.8-10.2% std=1.90]
  Expert: layer_21[4.1-9.0% std=1.16]
Step     274 | Loss: 4.1739 | LR: 1.71e-04 | GradNorm: 1.15 | Tok/s: 2547 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.70] | layer_5[5.5-7.3% std=0.45] | layer_11[3.8-12.7% std=1.97] | layer_17[4.8-7.8% std=0.87] | layer_22[2.3-10.5% std=2.46]
  Expert: layer_1[5.9-7.6% std=0.37]
  Expert: layer_2[5.7-6.7% std=0.22]
  Expert: layer_3[4.1-14.1% std=2.18]
  Expert: layer_4[5.5-6.9% std=0.31]
  Expert: layer_6[3.4-11.3% std=1.50]
  Expert: layer_7[2.0-17.3% std=3.03]
  Expert: layer_8[2.5-13.5% std=2.26]
  Expert: layer_9[4.1-8.0% std=0.83]
  Expert: layer_10[3.1-16.0% std=2.67]
  Expert: layer_12[2.5-14.8% std=2.84]
  Expert: layer_13[3.6-8.0% std=1.18]
  Expert: layer_14[2.2-13.6% std=2.43]
  Expert: layer_15[4.2-8.6% std=1.14]
  Expert: layer_16[4.1-13.1% std=2.00]
  Expert: layer_18[3.1-8.7% std=1.57]
  Expert: layer_19[4.7-9.5% std=1.20]
  Expert: layer_20[3.5-8.4% std=1.65]
  Expert: layer_21[2.9-11.8% std=1.94]
Step     275 | Loss: 4.1422 | LR: 1.72e-04 | GradNorm: 0.83 | Tok/s: 2547 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.72] | layer_5[5.5-7.3% std=0.45] | layer_11[4.0-10.9% std=1.77] | layer_17[4.1-8.4% std=1.09] | layer_22[1.2-9.5% std=1.88]
  Expert: layer_1[5.9-7.7% std=0.39]
  Expert: layer_2[5.7-6.9% std=0.22]
  Expert: layer_3[4.2-13.7% std=2.06]
  Expert: layer_4[5.5-7.5% std=0.48]
  Expert: layer_6[3.4-10.6% std=1.37]
  Expert: layer_7[2.1-15.7% std=2.66]
  Expert: layer_8[2.8-14.6% std=2.44]
  Expert: layer_9[3.8-7.7% std=0.87]
  Expert: layer_10[3.0-14.5% std=2.32]
  Expert: layer_12[3.0-14.5% std=2.71]
  Expert: layer_13[4.4-8.5% std=1.06]
  Expert: layer_14[3.3-13.2% std=2.20]
  Expert: layer_15[2.9-8.6% std=1.35]
  Expert: layer_16[3.5-12.8% std=2.02]
  Expert: layer_18[4.6-8.5% std=0.90]
  Expert: layer_19[3.6-8.1% std=1.12]
  Expert: layer_20[4.2-10.9% std=1.62]
  Expert: layer_21[3.7-8.7% std=1.36]
Step     276 | Loss: 4.1642 | LR: 1.73e-04 | GradNorm: 0.60 | Tok/s: 2573 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.70] | layer_5[5.7-7.2% std=0.42] | layer_11[3.9-11.9% std=1.83] | layer_17[4.7-8.3% std=1.02] | layer_22[1.2-9.6% std=1.91]
  Expert: layer_1[5.9-7.6% std=0.36]
  Expert: layer_2[5.9-6.7% std=0.21]
  Expert: layer_3[4.4-13.7% std=2.03]
  Expert: layer_4[5.7-7.5% std=0.42]
  Expert: layer_6[3.3-10.6% std=1.37]
  Expert: layer_7[2.5-15.8% std=2.68]
  Expert: layer_8[2.5-13.2% std=2.22]
  Expert: layer_9[3.7-8.2% std=1.01]
  Expert: layer_10[3.3-13.7% std=2.16]
  Expert: layer_12[2.4-15.0% std=2.88]
  Expert: layer_13[4.5-9.3% std=1.15]
  Expert: layer_14[4.0-12.0% std=1.78]
  Expert: layer_15[3.8-11.4% std=1.67]
  Expert: layer_16[2.7-13.3% std=2.24]
  Expert: layer_18[4.4-8.4% std=1.07]
  Expert: layer_19[4.3-7.9% std=1.05]
  Expert: layer_20[3.4-9.4% std=1.63]
  Expert: layer_21[3.5-9.4% std=1.48]
Step     277 | Loss: 4.1761 | LR: 1.73e-04 | GradNorm: 0.59 | Tok/s: 2550 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.71] | layer_5[5.7-7.0% std=0.35] | layer_11[3.9-10.4% std=1.45] | layer_17[4.9-8.1% std=0.79] | layer_22[1.4-9.0% std=1.68]
  Expert: layer_1[6.0-7.5% std=0.35]
  Expert: layer_2[5.8-6.7% std=0.22]
  Expert: layer_3[4.4-13.4% std=1.94]
  Expert: layer_4[5.7-7.6% std=0.42]
  Expert: layer_6[3.3-10.6% std=1.39]
  Expert: layer_7[2.6-16.7% std=2.89]
  Expert: layer_8[2.4-12.8% std=2.11]
  Expert: layer_9[3.8-8.5% std=0.96]
  Expert: layer_10[3.1-14.7% std=2.38]
  Expert: layer_12[2.4-15.2% std=2.92]
  Expert: layer_13[4.3-8.2% std=1.03]
  Expert: layer_14[3.5-12.5% std=1.96]
  Expert: layer_15[4.0-9.7% std=1.37]
  Expert: layer_16[3.6-11.5% std=1.73]
  Expert: layer_18[3.6-7.6% std=1.16]
  Expert: layer_19[4.8-7.9% std=0.89]
  Expert: layer_20[3.4-8.9% std=1.42]
  Expert: layer_21[3.5-9.8% std=1.50]
Step     278 | Loss: 4.1961 | LR: 1.74e-04 | GradNorm: 0.57 | Tok/s: 2540 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.1-8.0% std=0.69] | layer_5[5.8-6.8% std=0.30] | layer_11[3.7-10.3% std=1.52] | layer_17[5.3-7.7% std=0.74] | layer_22[1.2-8.7% std=1.81]
  Expert: layer_1[5.9-7.6% std=0.36]
  Expert: layer_2[5.8-6.8% std=0.25]
  Expert: layer_3[4.6-13.3% std=1.92]
  Expert: layer_4[5.7-7.6% std=0.42]
  Expert: layer_6[3.3-10.5% std=1.37]
  Expert: layer_7[2.2-17.2% std=3.04]
  Expert: layer_8[2.3-13.5% std=2.31]
  Expert: layer_9[3.8-8.0% std=0.90]
  Expert: layer_10[2.5-15.0% std=2.49]
  Expert: layer_12[2.0-16.8% std=3.22]
  Expert: layer_13[4.1-8.0% std=1.01]
  Expert: layer_14[3.1-14.9% std=2.55]
  Expert: layer_15[3.7-9.1% std=1.15]
  Expert: layer_16[3.9-12.0% std=1.82]
  Expert: layer_18[4.0-8.8% std=1.16]
  Expert: layer_19[4.2-7.4% std=0.66]
  Expert: layer_20[4.1-9.8% std=1.33]
  Expert: layer_21[3.6-10.4% std=1.51]
Step     279 | Loss: 4.1195 | LR: 1.74e-04 | GradNorm: 0.59 | Tok/s: 2523 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.67] | layer_5[5.7-6.7% std=0.27] | layer_11[3.9-9.5% std=1.45] | layer_17[4.2-8.5% std=1.10] | layer_22[1.3-8.8% std=1.76]
  Expert: layer_1[5.9-7.6% std=0.37]
  Expert: layer_2[5.8-6.8% std=0.22]
  Expert: layer_3[4.3-13.2% std=1.90]
  Expert: layer_4[5.8-7.6% std=0.44]
  Expert: layer_6[3.4-10.6% std=1.37]
  Expert: layer_7[2.2-16.9% std=2.95]
  Expert: layer_8[2.7-13.4% std=2.28]
  Expert: layer_9[4.1-7.9% std=0.86]
  Expert: layer_10[2.5-14.5% std=2.36]
  Expert: layer_12[2.1-15.7% std=2.97]
  Expert: layer_13[4.4-7.6% std=0.95]
  Expert: layer_14[3.1-14.6% std=2.56]
  Expert: layer_15[3.9-8.1% std=1.13]
  Expert: layer_16[3.8-11.4% std=1.68]
  Expert: layer_18[4.7-7.8% std=0.92]
  Expert: layer_19[4.5-8.5% std=0.95]
  Expert: layer_20[4.0-10.7% std=1.46]
  Expert: layer_21[3.9-10.2% std=1.49]
Step     280 | Loss: 4.0946 | LR: 1.75e-04 | GradNorm: 0.60 | Tok/s: 2543 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-8.0% std=0.69] | layer_5[5.7-6.8% std=0.27] | layer_11[3.6-10.4% std=1.54] | layer_17[4.9-8.3% std=0.80] | layer_22[1.3-8.7% std=1.75]
  Expert: layer_1[5.9-7.7% std=0.39]
  Expert: layer_2[5.8-6.6% std=0.20]
  Expert: layer_3[4.3-13.5% std=1.97]
  Expert: layer_4[5.6-7.6% std=0.47]
  Expert: layer_6[3.4-10.7% std=1.38]
  Expert: layer_7[2.2-16.8% std=2.94]
  Expert: layer_8[2.5-13.2% std=2.17]
  Expert: layer_9[4.6-7.9% std=0.79]
  Expert: layer_10[2.5-14.1% std=2.29]
  Expert: layer_12[1.9-15.9% std=2.99]
  Expert: layer_13[4.1-8.1% std=1.04]
  Expert: layer_14[3.3-14.3% std=2.36]
  Expert: layer_15[4.2-9.6% std=1.19]
  Expert: layer_16[3.5-12.2% std=1.93]
  Expert: layer_18[4.7-8.7% std=0.98]
  Expert: layer_19[4.8-7.5% std=0.85]
  Expert: layer_20[3.7-9.3% std=1.42]
  Expert: layer_21[3.9-10.1% std=1.66]
Step     281 | Loss: 4.0962 | LR: 1.76e-04 | GradNorm: 0.63 | Tok/s: 2545 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.68] | layer_5[5.3-6.7% std=0.32] | layer_11[3.8-9.7% std=1.44] | layer_17[4.6-8.1% std=0.87] | layer_22[1.7-9.0% std=1.73]
  Expert: layer_1[5.8-7.6% std=0.38]
  Expert: layer_2[5.9-6.4% std=0.17]
  Expert: layer_3[4.4-13.1% std=1.86]
  Expert: layer_4[5.4-7.7% std=0.51]
  Expert: layer_6[3.3-10.7% std=1.40]
  Expert: layer_7[2.6-16.9% std=2.97]
  Expert: layer_8[2.8-12.5% std=1.99]
  Expert: layer_9[4.9-7.5% std=0.68]
  Expert: layer_10[2.5-14.2% std=2.36]
  Expert: layer_12[2.4-14.8% std=2.80]
  Expert: layer_13[5.0-8.0% std=0.90]
  Expert: layer_14[3.2-12.8% std=2.08]
  Expert: layer_15[4.1-9.0% std=1.14]
  Expert: layer_16[4.4-11.2% std=1.56]
  Expert: layer_18[4.3-8.2% std=0.90]
  Expert: layer_19[4.7-8.0% std=0.93]
  Expert: layer_20[3.4-8.9% std=1.35]
  Expert: layer_21[4.5-9.2% std=1.19]
Step     282 | Loss: 4.0479 | LR: 1.76e-04 | GradNorm: 0.65 | Tok/s: 2538 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.67] | layer_5[5.2-6.9% std=0.39] | layer_11[3.6-11.2% std=1.66] | layer_17[4.6-8.4% std=0.99] | layer_22[1.7-9.3% std=1.72]
  Expert: layer_1[5.8-7.5% std=0.37]
  Expert: layer_2[5.9-6.7% std=0.22]
  Expert: layer_3[4.4-13.5% std=1.95]
  Expert: layer_4[5.5-7.3% std=0.45]
  Expert: layer_6[3.4-10.9% std=1.44]
  Expert: layer_7[3.0-16.3% std=2.81]
  Expert: layer_8[3.0-11.9% std=1.88]
  Expert: layer_9[4.5-8.3% std=0.90]
  Expert: layer_10[2.6-13.6% std=2.23]
  Expert: layer_12[2.1-15.0% std=2.75]
  Expert: layer_13[4.5-7.5% std=0.93]
  Expert: layer_14[3.2-15.1% std=2.49]
  Expert: layer_15[4.0-11.0% std=1.83]
  Expert: layer_16[3.3-11.9% std=1.93]
  Expert: layer_18[3.7-8.4% std=1.46]
  Expert: layer_19[4.8-7.5% std=0.71]
  Expert: layer_20[4.1-8.8% std=1.40]
  Expert: layer_21[3.9-9.0% std=1.45]
Step     283 | Loss: 4.0719 | LR: 1.77e-04 | GradNorm: 0.63 | Tok/s: 2530 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.8% std=0.68] | layer_5[5.3-6.9% std=0.38] | layer_11[3.9-10.0% std=1.47] | layer_17[4.5-8.1% std=1.02] | layer_22[1.5-10.3% std=1.96]
  Expert: layer_1[5.8-7.6% std=0.37]
  Expert: layer_2[6.0-6.6% std=0.17]
  Expert: layer_3[4.3-13.7% std=2.03]
  Expert: layer_4[5.4-7.0% std=0.46]
  Expert: layer_6[3.5-10.7% std=1.36]
  Expert: layer_7[2.6-16.5% std=2.93]
  Expert: layer_8[2.9-13.2% std=2.09]
  Expert: layer_9[4.5-7.7% std=0.75]
  Expert: layer_10[2.6-14.3% std=2.32]
  Expert: layer_12[2.2-16.2% std=3.04]
  Expert: layer_13[5.1-8.2% std=0.96]
  Expert: layer_14[3.0-12.1% std=1.84]
  Expert: layer_15[4.6-8.9% std=1.11]
  Expert: layer_16[3.4-12.5% std=1.85]
  Expert: layer_18[4.9-7.7% std=0.87]
  Expert: layer_19[4.4-8.5% std=1.05]
  Expert: layer_20[3.5-10.1% std=1.60]
  Expert: layer_21[3.3-9.3% std=1.45]
Step     284 | Loss: 4.0637 | LR: 1.78e-04 | GradNorm: 0.61 | Tok/s: 2540 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.8% std=0.67] | layer_5[5.4-6.9% std=0.44] | layer_11[4.2-10.4% std=1.56] | layer_17[3.6-9.2% std=1.26] | layer_22[1.9-7.8% std=1.48]
  Expert: layer_1[5.9-7.4% std=0.33]
  Expert: layer_2[6.1-6.6% std=0.17]
  Expert: layer_3[4.3-13.9% std=2.10]
  Expert: layer_4[5.6-6.8% std=0.34]
  Expert: layer_6[3.4-10.7% std=1.39]
  Expert: layer_7[2.7-16.6% std=2.93]
  Expert: layer_8[2.7-12.8% std=2.09]
  Expert: layer_9[4.2-8.3% std=0.91]
  Expert: layer_10[2.6-14.7% std=2.43]
  Expert: layer_12[2.4-14.4% std=2.60]
  Expert: layer_13[4.6-7.9% std=0.94]
  Expert: layer_14[3.2-14.9% std=2.47]
  Expert: layer_15[4.4-9.5% std=1.40]
  Expert: layer_16[3.5-11.8% std=1.73]
  Expert: layer_18[4.5-8.2% std=0.94]
  Expert: layer_19[4.7-8.5% std=1.06]
  Expert: layer_20[4.1-9.2% std=1.35]
  Expert: layer_21[4.3-9.2% std=1.20]
Step     285 | Loss: 4.0725 | LR: 1.78e-04 | GradNorm: 0.68 | Tok/s: 2547 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.8% std=0.65] | layer_5[5.2-7.0% std=0.50] | layer_11[3.7-10.8% std=1.61] | layer_17[5.4-8.1% std=0.82] | layer_22[1.4-9.8% std=1.89]
  Expert: layer_1[5.9-7.5% std=0.36]
  Expert: layer_2[6.1-6.6% std=0.13]
  Expert: layer_3[4.2-13.7% std=2.06]
  Expert: layer_4[5.5-6.9% std=0.36]
  Expert: layer_6[3.6-10.7% std=1.35]
  Expert: layer_7[2.7-16.2% std=2.81]
  Expert: layer_8[2.7-12.6% std=2.01]
  Expert: layer_9[4.4-8.2% std=0.93]
  Expert: layer_10[2.6-14.1% std=2.31]
  Expert: layer_12[2.3-15.2% std=2.80]
  Expert: layer_13[4.5-7.9% std=0.89]
  Expert: layer_14[3.5-14.3% std=2.27]
  Expert: layer_15[4.1-9.3% std=1.35]
  Expert: layer_16[3.3-13.7% std=2.21]
  Expert: layer_18[4.8-8.6% std=1.14]
  Expert: layer_19[4.4-8.1% std=0.95]
  Expert: layer_20[4.3-9.9% std=1.48]
  Expert: layer_21[3.9-10.2% std=1.61]
Step     286 | Loss: 4.1096 | LR: 1.79e-04 | GradNorm: 0.79 | Tok/s: 2548 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.8% std=0.63] | layer_5[5.3-7.0% std=0.46] | layer_11[3.6-9.9% std=1.50] | layer_17[4.5-8.0% std=1.09] | layer_22[1.2-8.4% std=1.60]
  Expert: layer_1[6.0-7.4% std=0.32]
  Expert: layer_2[6.0-6.6% std=0.15]
  Expert: layer_3[4.3-13.7% std=2.06]
  Expert: layer_4[5.6-6.9% std=0.34]
  Expert: layer_6[3.7-10.8% std=1.39]
  Expert: layer_7[2.8-15.6% std=2.73]
  Expert: layer_8[2.4-13.2% std=2.27]
  Expert: layer_9[4.4-8.7% std=0.99]
  Expert: layer_10[2.8-13.8% std=2.25]
  Expert: layer_12[2.9-13.9% std=2.50]
  Expert: layer_13[4.5-8.9% std=1.12]
  Expert: layer_14[3.1-13.1% std=2.14]
  Expert: layer_15[4.4-9.0% std=1.24]
  Expert: layer_16[4.5-10.7% std=1.49]
  Expert: layer_18[4.3-7.8% std=0.91]
  Expert: layer_19[4.6-9.0% std=1.15]
  Expert: layer_20[3.8-11.3% std=1.70]
  Expert: layer_21[3.9-9.0% std=1.30]
Step     287 | Loss: 4.0838 | LR: 1.79e-04 | GradNorm: 0.69 | Tok/s: 2543 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.65] | layer_5[5.4-7.1% std=0.55] | layer_11[3.9-9.8% std=1.48] | layer_17[5.5-7.6% std=0.67] | layer_22[1.7-9.4% std=1.67]
  Expert: layer_1[5.9-7.5% std=0.34]
  Expert: layer_2[6.0-6.6% std=0.14]
  Expert: layer_3[4.1-13.5% std=2.01]
  Expert: layer_4[5.4-6.9% std=0.38]
  Expert: layer_6[3.6-10.6% std=1.33]
  Expert: layer_7[2.6-15.2% std=2.56]
  Expert: layer_8[2.6-12.8% std=2.10]
  Expert: layer_9[4.4-8.7% std=1.03]
  Expert: layer_10[3.3-13.2% std=2.04]
  Expert: layer_12[2.5-13.0% std=2.33]
  Expert: layer_13[4.7-8.3% std=0.89]
  Expert: layer_14[3.3-13.3% std=2.04]
  Expert: layer_15[4.4-9.4% std=1.44]
  Expert: layer_16[3.7-12.0% std=1.93]
  Expert: layer_18[5.1-8.1% std=0.94]
  Expert: layer_19[4.7-8.0% std=0.88]
  Expert: layer_20[3.7-7.8% std=1.19]
  Expert: layer_21[4.2-9.5% std=1.42]
Step     288 | Loss: 4.0700 | LR: 1.80e-04 | GradNorm: 0.62 | Tok/s: 2546 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.67] | layer_5[5.2-7.1% std=0.52] | layer_11[3.8-9.9% std=1.44] | layer_17[5.0-8.4% std=0.90] | layer_22[1.3-8.4% std=1.66]
  Expert: layer_1[6.0-7.5% std=0.34]
  Expert: layer_2[6.0-6.7% std=0.17]
  Expert: layer_3[4.3-13.8% std=2.09]
  Expert: layer_4[5.6-6.9% std=0.36]
  Expert: layer_6[3.8-10.4% std=1.29]
  Expert: layer_7[2.2-15.6% std=2.68]
  Expert: layer_8[2.2-13.3% std=2.28]
  Expert: layer_9[4.4-8.2% std=1.02]
  Expert: layer_10[2.9-13.8% std=2.25]
  Expert: layer_12[2.8-14.0% std=2.51]
  Expert: layer_13[4.3-8.0% std=0.96]
  Expert: layer_14[3.8-14.3% std=2.29]
  Expert: layer_15[3.9-8.5% std=1.16]
  Expert: layer_16[3.6-12.6% std=1.95]
  Expert: layer_18[4.5-8.7% std=1.06]
  Expert: layer_19[4.4-8.6% std=0.88]
  Expert: layer_20[4.3-9.7% std=1.38]
  Expert: layer_21[3.7-9.7% std=1.50]
Step     289 | Loss: 4.0437 | LR: 1.81e-04 | GradNorm: 0.65 | Tok/s: 2537 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.8% std=0.67] | layer_5[5.4-7.0% std=0.49] | layer_11[3.8-9.1% std=1.43] | layer_17[4.2-8.6% std=1.23] | layer_22[1.3-8.4% std=1.68]
  Expert: layer_1[6.0-7.4% std=0.31]
  Expert: layer_2[5.9-6.8% std=0.20]
  Expert: layer_3[4.2-13.5% std=1.99]
  Expert: layer_4[5.8-7.0% std=0.30]
  Expert: layer_6[3.8-10.4% std=1.26]
  Expert: layer_7[2.2-15.6% std=2.71]
  Expert: layer_8[2.3-13.8% std=2.36]
  Expert: layer_9[4.5-8.2% std=0.94]
  Expert: layer_10[2.7-13.7% std=2.22]
  Expert: layer_12[2.9-13.3% std=2.45]
  Expert: layer_13[4.9-7.8% std=0.77]
  Expert: layer_14[3.4-12.8% std=2.01]
  Expert: layer_15[4.4-8.2% std=1.21]
  Expert: layer_16[3.8-11.4% std=1.61]
  Expert: layer_18[4.5-7.6% std=0.78]
  Expert: layer_19[3.9-8.5% std=1.15]
  Expert: layer_20[3.2-10.7% std=1.52]
  Expert: layer_21[4.4-9.5% std=1.26]
Step     290 | Loss: 4.0559 | LR: 1.81e-04 | GradNorm: 0.48 | Tok/s: 2547 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-8.0% std=0.70] | layer_5[5.3-7.2% std=0.50] | layer_11[3.7-10.3% std=1.55] | layer_17[4.7-8.4% std=0.99] | layer_22[1.6-8.5% std=1.55]
  Expert: layer_1[6.0-7.3% std=0.30]
  Expert: layer_2[5.7-6.9% std=0.25]
  Expert: layer_3[4.1-13.9% std=2.10]
  Expert: layer_4[5.6-6.8% std=0.27]
  Expert: layer_6[3.9-10.5% std=1.28]
  Expert: layer_7[2.4-15.7% std=2.69]
  Expert: layer_8[2.1-14.4% std=2.49]
  Expert: layer_9[4.4-8.7% std=1.11]
  Expert: layer_10[2.9-13.5% std=2.15]
  Expert: layer_12[3.0-13.1% std=2.31]
  Expert: layer_13[4.6-8.0% std=0.95]
  Expert: layer_14[3.1-13.4% std=2.11]
  Expert: layer_15[4.4-9.3% std=1.23]
  Expert: layer_16[3.8-11.8% std=1.83]
  Expert: layer_18[4.5-7.6% std=0.79]
  Expert: layer_19[4.3-7.9% std=1.01]
  Expert: layer_20[3.8-8.4% std=1.16]
  Expert: layer_21[4.2-8.4% std=1.15]
Step     291 | Loss: 4.1006 | LR: 1.82e-04 | GradNorm: 0.49 | Tok/s: 2554 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.69] | layer_5[5.4-7.3% std=0.51] | layer_11[3.8-10.6% std=1.59] | layer_17[4.8-7.9% std=0.86] | layer_22[1.6-9.5% std=1.72]
  Expert: layer_1[6.0-7.4% std=0.32]
  Expert: layer_2[5.9-6.9% std=0.22]
  Expert: layer_3[3.9-13.9% std=2.11]
  Expert: layer_4[5.6-6.6% std=0.28]
  Expert: layer_6[3.7-10.7% std=1.36]
  Expert: layer_7[2.4-16.0% std=2.70]
  Expert: layer_8[2.2-13.8% std=2.36]
  Expert: layer_9[4.3-8.3% std=1.08]
  Expert: layer_10[2.9-13.3% std=2.12]
  Expert: layer_12[2.9-13.0% std=2.37]
  Expert: layer_13[4.5-8.1% std=0.99]
  Expert: layer_14[3.2-13.3% std=2.09]
  Expert: layer_15[4.1-8.7% std=1.25]
  Expert: layer_16[3.6-12.9% std=2.00]
  Expert: layer_18[4.8-8.2% std=0.99]
  Expert: layer_19[4.5-8.2% std=0.99]
  Expert: layer_20[4.1-9.2% std=1.25]
  Expert: layer_21[4.0-8.5% std=1.14]
Step     292 | Loss: 4.0085 | LR: 1.82e-04 | GradNorm: 0.49 | Tok/s: 2496 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.69] | layer_5[5.5-7.1% std=0.41] | layer_11[3.8-10.3% std=1.48] | layer_17[4.4-7.9% std=0.91] | layer_22[1.8-9.7% std=1.71]
  Expert: layer_1[5.8-7.4% std=0.34]
  Expert: layer_2[5.9-6.8% std=0.20]
  Expert: layer_3[4.0-13.5% std=2.01]
  Expert: layer_4[5.8-6.9% std=0.29]
  Expert: layer_6[3.8-10.4% std=1.27]
  Expert: layer_7[2.5-15.6% std=2.59]
  Expert: layer_8[2.3-13.9% std=2.32]
  Expert: layer_9[4.6-8.5% std=0.97]
  Expert: layer_10[3.2-12.1% std=1.83]
  Expert: layer_12[2.8-12.6% std=2.27]
  Expert: layer_13[4.7-7.8% std=0.83]
  Expert: layer_14[3.0-12.0% std=1.84]
  Expert: layer_15[4.6-9.1% std=1.17]
  Expert: layer_16[3.9-12.5% std=1.90]
  Expert: layer_18[5.1-7.6% std=0.75]
  Expert: layer_19[4.8-8.2% std=0.86]
  Expert: layer_20[3.7-8.9% std=1.06]
  Expert: layer_21[4.2-9.0% std=1.17]
Step     293 | Loss: 4.1001 | LR: 1.83e-04 | GradNorm: 0.60 | Tok/s: 2510 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.69] | layer_5[5.5-7.2% std=0.43] | layer_11[4.1-9.2% std=1.27] | layer_17[4.4-8.7% std=1.15] | layer_22[2.1-8.9% std=1.54]
  Expert: layer_1[5.7-7.4% std=0.35]
  Expert: layer_2[6.0-6.7% std=0.19]
  Expert: layer_3[4.1-13.6% std=2.03]
  Expert: layer_4[5.9-6.7% std=0.25]
  Expert: layer_6[3.9-10.1% std=1.23]
  Expert: layer_7[2.3-16.1% std=2.76]
  Expert: layer_8[2.2-14.1% std=2.38]
  Expert: layer_9[4.3-8.3% std=1.03]
  Expert: layer_10[3.2-12.8% std=2.00]
  Expert: layer_12[3.0-12.4% std=2.22]
  Expert: layer_13[4.7-8.1% std=0.96]
  Expert: layer_14[2.9-13.0% std=2.30]
  Expert: layer_15[4.1-8.4% std=1.18]
  Expert: layer_16[4.0-12.0% std=1.86]
  Expert: layer_18[4.6-7.4% std=0.74]
  Expert: layer_19[4.2-8.0% std=0.99]
  Expert: layer_20[3.8-7.9% std=1.05]
  Expert: layer_21[4.3-8.6% std=1.17]
Step     294 | Loss: 4.0339 | LR: 1.84e-04 | GradNorm: 0.63 | Tok/s: 2479 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[4.9-7.9% std=0.69] | layer_5[5.5-7.0% std=0.39] | layer_11[3.8-11.5% std=1.70] | layer_17[5.0-7.9% std=0.92] | layer_22[1.6-8.6% std=1.56]
  Expert: layer_1[5.7-7.5% std=0.38]
  Expert: layer_2[6.0-6.7% std=0.17]
  Expert: layer_3[4.2-13.8% std=2.07]
  Expert: layer_4[5.7-6.8% std=0.32]
  Expert: layer_6[4.1-10.2% std=1.21]
  Expert: layer_7[2.3-16.6% std=2.88]
  Expert: layer_8[2.2-14.5% std=2.52]
  Expert: layer_9[4.6-8.5% std=1.05]
  Expert: layer_10[3.3-12.6% std=1.93]
  Expert: layer_12[2.6-13.8% std=2.44]
  Expert: layer_13[4.5-9.0% std=1.05]
  Expert: layer_14[2.7-12.7% std=2.03]
  Expert: layer_15[4.5-9.9% std=1.26]
  Expert: layer_16[3.7-12.7% std=1.98]
  Expert: layer_18[4.5-8.7% std=1.08]
  Expert: layer_19[4.2-8.2% std=1.02]
  Expert: layer_20[4.5-9.0% std=1.04]
  Expert: layer_21[4.2-9.7% std=1.37]
Step     295 | Loss: 4.0494 | LR: 1.84e-04 | GradNorm: 0.78 | Tok/s: 2486 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.8% std=0.68] | layer_5[5.6-7.0% std=0.42] | layer_11[3.8-9.4% std=1.31] | layer_17[5.0-9.0% std=1.07] | layer_22[2.2-8.1% std=1.36]
  Expert: layer_1[5.7-7.4% std=0.32]
  Expert: layer_2[5.9-6.8% std=0.19]
  Expert: layer_3[4.4-13.6% std=2.00]
  Expert: layer_4[5.5-7.0% std=0.36]
  Expert: layer_6[4.1-10.2% std=1.24]
  Expert: layer_7[2.2-16.8% std=2.90]
  Expert: layer_8[2.4-13.5% std=2.24]
  Expert: layer_9[4.4-8.5% std=1.06]
  Expert: layer_10[3.4-13.1% std=2.01]
  Expert: layer_12[3.0-12.8% std=2.38]
  Expert: layer_13[4.2-8.4% std=1.08]
  Expert: layer_14[3.1-12.7% std=2.18]
  Expert: layer_15[4.1-8.5% std=1.18]
  Expert: layer_16[3.9-11.8% std=1.74]
  Expert: layer_18[4.6-7.9% std=0.87]
  Expert: layer_19[4.2-8.7% std=1.06]
  Expert: layer_20[4.4-8.7% std=1.14]
  Expert: layer_21[4.0-12.2% std=2.01]
Step     296 | Loss: 4.0082 | LR: 1.85e-04 | GradNorm: 0.82 | Tok/s: 2549 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.70] | layer_5[5.7-6.9% std=0.40] | layer_11[4.0-11.4% std=1.76] | layer_17[5.0-8.1% std=1.02] | layer_22[1.1-8.6% std=1.63]
  Expert: layer_1[5.8-7.7% std=0.39]
  Expert: layer_2[5.9-6.6% std=0.16]
  Expert: layer_3[4.4-13.3% std=1.94]
  Expert: layer_4[5.5-6.9% std=0.33]
  Expert: layer_6[3.8-9.7% std=1.13]
  Expert: layer_7[2.4-16.5% std=2.87]
  Expert: layer_8[2.4-13.8% std=2.36]
  Expert: layer_9[4.5-8.3% std=0.94]
  Expert: layer_10[3.4-12.2% std=1.79]
  Expert: layer_12[2.8-13.2% std=2.30]
  Expert: layer_13[4.6-8.8% std=0.93]
  Expert: layer_14[3.1-11.9% std=1.82]
  Expert: layer_15[4.1-10.6% std=1.55]
  Expert: layer_16[3.7-10.6% std=1.67]
  Expert: layer_18[4.0-8.9% std=1.26]
  Expert: layer_19[4.2-8.7% std=1.24]
  Expert: layer_20[4.2-8.2% std=1.04]
  Expert: layer_21[4.1-8.5% std=1.28]
Step     297 | Loss: 4.0342 | LR: 1.86e-04 | GradNorm: 0.64 | Tok/s: 2533 | Mem: 70.1GB | Data: train_2k/037.npy
  Expert: layer_0[5.0-7.9% std=0.70] | layer_5[5.7-7.0% std=0.36] | layer_11[4.2-9.3% std=1.38] | layer_17[4.3-8.9% std=1.15] | layer_22[1.2-8.3% std=1.64]
  Expert: layer_1[5.6-7.6% std=0.38]
  Expert: layer_2[5.9-6.6% std=0.19]
  Expert: layer_3[4.3-13.3% std=1.94]
  Expert: layer_4[5.3-6.7% std=0.37]
  Expert: layer_6[3.7-9.7% std=1.15]
  Expert: layer_7[2.3-17.0% std=3.00]
  Expert: layer_8[2.6-13.5% std=2.26]
  Expert: layer_9[4.5-7.4% std=0.78]
  Expert: layer_10[2.9-12.6% std=1.97]
  Expert: layer_12[2.9-13.9% std=2.49]
  Expert: layer_13[4.8-7.8% std=0.89]
  Expert: layer_14[3.5-11.8% std=2.01]
  Expert: layer_15[4.3-8.3% std=1.23]
  Expert: layer_16[3.7-11.7% std=1.79]
  Expert: layer_18[4.2-7.3% std=0.87]
  Expert: layer_19[3.6-11.2% std=1.54]
  Expert: layer_20[4.0-9.2% std=1.32]
  Expert: layer_21[3.4-9.6% std=1.43]
Step     298 | Loss: 4.0210 | LR: 1.86e-04 | GradNorm: 0.60 | Tok/s: 1821 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.0-7.9% std=0.69] | layer_5[5.8-6.7% std=0.28] | layer_11[4.1-9.0% std=1.25] | layer_17[4.8-8.2% std=0.85] | layer_22[1.4-8.0% std=1.55]
  Expert: layer_1[5.8-7.4% std=0.34]
  Expert: layer_2[5.9-6.5% std=0.15]
  Expert: layer_3[4.3-13.8% std=2.04]
  Expert: layer_4[5.0-6.8% std=0.45]
  Expert: layer_6[3.8-9.8% std=1.17]
  Expert: layer_7[2.2-16.8% std=2.91]
  Expert: layer_8[2.7-12.9% std=2.17]
  Expert: layer_9[4.4-8.2% std=0.91]
  Expert: layer_10[3.2-12.8% std=1.96]
  Expert: layer_12[3.3-12.9% std=2.20]
  Expert: layer_13[3.9-7.7% std=0.96]
  Expert: layer_14[3.7-12.3% std=1.91]
  Expert: layer_15[3.9-9.7% std=1.27]
  Expert: layer_16[3.5-12.0% std=1.89]
  Expert: layer_18[5.0-7.9% std=1.06]
  Expert: layer_19[4.0-8.1% std=1.12]
  Expert: layer_20[4.8-8.0% std=0.94]
  Expert: layer_21[3.4-9.5% std=1.50]
Step     299 | Loss: 3.9988 | LR: 1.87e-04 | GradNorm: 0.52 | Tok/s: 2537 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.0-7.9% std=0.68] | layer_5[5.7-7.0% std=0.36] | layer_11[3.9-9.7% std=1.44] | layer_17[4.3-7.4% std=0.92] | layer_22[1.0-8.0% std=1.61]
  Expert: layer_1[5.8-7.4% std=0.32]
  Expert: layer_2[6.0-6.5% std=0.15]
  Expert: layer_3[4.3-14.0% std=2.11]
  Expert: layer_4[5.0-7.1% std=0.52]
  Expert: layer_6[3.8-10.0% std=1.17]
  Expert: layer_7[1.9-16.8% std=2.94]
  Expert: layer_8[2.8-12.7% std=2.12]
  Expert: layer_9[4.3-9.1% std=1.02]
  Expert: layer_10[3.2-12.6% std=1.92]
  Expert: layer_12[3.1-12.3% std=2.08]
  Expert: layer_13[4.6-8.6% std=0.87]
  Expert: layer_14[4.2-10.3% std=1.32]
  Expert: layer_15[4.2-9.0% std=1.29]
  Expert: layer_16[3.6-10.7% std=1.61]
  Expert: layer_18[4.8-8.2% std=0.87]
  Expert: layer_19[3.9-8.5% std=1.23]
  Expert: layer_20[4.8-8.1% std=0.89]
  Expert: layer_21[4.0-8.5% std=1.18]
Step     300 | Loss: 4.0280 | LR: 1.88e-04 | GradNorm: 0.51 | Tok/s: 2521 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.9% std=0.66] | layer_5[5.5-7.0% std=0.32] | layer_11[4.3-9.1% std=1.37] | layer_17[4.4-8.0% std=0.99] | layer_22[1.2-9.1% std=1.64]
  Expert: layer_1[5.9-7.4% std=0.33]
  Expert: layer_2[5.9-6.6% std=0.15]
  Expert: layer_3[4.3-13.9% std=2.08]
  Expert: layer_4[5.0-7.4% std=0.53]
  Expert: layer_6[3.7-10.0% std=1.19]
  Expert: layer_7[1.9-16.8% std=2.96]
  Expert: layer_8[2.7-12.8% std=2.12]
  Expert: layer_9[4.5-8.6% std=0.81]
  Expert: layer_10[2.9-12.6% std=1.95]
  Expert: layer_12[3.1-12.8% std=2.10]
  Expert: layer_13[4.7-8.5% std=0.95]
  Expert: layer_14[4.3-10.5% std=1.38]
  Expert: layer_15[4.3-8.4% std=1.14]
  Expert: layer_16[3.9-11.3% std=1.66]
  Expert: layer_18[5.1-7.3% std=0.62]
  Expert: layer_19[4.1-8.2% std=1.10]
  Expert: layer_20[4.3-8.0% std=0.90]
  Expert: layer_21[4.0-8.7% std=1.19]
Step     301 | Loss: 4.0716 | LR: 1.88e-04 | GradNorm: 0.57 | Tok/s: 2542 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.0-7.9% std=0.67] | layer_5[5.5-7.1% std=0.34] | layer_11[4.5-8.9% std=1.12] | layer_17[4.4-8.3% std=0.97] | layer_22[1.2-9.2% std=1.66]
  Expert: layer_1[5.9-7.4% std=0.34]
  Expert: layer_2[6.0-6.5% std=0.16]
  Expert: layer_3[4.4-13.8% std=2.06]
  Expert: layer_4[5.1-7.4% std=0.51]
  Expert: layer_6[3.9-10.1% std=1.19]
  Expert: layer_7[1.9-16.8% std=2.94]
  Expert: layer_8[2.7-12.5% std=2.04]
  Expert: layer_9[4.5-8.6% std=0.80]
  Expert: layer_10[2.8-12.6% std=1.97]
  Expert: layer_12[3.2-11.9% std=1.93]
  Expert: layer_13[4.5-7.6% std=0.85]
  Expert: layer_14[4.1-12.0% std=1.71]
  Expert: layer_15[4.7-8.8% std=1.24]
  Expert: layer_16[3.8-11.0% std=1.60]
  Expert: layer_18[4.2-7.4% std=0.87]
  Expert: layer_19[3.8-7.8% std=1.11]
  Expert: layer_20[4.1-7.9% std=0.99]
  Expert: layer_21[3.7-8.7% std=1.39]
Step     302 | Loss: 4.0445 | LR: 1.89e-04 | GradNorm: 0.56 | Tok/s: 426 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.66] | layer_5[5.5-7.2% std=0.34] | layer_11[4.0-9.1% std=1.33] | layer_17[5.1-7.8% std=0.77] | layer_22[1.4-8.5% std=1.54]
  Expert: layer_1[5.9-7.4% std=0.33]
  Expert: layer_2[6.0-6.7% std=0.20]
  Expert: layer_3[4.5-13.8% std=2.04]
  Expert: layer_4[5.2-7.3% std=0.51]
  Expert: layer_6[3.8-9.9% std=1.15]
  Expert: layer_7[1.9-16.3% std=2.82]
  Expert: layer_8[2.8-12.2% std=1.97]
  Expert: layer_9[4.6-8.1% std=0.75]
  Expert: layer_10[2.9-12.5% std=1.97]
  Expert: layer_12[3.5-12.0% std=2.01]
  Expert: layer_13[4.0-7.8% std=0.85]
  Expert: layer_14[4.4-11.2% std=1.50]
  Expert: layer_15[4.2-9.1% std=1.27]
  Expert: layer_16[3.2-12.8% std=2.06]
  Expert: layer_18[4.8-7.5% std=0.77]
  Expert: layer_19[5.0-7.6% std=0.75]
  Expert: layer_20[3.5-8.2% std=1.10]
  Expert: layer_21[4.4-8.0% std=0.93]
Step     303 | Loss: 3.9798 | LR: 1.89e-04 | GradNorm: 0.52 | Tok/s: 2556 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.64] | layer_5[5.8-7.4% std=0.36] | layer_11[4.1-8.6% std=1.10] | layer_17[3.6-7.8% std=1.00] | layer_22[1.2-8.3% std=1.61]
  Expert: layer_1[5.9-7.4% std=0.33]
  Expert: layer_2[6.0-6.5% std=0.19]
  Expert: layer_3[4.7-13.7% std=2.01]
  Expert: layer_4[5.2-7.3% std=0.47]
  Expert: layer_6[4.2-10.2% std=1.16]
  Expert: layer_7[2.1-16.3% std=2.80]
  Expert: layer_8[2.8-12.1% std=1.91]
  Expert: layer_9[4.7-8.9% std=0.92]
  Expert: layer_10[3.0-12.7% std=2.00]
  Expert: layer_12[3.2-12.0% std=2.07]
  Expert: layer_13[4.2-7.8% std=0.84]
  Expert: layer_14[4.0-10.4% std=1.34]
  Expert: layer_15[4.4-8.8% std=1.20]
  Expert: layer_16[4.3-10.6% std=1.39]
  Expert: layer_18[4.9-7.5% std=0.86]
  Expert: layer_19[4.7-8.8% std=1.12]
  Expert: layer_20[4.4-9.0% std=1.14]
  Expert: layer_21[4.4-8.4% std=0.98]
Step     304 | Loss: 4.0276 | LR: 1.90e-04 | GradNorm: 0.43 | Tok/s: 1322 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.63] | layer_5[5.8-7.2% std=0.34] | layer_11[4.4-8.4% std=1.06] | layer_17[4.5-7.7% std=0.89] | layer_22[1.5-7.6% std=1.38]
  Expert: layer_1[5.8-7.4% std=0.34]
  Expert: layer_2[5.9-6.5% std=0.19]
  Expert: layer_3[4.8-13.5% std=1.96]
  Expert: layer_4[5.4-7.3% std=0.44]
  Expert: layer_6[4.0-10.0% std=1.15]
  Expert: layer_7[2.1-15.8% std=2.72]
  Expert: layer_8[2.8-12.6% std=2.06]
  Expert: layer_9[4.6-8.6% std=0.89]
  Expert: layer_10[2.9-12.7% std=1.99]
  Expert: layer_12[3.7-12.1% std=1.94]
  Expert: layer_13[4.2-7.3% std=0.79]
  Expert: layer_14[3.9-10.7% std=1.44]
  Expert: layer_15[4.3-8.5% std=1.00]
  Expert: layer_16[4.6-10.9% std=1.46]
  Expert: layer_18[5.0-7.1% std=0.72]
  Expert: layer_19[4.8-7.5% std=0.81]
  Expert: layer_20[4.7-7.9% std=0.85]
  Expert: layer_21[4.4-8.5% std=0.95]
Step     305 | Loss: 3.9821 | LR: 1.91e-04 | GradNorm: 0.46 | Tok/s: 1017 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.64] | layer_5[5.7-7.3% std=0.39] | layer_11[4.0-9.2% std=1.24] | layer_17[4.9-7.9% std=0.82] | layer_22[1.5-8.0% std=1.48]
  Expert: layer_1[5.8-7.3% std=0.31]
  Expert: layer_2[6.0-6.6% std=0.17]
  Expert: layer_3[4.9-12.7% std=1.75]
  Expert: layer_4[5.5-7.3% std=0.43]
  Expert: layer_6[4.0-10.0% std=1.17]
  Expert: layer_7[2.2-15.5% std=2.66]
  Expert: layer_8[2.9-12.9% std=2.09]
  Expert: layer_9[4.7-8.3% std=0.82]
  Expert: layer_10[3.0-12.2% std=1.84]
  Expert: layer_12[3.4-11.9% std=1.90]
  Expert: layer_13[4.3-7.3% std=0.82]
  Expert: layer_14[4.1-10.5% std=1.31]
  Expert: layer_15[4.2-9.8% std=1.27]
  Expert: layer_16[3.8-11.7% std=1.66]
  Expert: layer_18[4.6-8.1% std=0.89]
  Expert: layer_19[4.8-7.7% std=0.92]
  Expert: layer_20[3.5-8.0% std=1.04]
  Expert: layer_21[3.1-8.6% std=1.24]
Step     306 | Loss: 4.0089 | LR: 1.91e-04 | GradNorm: 0.44 | Tok/s: 854 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.64] | layer_5[5.6-7.2% std=0.42] | layer_11[4.3-9.5% std=1.21] | layer_17[4.2-8.0% std=0.93] | layer_22[1.9-8.0% std=1.38]
  Expert: layer_1[5.8-7.3% std=0.31]
  Expert: layer_2[5.9-6.5% std=0.19]
  Expert: layer_3[4.9-12.3% std=1.65]
  Expert: layer_4[5.4-7.2% std=0.40]
  Expert: layer_6[4.2-9.9% std=1.12]
  Expert: layer_7[2.2-16.4% std=2.84]
  Expert: layer_8[2.6-12.5% std=2.02]
  Expert: layer_9[4.8-8.3% std=0.84]
  Expert: layer_10[2.8-12.7% std=1.97]
  Expert: layer_12[3.6-12.5% std=2.02]
  Expert: layer_13[4.2-7.2% std=0.84]
  Expert: layer_14[4.4-11.2% std=1.57]
  Expert: layer_15[3.9-8.9% std=1.29]
  Expert: layer_16[4.2-11.1% std=1.51]
  Expert: layer_18[4.7-7.8% std=0.81]
  Expert: layer_19[4.3-7.9% std=1.03]
  Expert: layer_20[3.7-8.7% std=1.17]
  Expert: layer_21[3.0-8.9% std=1.30]
Step     307 | Loss: 4.0040 | LR: 1.92e-04 | GradNorm: 0.44 | Tok/s: 879 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.63] | layer_5[5.8-7.1% std=0.37] | layer_11[4.4-9.0% std=1.18] | layer_17[4.5-8.3% std=0.93] | layer_22[1.8-8.6% std=1.44]
  Expert: layer_1[5.8-7.3% std=0.30]
  Expert: layer_2[5.9-6.6% std=0.17]
  Expert: layer_3[4.8-12.5% std=1.70]
  Expert: layer_4[5.5-7.1% std=0.37]
  Expert: layer_6[4.0-10.2% std=1.19]
  Expert: layer_7[2.0-16.7% std=2.97]
  Expert: layer_8[2.8-13.2% std=2.13]
  Expert: layer_9[4.8-8.4% std=0.83]
  Expert: layer_10[3.0-12.9% std=2.00]
  Expert: layer_12[3.2-13.3% std=2.26]
  Expert: layer_13[4.1-7.5% std=0.82]
  Expert: layer_14[4.6-10.0% std=1.27]
  Expert: layer_15[3.4-8.9% std=1.25]
  Expert: layer_16[4.2-10.8% std=1.37]
  Expert: layer_18[5.0-7.2% std=0.59]
  Expert: layer_19[4.8-8.4% std=0.98]
  Expert: layer_20[3.6-7.9% std=1.10]
  Expert: layer_21[3.9-8.0% std=1.10]
Step     308 | Loss: 3.9577 | LR: 1.92e-04 | GradNorm: 0.52 | Tok/s: 1015 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.64] | layer_5[5.7-7.3% std=0.42] | layer_11[4.3-10.0% std=1.28] | layer_17[4.2-7.3% std=0.87] | layer_22[1.8-7.9% std=1.34]
  Expert: layer_1[5.9-7.3% std=0.31]
  Expert: layer_2[5.9-6.6% std=0.19]
  Expert: layer_3[4.7-12.6% std=1.73]
  Expert: layer_4[5.5-7.2% std=0.39]
  Expert: layer_6[4.0-10.2% std=1.20]
  Expert: layer_7[2.0-16.8% std=2.99]
  Expert: layer_8[2.8-12.5% std=1.99]
  Expert: layer_9[4.7-8.8% std=1.02]
  Expert: layer_10[3.1-13.1% std=2.05]
  Expert: layer_12[3.2-13.0% std=2.19]
  Expert: layer_13[3.9-7.8% std=0.90]
  Expert: layer_14[4.7-11.1% std=1.47]
  Expert: layer_15[4.1-9.2% std=1.42]
  Expert: layer_16[4.5-11.2% std=1.47]
  Expert: layer_18[5.0-8.5% std=0.97]
  Expert: layer_19[4.6-8.5% std=0.98]
  Expert: layer_20[4.6-7.8% std=0.98]
  Expert: layer_21[4.5-9.0% std=1.23]
Step     309 | Loss: 3.9709 | LR: 1.93e-04 | GradNorm: 0.56 | Tok/s: 2520 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.65] | layer_5[5.8-7.2% std=0.35] | layer_11[4.7-9.1% std=1.13] | layer_17[4.9-8.4% std=0.95] | layer_22[1.6-8.8% std=1.51]
  Expert: layer_1[5.9-7.3% std=0.29]
  Expert: layer_2[6.0-6.6% std=0.16]
  Expert: layer_3[4.6-12.6% std=1.75]
  Expert: layer_4[5.5-7.3% std=0.45]
  Expert: layer_6[3.9-10.1% std=1.21]
  Expert: layer_7[2.1-16.8% std=3.01]
  Expert: layer_8[2.7-12.8% std=2.00]
  Expert: layer_9[5.1-8.0% std=0.80]
  Expert: layer_10[3.4-12.8% std=1.95]
  Expert: layer_12[2.9-14.1% std=2.32]
  Expert: layer_13[4.3-7.9% std=0.91]
  Expert: layer_14[4.5-9.4% std=1.21]
  Expert: layer_15[3.4-8.0% std=1.20]
  Expert: layer_16[4.6-10.1% std=1.25]
  Expert: layer_18[4.9-7.4% std=0.70]
  Expert: layer_19[4.5-8.2% std=1.01]
  Expert: layer_20[3.6-8.0% std=1.10]
  Expert: layer_21[3.3-8.6% std=1.22]
Step     310 | Loss: 3.9758 | LR: 1.94e-04 | GradNorm: 0.56 | Tok/s: 2452 | Mem: 70.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.65] | layer_5[5.6-7.3% std=0.44] | layer_11[4.5-8.6% std=1.09] | layer_17[4.8-8.5% std=0.98] | layer_22[1.7-8.2% std=1.33]
  Expert: layer_1[6.0-7.2% std=0.26]
  Expert: layer_2[6.1-6.6% std=0.15]
  Expert: layer_3[4.6-12.7% std=1.76]
  Expert: layer_4[5.7-7.3% std=0.44]
  Expert: layer_6[3.7-10.4% std=1.28]
  Expert: layer_7[2.2-16.7% std=2.96]
  Expert: layer_8[2.8-12.2% std=1.92]
  Expert: layer_9[4.9-8.6% std=1.01]
  Expert: layer_10[3.4-12.4% std=1.90]
  Expert: layer_12[3.5-13.1% std=2.11]
  Expert: layer_13[4.2-7.1% std=0.80]
  Expert: layer_14[4.6-11.4% std=1.53]
  Expert: layer_15[3.5-8.6% std=1.23]
  Expert: layer_16[4.6-10.3% std=1.27]
  Expert: layer_18[5.0-8.3% std=0.81]
  Expert: layer_19[4.6-8.3% std=1.03]
  Expert: layer_20[4.4-7.8% std=0.82]
  Expert: layer_21[4.0-9.1% std=1.20]

Per-layer gamma: 0.0001 ~ 0.0005 (sqrt, 23 MoE layers)

============================================================
학습 시작 (step=310, epoch=0)
============================================================

=== Epoch 1/1 ===
Step     311 | Loss: 3.9025 | LR: 1.94e-04 | GradNorm: 0.60 | Tok/s: 2634 | Mem: 65.4GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.65] | layer_5[5.5-7.1% std=0.39] | layer_11[4.2-8.7% std=1.17] | layer_17[5.2-8.1% std=0.79] | layer_22[1.3-8.2% std=1.66]
  Expert: layer_1[5.9-7.4% std=0.31]
  Expert: layer_2[5.8-6.6% std=0.19]
  Expert: layer_3[4.8-13.1% std=1.85]
  Expert: layer_4[5.4-7.3% std=0.44]
  Expert: layer_6[3.7-10.4% std=1.27]
  Expert: layer_7[2.1-16.5% std=2.96]
  Expert: layer_8[2.7-13.2% std=2.10]
  Expert: layer_9[5.0-8.6% std=0.92]
  Expert: layer_10[3.4-12.3% std=1.90]
  Expert: layer_12[3.2-14.5% std=2.41]
  Expert: layer_13[4.1-7.5% std=0.89]
  Expert: layer_14[4.5-9.5% std=1.25]
  Expert: layer_15[3.1-8.9% std=1.38]
  Expert: layer_16[4.4-10.1% std=1.25]
  Expert: layer_18[4.9-8.7% std=1.03]
  Expert: layer_19[4.5-8.2% std=1.01]
  Expert: layer_20[4.0-7.7% std=0.95]
  Expert: layer_21[2.7-9.1% std=1.42]
Step     312 | Loss: 4.0203 | LR: 1.95e-04 | GradNorm: 0.71 | Tok/s: 278 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.63] | layer_5[5.8-6.9% std=0.29] | layer_11[4.5-8.4% std=1.10] | layer_17[4.3-8.2% std=1.08] | layer_22[1.7-7.9% std=1.44]
  Expert: layer_1[6.0-7.2% std=0.25]
  Expert: layer_2[6.0-6.6% std=0.16]
  Expert: layer_3[4.5-13.4% std=1.93]
  Expert: layer_4[5.5-7.3% std=0.41]
  Expert: layer_6[3.6-10.6% std=1.35]
  Expert: layer_7[2.3-16.2% std=2.91]
  Expert: layer_8[2.7-12.7% std=2.02]
  Expert: layer_9[4.8-8.3% std=0.88]
  Expert: layer_10[3.5-12.9% std=2.03]
  Expert: layer_12[3.6-13.3% std=2.15]
  Expert: layer_13[4.7-7.6% std=0.84]
  Expert: layer_14[4.2-10.7% std=1.55]
  Expert: layer_15[3.3-7.9% std=1.12]
  Expert: layer_16[4.8-10.2% std=1.30]
  Expert: layer_18[4.8-8.1% std=0.81]
  Expert: layer_19[3.8-7.5% std=1.03]
  Expert: layer_20[4.2-8.1% std=1.08]
  Expert: layer_21[4.1-7.7% std=1.01]
Step     313 | Loss: 3.9523 | LR: 1.96e-04 | GradNorm: 0.63 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.7% std=0.59] | layer_5[5.6-7.1% std=0.35] | layer_11[4.5-9.4% std=1.11] | layer_17[5.0-9.0% std=0.91] | layer_22[1.6-8.9% std=1.62]
  Expert: layer_1[5.9-7.2% std=0.27]
  Expert: layer_2[5.9-6.7% std=0.20]
  Expert: layer_3[4.8-13.3% std=1.90]
  Expert: layer_4[5.6-7.8% std=0.52]
  Expert: layer_6[3.9-10.9% std=1.41]
  Expert: layer_7[2.3-16.0% std=2.79]
  Expert: layer_8[2.8-12.1% std=1.88]
  Expert: layer_9[4.9-8.9% std=1.00]
  Expert: layer_10[3.7-12.3% std=1.84]
  Expert: layer_12[3.0-13.4% std=2.21]
  Expert: layer_13[4.3-7.5% std=0.80]
  Expert: layer_14[4.3-10.6% std=1.43]
  Expert: layer_15[3.4-9.5% std=1.48]
  Expert: layer_16[4.4-9.5% std=1.21]
  Expert: layer_18[4.8-9.1% std=1.19]
  Expert: layer_19[4.2-9.4% std=1.19]
  Expert: layer_20[4.5-8.3% std=1.00]
  Expert: layer_21[3.8-9.2% std=1.39]
Step     314 | Loss: 3.9363 | LR: 1.96e-04 | GradNorm: 0.55 | Tok/s: 2678 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.64] | layer_5[5.8-6.9% std=0.35] | layer_11[4.4-7.8% std=0.98] | layer_17[4.8-7.3% std=0.63] | layer_22[1.7-8.0% std=1.35]
  Expert: layer_1[6.0-7.2% std=0.25]
  Expert: layer_2[6.1-6.7% std=0.18]
  Expert: layer_3[4.5-13.1% std=1.88]
  Expert: layer_4[5.4-7.4% std=0.48]
  Expert: layer_6[3.7-10.5% std=1.32]
  Expert: layer_7[2.4-15.2% std=2.57]
  Expert: layer_8[2.8-11.9% std=1.82]
  Expert: layer_9[4.9-8.1% std=0.79]
  Expert: layer_10[3.7-12.5% std=1.94]
  Expert: layer_12[3.5-12.6% std=1.98]
  Expert: layer_13[4.4-7.7% std=0.79]
  Expert: layer_14[4.6-10.0% std=1.30]
  Expert: layer_15[3.1-8.1% std=1.15]
  Expert: layer_16[5.0-8.6% std=0.88]
  Expert: layer_18[4.9-7.5% std=0.70]
  Expert: layer_19[4.2-8.0% std=1.05]
  Expert: layer_20[4.6-8.2% std=0.97]
  Expert: layer_21[3.8-8.1% std=0.96]
Step     315 | Loss: 3.9137 | LR: 1.97e-04 | GradNorm: 0.47 | Tok/s: 2678 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.62] | layer_5[5.8-6.8% std=0.29] | layer_11[4.1-8.0% std=1.00] | layer_17[5.0-7.8% std=0.81] | layer_22[1.8-8.3% std=1.54]
  Expert: layer_1[5.9-7.2% std=0.27]
  Expert: layer_2[6.1-6.6% std=0.16]
  Expert: layer_3[4.5-13.4% std=1.97]
  Expert: layer_4[5.5-7.5% std=0.48]
  Expert: layer_6[3.8-10.3% std=1.29]
  Expert: layer_7[2.4-14.8% std=2.49]
  Expert: layer_8[2.8-11.5% std=1.79]
  Expert: layer_9[4.7-7.9% std=0.80]
  Expert: layer_10[3.6-12.4% std=1.87]
  Expert: layer_12[3.9-11.7% std=1.79]
  Expert: layer_13[4.6-7.0% std=0.70]
  Expert: layer_14[4.6-9.9% std=1.34]
  Expert: layer_15[3.6-8.0% std=1.01]
  Expert: layer_16[5.2-9.2% std=0.99]
  Expert: layer_18[5.0-7.5% std=0.91]
  Expert: layer_19[4.1-8.9% std=1.14]
  Expert: layer_20[4.6-8.2% std=0.94]
  Expert: layer_21[3.9-8.3% std=1.10]
Step     316 | Loss: 3.9142 | LR: 1.98e-04 | GradNorm: 0.54 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.67] | layer_5[5.7-7.3% std=0.41] | layer_11[4.5-8.6% std=0.87] | layer_17[4.6-7.4% std=0.66] | layer_22[1.8-7.5% std=1.32]
  Expert: layer_1[6.0-7.3% std=0.28]
  Expert: layer_2[6.0-6.6% std=0.17]
  Expert: layer_3[4.3-13.4% std=1.99]
  Expert: layer_4[5.6-7.3% std=0.44]
  Expert: layer_6[3.7-10.0% std=1.26]
  Expert: layer_7[2.5-15.0% std=2.48]
  Expert: layer_8[2.5-11.3% std=1.77]
  Expert: layer_9[4.6-7.6% std=0.68]
  Expert: layer_10[3.2-12.8% std=1.97]
  Expert: layer_12[3.6-10.9% std=1.57]
  Expert: layer_13[4.4-7.5% std=0.82]
  Expert: layer_14[4.2-11.0% std=1.51]
  Expert: layer_15[3.6-8.1% std=1.07]
  Expert: layer_16[4.4-8.7% std=1.13]
  Expert: layer_18[5.2-7.9% std=0.81]
  Expert: layer_19[3.9-7.9% std=1.07]
  Expert: layer_20[4.9-8.5% std=0.87]
  Expert: layer_21[3.4-9.3% std=1.18]
Step     317 | Loss: 3.9441 | LR: 1.98e-04 | GradNorm: 0.56 | Tok/s: 2642 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.62] | layer_5[5.7-6.9% std=0.32] | layer_11[4.4-8.6% std=1.11] | layer_17[4.6-7.8% std=0.94] | layer_22[1.4-8.9% std=1.63]
  Expert: layer_1[6.0-7.1% std=0.24]
  Expert: layer_2[6.0-6.5% std=0.17]
  Expert: layer_3[4.4-13.4% std=1.95]
  Expert: layer_4[5.6-7.5% std=0.44]
  Expert: layer_6[3.6-10.2% std=1.28]
  Expert: layer_7[2.6-15.1% std=2.55]
  Expert: layer_8[2.7-11.6% std=1.90]
  Expert: layer_9[4.7-7.9% std=0.77]
  Expert: layer_10[3.2-12.5% std=1.91]
  Expert: layer_12[3.6-11.6% std=1.86]
  Expert: layer_13[4.1-7.6% std=0.84]
  Expert: layer_14[4.8-8.9% std=1.12]
  Expert: layer_15[3.2-8.5% std=1.24]
  Expert: layer_16[5.0-8.9% std=1.00]
  Expert: layer_18[4.9-8.3% std=0.98]
  Expert: layer_19[4.3-8.1% std=1.06]
  Expert: layer_20[4.0-8.6% std=1.11]
  Expert: layer_21[3.9-8.1% std=1.02]
Step     318 | Loss: 3.8789 | LR: 1.99e-04 | GradNorm: 0.55 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.62] | layer_5[5.7-7.1% std=0.40] | layer_11[5.3-7.2% std=0.62] | layer_17[4.8-7.6% std=0.78] | layer_22[1.9-8.1% std=1.37]
  Expert: layer_1[6.0-7.1% std=0.26]
  Expert: layer_2[5.9-6.5% std=0.16]
  Expert: layer_3[4.3-12.9% std=1.82]
  Expert: layer_4[5.7-7.4% std=0.45]
  Expert: layer_6[3.7-9.8% std=1.20]
  Expert: layer_7[2.5-14.8% std=2.50]
  Expert: layer_8[2.8-11.5% std=1.82]
  Expert: layer_9[5.1-7.5% std=0.59]
  Expert: layer_10[3.5-12.1% std=1.81]
  Expert: layer_12[3.4-11.0% std=1.65]
  Expert: layer_13[4.4-7.4% std=0.86]
  Expert: layer_14[4.4-9.7% std=1.41]
  Expert: layer_15[3.0-8.2% std=1.20]
  Expert: layer_16[4.7-8.5% std=1.06]
  Expert: layer_18[5.1-7.2% std=0.72]
  Expert: layer_19[3.9-8.0% std=1.18]
  Expert: layer_20[4.9-8.9% std=1.04]
  Expert: layer_21[4.2-8.3% std=1.26]
Step     319 | Loss: 3.9077 | LR: 1.99e-04 | GradNorm: 0.61 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.62] | layer_5[5.7-7.4% std=0.44] | layer_11[4.8-8.9% std=1.08] | layer_17[5.2-8.0% std=0.85] | layer_22[2.0-8.2% std=1.33]
  Expert: layer_1[6.0-7.2% std=0.27]
  Expert: layer_2[5.9-6.6% std=0.17]
  Expert: layer_3[4.8-13.0% std=1.83]
  Expert: layer_4[5.6-7.6% std=0.50]
  Expert: layer_6[4.0-9.5% std=1.13]
  Expert: layer_7[2.5-14.8% std=2.52]
  Expert: layer_8[2.6-11.6% std=1.92]
  Expert: layer_9[5.3-7.7% std=0.74]
  Expert: layer_10[3.4-10.9% std=1.56]
  Expert: layer_12[3.5-11.4% std=1.68]
  Expert: layer_13[4.3-7.3% std=0.79]
  Expert: layer_14[4.3-9.7% std=1.29]
  Expert: layer_15[3.4-9.8% std=1.41]
  Expert: layer_16[4.5-10.3% std=1.43]
  Expert: layer_18[5.1-7.9% std=0.84]
  Expert: layer_19[4.2-7.5% std=0.99]
  Expert: layer_20[5.3-7.7% std=0.68]
  Expert: layer_21[3.5-9.7% std=1.47]
Step     320 | Loss: 3.8835 | LR: 2.00e-04 | GradNorm: 0.70 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.65] | layer_5[5.7-7.4% std=0.45] | layer_11[4.8-7.7% std=0.80] | layer_17[4.5-8.8% std=1.08] | layer_22[1.7-8.9% std=1.64]
  Expert: layer_1[6.0-7.1% std=0.27]
  Expert: layer_2[6.0-6.5% std=0.15]
  Expert: layer_3[4.7-12.5% std=1.71]
  Expert: layer_4[5.7-7.4% std=0.48]
  Expert: layer_6[4.0-9.3% std=1.06]
  Expert: layer_7[2.4-15.0% std=2.57]
  Expert: layer_8[2.9-12.0% std=1.91]
  Expert: layer_9[5.5-7.5% std=0.60]
  Expert: layer_10[3.6-11.5% std=1.67]
  Expert: layer_12[3.3-11.7% std=1.83]
  Expert: layer_13[4.9-8.0% std=0.79]
  Expert: layer_14[4.5-9.3% std=1.24]
  Expert: layer_15[2.9-9.3% std=1.44]
  Expert: layer_16[4.7-9.9% std=1.26]
  Expert: layer_18[4.6-8.3% std=0.86]
  Expert: layer_19[3.8-10.1% std=1.45]
  Expert: layer_20[4.4-7.7% std=0.83]
  Expert: layer_21[4.9-9.2% std=0.94]
Step     321 | Loss: 3.9050 | LR: 2.01e-04 | GradNorm: 0.75 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.61] | layer_5[5.4-7.1% std=0.42] | layer_11[5.1-7.8% std=0.78] | layer_17[3.6-8.8% std=1.25] | layer_22[1.3-8.7% std=1.81]
  Expert: layer_1[6.0-7.1% std=0.25]
  Expert: layer_2[5.9-6.6% std=0.16]
  Expert: layer_3[5.0-13.3% std=1.91]
  Expert: layer_4[5.7-7.7% std=0.53]
  Expert: layer_6[4.2-9.6% std=1.09]
  Expert: layer_7[2.6-15.0% std=2.53]
  Expert: layer_8[2.7-12.4% std=2.05]
  Expert: layer_9[5.3-7.9% std=0.77]
  Expert: layer_10[3.8-11.1% std=1.61]
  Expert: layer_12[3.8-11.4% std=1.62]
  Expert: layer_13[4.7-7.3% std=0.77]
  Expert: layer_14[4.2-10.3% std=1.50]
  Expert: layer_15[2.9-7.8% std=1.15]
  Expert: layer_16[4.7-8.7% std=1.20]
  Expert: layer_18[4.5-7.7% std=0.79]
  Expert: layer_19[4.2-8.9% std=1.41]
  Expert: layer_20[4.3-7.5% std=0.91]
  Expert: layer_21[4.0-8.4% std=1.17]
Step     322 | Loss: 3.9810 | LR: 2.01e-04 | GradNorm: 0.62 | Tok/s: 2665 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-8.0% std=0.64] | layer_5[5.5-7.5% std=0.56] | layer_11[4.6-8.6% std=1.06] | layer_17[5.3-7.4% std=0.61] | layer_22[1.1-8.5% std=1.77]
  Expert: layer_1[5.9-7.2% std=0.27]
  Expert: layer_2[5.8-6.6% std=0.17]
  Expert: layer_3[4.7-13.5% std=1.94]
  Expert: layer_4[5.7-7.5% std=0.50]
  Expert: layer_6[4.3-9.6% std=1.06]
  Expert: layer_7[2.2-14.9% std=2.54]
  Expert: layer_8[3.0-12.1% std=1.90]
  Expert: layer_9[5.4-7.5% std=0.62]
  Expert: layer_10[3.6-10.5% std=1.43]
  Expert: layer_12[3.3-13.1% std=2.04]
  Expert: layer_13[4.6-8.2% std=0.85]
  Expert: layer_14[5.0-10.0% std=1.22]
  Expert: layer_15[3.1-8.8% std=1.41]
  Expert: layer_16[4.3-10.9% std=1.53]
  Expert: layer_18[4.3-8.4% std=1.04]
  Expert: layer_19[4.3-9.3% std=1.24]
  Expert: layer_20[4.2-8.7% std=1.19]
  Expert: layer_21[3.4-9.1% std=1.29]
Step     323 | Loss: 3.9270 | LR: 2.02e-04 | GradNorm: 0.55 | Tok/s: 2666 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.61] | layer_5[5.4-7.3% std=0.55] | layer_11[5.3-7.2% std=0.61] | layer_17[4.2-8.3% std=0.99] | layer_22[1.2-8.1% std=1.55]
  Expert: layer_1[6.0-7.1% std=0.26]
  Expert: layer_2[5.9-6.6% std=0.16]
  Expert: layer_3[4.5-13.2% std=1.87]
  Expert: layer_4[5.6-7.5% std=0.51]
  Expert: layer_6[4.6-9.6% std=1.03]
  Expert: layer_7[2.2-15.0% std=2.58]
  Expert: layer_8[3.0-11.9% std=1.78]
  Expert: layer_9[4.8-7.7% std=0.73]
  Expert: layer_10[3.6-11.4% std=1.59]
  Expert: layer_12[3.8-12.7% std=1.95]
  Expert: layer_13[5.1-7.4% std=0.61]
  Expert: layer_14[4.4-10.6% std=1.50]
  Expert: layer_15[3.4-8.3% std=1.26]
  Expert: layer_16[4.5-9.0% std=0.90]
  Expert: layer_18[5.0-7.2% std=0.54]
  Expert: layer_19[4.7-8.6% std=1.02]
  Expert: layer_20[4.9-8.2% std=0.96]
  Expert: layer_21[3.7-8.9% std=1.12]
Step     324 | Loss: 3.9089 | LR: 2.03e-04 | GradNorm: 0.53 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.60] | layer_5[5.4-7.5% std=0.54] | layer_11[4.8-7.5% std=0.84] | layer_17[3.7-8.0% std=1.27] | layer_22[1.4-8.5% std=1.61]
  Expert: layer_1[6.0-7.1% std=0.26]
  Expert: layer_2[5.9-6.5% std=0.15]
  Expert: layer_3[4.6-12.9% std=1.79]
  Expert: layer_4[5.5-7.4% std=0.59]
  Expert: layer_6[4.4-9.6% std=1.07]
  Expert: layer_7[2.1-14.1% std=2.47]
  Expert: layer_8[3.2-12.3% std=1.89]
  Expert: layer_9[5.1-8.2% std=0.85]
  Expert: layer_10[4.0-10.5% std=1.35]
  Expert: layer_12[3.6-12.1% std=1.84]
  Expert: layer_13[4.8-7.4% std=0.72]
  Expert: layer_14[4.3-9.8% std=1.21]
  Expert: layer_15[3.4-9.3% std=1.45]
  Expert: layer_16[4.6-9.2% std=1.00]
  Expert: layer_18[4.5-8.2% std=1.00]
  Expert: layer_19[4.7-7.8% std=1.13]
  Expert: layer_20[4.7-8.8% std=1.00]
  Expert: layer_21[4.0-7.5% std=0.95]
Step     325 | Loss: 3.9107 | LR: 2.03e-04 | GradNorm: 0.49 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.9% std=0.64] | layer_5[5.4-7.8% std=0.63] | layer_11[4.9-7.7% std=0.84] | layer_17[4.7-7.8% std=0.92] | layer_22[1.7-8.0% std=1.43]
  Expert: layer_1[6.0-7.1% std=0.25]
  Expert: layer_2[6.0-6.7% std=0.18]
  Expert: layer_3[4.5-12.5% std=1.71]
  Expert: layer_4[5.5-7.4% std=0.55]
  Expert: layer_6[4.4-9.5% std=1.04]
  Expert: layer_7[2.0-14.0% std=2.38]
  Expert: layer_8[3.2-11.9% std=1.79]
  Expert: layer_9[5.4-8.4% std=0.77]
  Expert: layer_10[4.2-10.1% std=1.26]
  Expert: layer_12[3.4-12.2% std=1.86]
  Expert: layer_13[5.0-7.2% std=0.72]
  Expert: layer_14[4.4-9.0% std=1.07]
  Expert: layer_15[3.0-8.6% std=1.37]
  Expert: layer_16[4.9-9.5% std=1.02]
  Expert: layer_18[5.0-8.7% std=0.81]
  Expert: layer_19[4.8-7.1% std=0.76]
  Expert: layer_20[4.9-7.3% std=0.67]
  Expert: layer_21[3.0-8.4% std=1.19]
Step     326 | Loss: 3.9297 | LR: 2.04e-04 | GradNorm: 0.53 | Tok/s: 2647 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.61] | layer_5[5.5-7.5% std=0.57] | layer_11[4.6-7.2% std=0.84] | layer_17[4.7-7.4% std=0.75] | layer_22[1.2-8.0% std=1.55]
  Expert: layer_1[6.0-7.1% std=0.24]
  Expert: layer_2[6.0-6.6% std=0.18]
  Expert: layer_3[4.3-12.3% std=1.66]
  Expert: layer_4[5.5-7.7% std=0.59]
  Expert: layer_6[4.3-9.4% std=1.05]
  Expert: layer_7[2.0-14.3% std=2.42]
  Expert: layer_8[3.2-12.1% std=1.85]
  Expert: layer_9[4.9-8.2% std=0.91]
  Expert: layer_10[3.5-9.8% std=1.30]
  Expert: layer_12[3.5-12.2% std=1.80]
  Expert: layer_13[4.6-7.3% std=0.65]
  Expert: layer_14[4.3-10.9% std=1.48]
  Expert: layer_15[3.3-9.0% std=1.39]
  Expert: layer_16[5.1-10.3% std=1.21]
  Expert: layer_18[4.3-7.4% std=0.72]
  Expert: layer_19[4.1-8.5% std=0.95]
  Expert: layer_20[4.6-7.8% std=0.76]
  Expert: layer_21[3.4-7.7% std=1.12]
Step     327 | Loss: 3.9619 | LR: 2.04e-04 | GradNorm: 0.56 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.60] | layer_5[5.6-7.2% std=0.49] | layer_11[4.5-7.9% std=0.90] | layer_17[4.5-8.0% std=0.99] | layer_22[1.2-9.3% std=1.83]
  Expert: layer_1[6.0-7.2% std=0.26]
  Expert: layer_2[6.0-6.5% std=0.14]
  Expert: layer_3[4.3-12.2% std=1.64]
  Expert: layer_4[5.6-7.8% std=0.55]
  Expert: layer_6[4.4-9.4% std=1.01]
  Expert: layer_7[2.1-14.0% std=2.36]
  Expert: layer_8[3.1-11.6% std=1.74]
  Expert: layer_9[4.9-8.2% std=0.93]
  Expert: layer_10[3.4-10.0% std=1.26]
  Expert: layer_12[3.6-12.1% std=1.86]
  Expert: layer_13[4.7-7.8% std=0.69]
  Expert: layer_14[4.6-9.9% std=1.26]
  Expert: layer_15[4.0-8.6% std=1.24]
  Expert: layer_16[5.2-9.7% std=1.00]
  Expert: layer_18[4.8-7.3% std=0.70]
  Expert: layer_19[4.5-8.4% std=1.03]
  Expert: layer_20[4.7-7.8% std=0.87]
  Expert: layer_21[3.6-8.6% std=1.05]
Step     328 | Loss: 3.8308 | LR: 2.05e-04 | GradNorm: 0.58 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.7% std=0.61] | layer_5[5.4-7.2% std=0.47] | layer_11[4.4-8.0% std=1.04] | layer_17[4.0-7.7% std=0.84] | layer_22[1.4-8.5% std=1.56]
  Expert: layer_1[6.0-7.2% std=0.26]
  Expert: layer_2[5.9-6.5% std=0.17]
  Expert: layer_3[4.3-12.1% std=1.61]
  Expert: layer_4[5.6-7.7% std=0.56]
  Expert: layer_6[4.2-9.3% std=1.02]
  Expert: layer_7[2.1-13.4% std=2.22]
  Expert: layer_8[3.4-11.5% std=1.73]
  Expert: layer_9[4.8-8.3% std=0.88]
  Expert: layer_10[3.5-10.0% std=1.29]
  Expert: layer_12[3.7-11.2% std=1.62]
  Expert: layer_13[4.7-8.0% std=0.90]
  Expert: layer_14[4.5-10.4% std=1.31]
  Expert: layer_15[3.7-8.7% std=1.37]
  Expert: layer_16[4.8-9.9% std=1.14]
  Expert: layer_18[4.6-7.6% std=0.77]
  Expert: layer_19[4.3-7.7% std=0.82]
  Expert: layer_20[5.1-8.6% std=0.95]
  Expert: layer_21[4.0-8.1% std=1.00]
Step     329 | Loss: 3.9066 | LR: 2.06e-04 | GradNorm: 0.61 | Tok/s: 2647 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.62] | layer_5[5.5-7.0% std=0.44] | layer_11[5.1-7.3% std=0.68] | layer_17[3.5-7.9% std=1.02] | layer_22[1.8-8.8% std=1.55]
  Expert: layer_1[6.0-7.1% std=0.24]
  Expert: layer_2[6.0-6.5% std=0.16]
  Expert: layer_3[4.4-11.7% std=1.52]
  Expert: layer_4[5.5-7.6% std=0.55]
  Expert: layer_6[4.5-9.4% std=1.03]
  Expert: layer_7[2.1-13.4% std=2.16]
  Expert: layer_8[3.4-10.8% std=1.60]
  Expert: layer_9[5.3-8.4% std=0.78]
  Expert: layer_10[3.5-10.7% std=1.42]
  Expert: layer_12[4.2-10.7% std=1.58]
  Expert: layer_13[4.4-8.2% std=1.00]
  Expert: layer_14[4.7-9.5% std=1.39]
  Expert: layer_15[3.9-8.6% std=1.14]
  Expert: layer_16[5.0-9.1% std=0.91]
  Expert: layer_18[4.9-8.0% std=0.83]
  Expert: layer_19[4.7-7.8% std=0.82]
  Expert: layer_20[4.9-9.3% std=1.03]
  Expert: layer_21[4.0-8.1% std=1.00]
Step     330 | Loss: 3.9163 | LR: 2.06e-04 | GradNorm: 0.58 | Tok/s: 2658 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.62] | layer_5[5.6-6.9% std=0.42] | layer_11[4.5-7.5% std=0.84] | layer_17[4.9-7.9% std=0.93] | layer_22[1.8-8.6% std=1.42]
  Expert: layer_1[6.1-7.2% std=0.27]
  Expert: layer_2[6.1-6.5% std=0.11]
  Expert: layer_3[4.4-11.8% std=1.54]
  Expert: layer_4[5.5-7.5% std=0.58]
  Expert: layer_6[4.2-9.3% std=1.01]
  Expert: layer_7[2.2-12.7% std=2.05]
  Expert: layer_8[3.5-10.8% std=1.51]
  Expert: layer_9[5.4-7.7% std=0.55]
  Expert: layer_10[3.6-9.7% std=1.21]
  Expert: layer_12[3.9-10.6% std=1.42]
  Expert: layer_13[4.8-8.0% std=0.90]
  Expert: layer_14[4.4-9.4% std=1.24]
  Expert: layer_15[3.4-9.0% std=1.27]
  Expert: layer_16[4.5-10.1% std=1.28]
  Expert: layer_18[4.8-7.6% std=0.80]
  Expert: layer_19[4.2-7.9% std=0.96]
  Expert: layer_20[5.0-7.8% std=0.87]
  Expert: layer_21[4.4-8.0% std=0.94]
Step     331 | Loss: 3.9118 | LR: 2.07e-04 | GradNorm: 0.59 | Tok/s: 2642 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.61] | layer_5[5.5-7.2% std=0.46] | layer_11[4.3-8.0% std=0.91] | layer_17[4.3-8.8% std=1.15] | layer_22[1.6-8.8% std=1.48]
  Expert: layer_1[6.1-7.2% std=0.26]
  Expert: layer_2[6.1-6.5% std=0.13]
  Expert: layer_3[4.7-11.9% std=1.56]
  Expert: layer_4[5.6-7.6% std=0.59]
  Expert: layer_6[4.1-9.6% std=1.10]
  Expert: layer_7[2.1-13.1% std=2.10]
  Expert: layer_8[3.3-10.8% std=1.54]
  Expert: layer_9[5.4-7.8% std=0.56]
  Expert: layer_10[3.1-9.7% std=1.35]
  Expert: layer_12[3.6-10.4% std=1.45]
  Expert: layer_13[4.7-8.0% std=0.89]
  Expert: layer_14[4.1-11.0% std=1.55]
  Expert: layer_15[4.5-8.8% std=1.05]
  Expert: layer_16[5.2-9.5% std=0.99]
  Expert: layer_18[4.8-8.0% std=0.93]
  Expert: layer_19[3.7-8.3% std=1.00]
  Expert: layer_20[4.9-8.1% std=0.82]
  Expert: layer_21[3.6-8.0% std=1.15]
Step     332 | Loss: 3.8590 | LR: 2.08e-04 | GradNorm: 0.51 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.7% std=0.60] | layer_5[5.7-7.1% std=0.39] | layer_11[5.0-8.0% std=0.87] | layer_17[4.6-7.9% std=0.94] | layer_22[1.3-8.2% std=1.61]
  Expert: layer_1[6.0-7.1% std=0.27]
  Expert: layer_2[6.0-6.6% std=0.14]
  Expert: layer_3[4.9-12.5% std=1.68]
  Expert: layer_4[5.6-7.7% std=0.57]
  Expert: layer_6[4.3-9.7% std=1.07]
  Expert: layer_7[2.1-13.8% std=2.25]
  Expert: layer_8[3.0-11.2% std=1.76]
  Expert: layer_9[5.1-8.5% std=0.87]
  Expert: layer_10[3.5-10.0% std=1.33]
  Expert: layer_12[3.6-11.0% std=1.55]
  Expert: layer_13[4.4-7.7% std=0.91]
  Expert: layer_14[4.8-9.3% std=1.03]
  Expert: layer_15[4.4-8.9% std=1.10]
  Expert: layer_16[4.7-9.3% std=1.00]
  Expert: layer_18[4.9-7.8% std=0.79]
  Expert: layer_19[4.4-7.8% std=0.97]
  Expert: layer_20[5.3-7.3% std=0.49]
  Expert: layer_21[4.0-8.8% std=1.02]
Step     333 | Loss: 3.8651 | LR: 2.08e-04 | GradNorm: 0.41 | Tok/s: 2656 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.63] | layer_5[5.7-6.9% std=0.38] | layer_11[5.0-7.6% std=0.80] | layer_17[4.6-8.0% std=0.78] | layer_22[1.3-7.8% std=1.58]
  Expert: layer_1[6.0-7.2% std=0.29]
  Expert: layer_2[6.1-6.5% std=0.13]
  Expert: layer_3[4.9-12.5% std=1.67]
  Expert: layer_4[5.4-7.4% std=0.54]
  Expert: layer_6[4.3-9.6% std=1.06]
  Expert: layer_7[2.0-14.0% std=2.29]
  Expert: layer_8[3.1-10.8% std=1.55]
  Expert: layer_9[5.3-8.0% std=0.66]
  Expert: layer_10[3.8-10.9% std=1.52]
  Expert: layer_12[4.0-10.6% std=1.48]
  Expert: layer_13[4.2-8.0% std=0.99]
  Expert: layer_14[4.9-9.2% std=1.10]
  Expert: layer_15[4.1-7.7% std=1.00]
  Expert: layer_16[4.5-9.0% std=0.99]
  Expert: layer_18[5.2-7.2% std=0.56]
  Expert: layer_19[4.3-7.4% std=0.88]
  Expert: layer_20[5.4-7.7% std=0.67]
  Expert: layer_21[3.7-7.4% std=0.88]
Step     334 | Loss: 3.8515 | LR: 2.09e-04 | GradNorm: 0.45 | Tok/s: 2627 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.0-7.8% std=0.63] | layer_5[5.6-6.9% std=0.41] | layer_11[4.9-8.3% std=0.94] | layer_17[5.4-7.8% std=0.69] | layer_22[1.4-8.2% std=1.56]
  Expert: layer_1[5.9-7.2% std=0.28]
  Expert: layer_2[5.9-6.5% std=0.15]
  Expert: layer_3[5.0-12.4% std=1.68]
  Expert: layer_4[5.3-7.7% std=0.60]
  Expert: layer_6[4.5-9.5% std=1.06]
  Expert: layer_7[2.2-14.3% std=2.34]
  Expert: layer_8[3.3-10.7% std=1.63]
  Expert: layer_9[5.3-7.4% std=0.63]
  Expert: layer_10[4.0-10.5% std=1.42]
  Expert: layer_12[4.2-10.3% std=1.40]
  Expert: layer_13[3.8-7.8% std=0.98]
  Expert: layer_14[4.8-10.0% std=1.19]
  Expert: layer_15[3.8-8.5% std=1.30]
  Expert: layer_16[4.6-10.2% std=1.25]
  Expert: layer_18[4.9-8.0% std=0.88]
  Expert: layer_19[4.5-7.7% std=0.89]
  Expert: layer_20[5.0-7.7% std=0.73]
  Expert: layer_21[3.9-8.6% std=1.01]
Step     335 | Loss: 3.8030 | LR: 2.09e-04 | GradNorm: 0.45 | Tok/s: 2655 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.0-7.9% std=0.64] | layer_5[5.5-7.0% std=0.38] | layer_11[5.2-7.4% std=0.73] | layer_17[4.4-7.7% std=0.78] | layer_22[1.3-8.2% std=1.55]
  Expert: layer_1[6.0-7.2% std=0.29]
  Expert: layer_2[5.9-6.5% std=0.17]
  Expert: layer_3[5.0-12.5% std=1.68]
  Expert: layer_4[5.4-7.5% std=0.59]
  Expert: layer_6[4.4-9.3% std=1.03]
  Expert: layer_7[2.2-14.6% std=2.42]
  Expert: layer_8[3.3-11.2% std=1.64]
  Expert: layer_9[5.5-7.4% std=0.52]
  Expert: layer_10[4.1-10.4% std=1.37]
  Expert: layer_12[3.9-10.8% std=1.54]
  Expert: layer_13[4.4-7.6% std=0.93]
  Expert: layer_14[4.5-8.8% std=0.96]
  Expert: layer_15[3.7-8.0% std=1.05]
  Expert: layer_16[5.2-8.9% std=0.82]
  Expert: layer_18[4.8-8.1% std=0.73]
  Expert: layer_19[4.3-8.0% std=1.04]
  Expert: layer_20[4.5-8.7% std=0.99]
  Expert: layer_21[3.5-7.4% std=1.00]
Step     336 | Loss: 3.9003 | LR: 2.10e-04 | GradNorm: 0.52 | Tok/s: 2642 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.7% std=0.59] | layer_5[5.4-6.8% std=0.38] | layer_11[5.0-7.5% std=0.74] | layer_17[4.3-7.8% std=0.87] | layer_22[2.0-8.1% std=1.45]
  Expert: layer_1[5.9-7.1% std=0.28]
  Expert: layer_2[5.9-6.6% std=0.18]
  Expert: layer_3[4.9-12.6% std=1.72]
  Expert: layer_4[5.5-7.5% std=0.56]
  Expert: layer_6[4.5-9.3% std=0.99]
  Expert: layer_7[2.5-14.8% std=2.45]
  Expert: layer_8[3.1-10.6% std=1.65]
  Expert: layer_9[5.0-7.4% std=0.65]
  Expert: layer_10[4.3-10.5% std=1.46]
  Expert: layer_12[4.4-10.5% std=1.40]
  Expert: layer_13[4.2-7.4% std=1.06]
  Expert: layer_14[4.6-9.2% std=1.14]
  Expert: layer_15[3.5-8.7% std=1.33]
  Expert: layer_16[4.3-9.0% std=1.02]
  Expert: layer_18[4.7-7.6% std=0.82]
  Expert: layer_19[4.4-7.7% std=0.97]
  Expert: layer_20[4.3-8.2% std=1.03]
  Expert: layer_21[4.2-8.9% std=1.13]
Step     337 | Loss: 3.8841 | LR: 2.11e-04 | GradNorm: 0.54 | Tok/s: 2643 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.7% std=0.60] | layer_5[5.5-6.8% std=0.37] | layer_11[5.2-8.4% std=0.85] | layer_17[4.0-7.8% std=0.86] | layer_22[0.8-9.3% std=1.78]
  Expert: layer_1[5.9-7.3% std=0.31]
  Expert: layer_2[5.9-6.7% std=0.17]
  Expert: layer_3[5.0-12.8% std=1.75]
  Expert: layer_4[5.3-7.5% std=0.60]
  Expert: layer_6[4.4-9.5% std=1.04]
  Expert: layer_7[2.2-14.7% std=2.48]
  Expert: layer_8[3.5-11.5% std=1.74]
  Expert: layer_9[5.1-7.5% std=0.68]
  Expert: layer_10[4.1-9.7% std=1.42]
  Expert: layer_12[3.6-10.9% std=1.61]
  Expert: layer_13[4.3-8.2% std=1.00]
  Expert: layer_14[4.7-8.2% std=0.81]
  Expert: layer_15[3.6-9.2% std=1.38]
  Expert: layer_16[5.3-9.1% std=0.89]
  Expert: layer_18[4.8-8.0% std=0.92]
  Expert: layer_19[4.1-9.1% std=1.32]
  Expert: layer_20[5.3-7.5% std=0.58]
  Expert: layer_21[3.9-8.2% std=0.91]
Step     338 | Loss: 3.9055 | LR: 2.11e-04 | GradNorm: 0.57 | Tok/s: 2650 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.0-7.8% std=0.63] | layer_5[5.5-7.1% std=0.36] | layer_11[5.1-7.9% std=0.82] | layer_17[4.0-8.0% std=1.04] | layer_22[1.4-8.1% std=1.52]
  Expert: layer_1[6.0-7.2% std=0.29]
  Expert: layer_2[6.0-6.7% std=0.21]
  Expert: layer_3[4.7-12.9% std=1.79]
  Expert: layer_4[5.5-7.3% std=0.55]
  Expert: layer_6[4.2-9.1% std=0.99]
  Expert: layer_7[2.2-15.4% std=2.62]
  Expert: layer_8[3.4-11.5% std=1.71]
  Expert: layer_9[5.1-7.3% std=0.66]
  Expert: layer_10[4.0-10.5% std=1.52]
  Expert: layer_12[3.8-10.9% std=1.47]
  Expert: layer_13[4.0-8.4% std=1.17]
  Expert: layer_14[4.3-10.3% std=1.48]
  Expert: layer_15[3.4-8.9% std=1.21]
  Expert: layer_16[4.3-9.8% std=1.09]
  Expert: layer_18[4.7-8.6% std=0.89]
  Expert: layer_19[4.2-8.0% std=1.04]
  Expert: layer_20[5.1-8.2% std=0.92]
  Expert: layer_21[4.8-8.9% std=1.03]
Step     339 | Loss: 3.7941 | LR: 2.12e-04 | GradNorm: 0.51 | Tok/s: 2648 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.61] | layer_5[5.5-7.0% std=0.38] | layer_11[5.1-7.9% std=0.80] | layer_17[4.4-8.2% std=0.99] | layer_22[0.9-8.6% std=1.59]
  Expert: layer_1[5.9-7.2% std=0.30]
  Expert: layer_2[6.0-6.7% std=0.18]
  Expert: layer_3[4.9-12.6% std=1.70]
  Expert: layer_4[5.4-7.5% std=0.57]
  Expert: layer_6[4.4-8.9% std=0.90]
  Expert: layer_7[2.1-14.6% std=2.44]
  Expert: layer_8[3.6-11.5% std=1.69]
  Expert: layer_9[5.3-7.2% std=0.56]
  Expert: layer_10[4.0-9.7% std=1.38]
  Expert: layer_12[3.9-10.6% std=1.41]
  Expert: layer_13[4.2-8.1% std=0.97]
  Expert: layer_14[4.3-7.8% std=0.93]
  Expert: layer_15[3.4-8.6% std=1.16]
  Expert: layer_16[5.4-9.1% std=0.86]
  Expert: layer_18[4.5-8.0% std=1.02]
  Expert: layer_19[4.0-8.0% std=1.20]
  Expert: layer_20[4.8-8.1% std=0.71]
  Expert: layer_21[3.7-7.6% std=0.93]
Step     340 | Loss: 3.8759 | LR: 2.13e-04 | GradNorm: 0.52 | Tok/s: 2648 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.61] | layer_5[5.4-7.0% std=0.38] | layer_11[5.6-7.7% std=0.72] | layer_17[4.1-8.2% std=0.96] | layer_22[1.1-8.5% std=1.59]
  Expert: layer_1[6.0-7.2% std=0.29]
  Expert: layer_2[6.0-6.6% std=0.21]
  Expert: layer_3[4.8-12.3% std=1.62]
  Expert: layer_4[5.5-7.2% std=0.53]
  Expert: layer_6[4.2-8.8% std=0.95]
  Expert: layer_7[2.0-14.8% std=2.45]
  Expert: layer_8[3.7-10.6% std=1.44]
  Expert: layer_9[5.6-6.9% std=0.44]
  Expert: layer_10[4.3-10.3% std=1.41]
  Expert: layer_12[3.9-10.1% std=1.27]
  Expert: layer_13[4.0-8.1% std=1.13]
  Expert: layer_14[4.9-8.4% std=1.05]
  Expert: layer_15[2.7-8.3% std=1.31]
  Expert: layer_16[4.7-9.7% std=1.21]
  Expert: layer_18[4.8-7.7% std=0.75]
  Expert: layer_19[4.7-8.0% std=0.93]
  Expert: layer_20[4.5-7.7% std=0.86]
  Expert: layer_21[3.9-8.0% std=1.04]
Step     341 | Loss: 3.8432 | LR: 2.13e-04 | GradNorm: 0.50 | Tok/s: 2635 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.7% std=0.59] | layer_5[5.4-7.3% std=0.45] | layer_11[5.2-8.1% std=0.78] | layer_17[4.0-8.0% std=1.04] | layer_22[1.3-8.2% std=1.51]
  Expert: layer_1[6.0-7.1% std=0.27]
  Expert: layer_2[6.0-6.7% std=0.19]
  Expert: layer_3[4.6-12.3% std=1.66]
  Expert: layer_4[5.5-7.3% std=0.53]
  Expert: layer_6[4.4-8.8% std=0.96]
  Expert: layer_7[2.2-14.5% std=2.40]
  Expert: layer_8[3.8-10.8% std=1.51]
  Expert: layer_9[5.1-7.4% std=0.65]
  Expert: layer_10[4.3-10.0% std=1.39]
  Expert: layer_12[4.4-9.5% std=1.15]
  Expert: layer_13[4.3-7.6% std=0.97]
  Expert: layer_14[4.5-8.8% std=1.04]
  Expert: layer_15[3.0-8.1% std=1.19]
  Expert: layer_16[4.9-8.6% std=0.93]
  Expert: layer_18[4.3-8.2% std=1.02]
  Expert: layer_19[4.2-8.0% std=1.10]
  Expert: layer_20[5.0-7.3% std=0.72]
  Expert: layer_21[4.2-7.8% std=0.91]
Step     342 | Loss: 3.8446 | LR: 2.14e-04 | GradNorm: 0.48 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.1-7.8% std=0.63] | layer_5[5.7-7.2% std=0.41] | layer_11[5.2-7.9% std=0.87] | layer_17[4.2-7.7% std=0.88] | layer_22[1.0-8.0% std=1.56]
  Expert: layer_1[5.9-7.1% std=0.27]
  Expert: layer_2[5.8-6.7% std=0.21]
  Expert: layer_3[4.7-11.6% std=1.47]
  Expert: layer_4[5.5-7.2% std=0.52]
  Expert: layer_6[4.3-8.7% std=0.94]
  Expert: layer_7[2.1-14.7% std=2.47]
  Expert: layer_8[3.9-11.0% std=1.45]
  Expert: layer_9[5.2-7.6% std=0.66]
  Expert: layer_10[4.2-10.1% std=1.41]
  Expert: layer_12[3.8-9.7% std=1.32]
  Expert: layer_13[4.5-7.7% std=0.80]
  Expert: layer_14[4.9-7.9% std=0.82]
  Expert: layer_15[3.2-9.0% std=1.46]
  Expert: layer_16[5.1-8.8% std=0.80]
  Expert: layer_18[5.0-8.5% std=0.85]
  Expert: layer_19[3.6-8.2% std=1.27]
  Expert: layer_20[5.0-7.3% std=0.65]
  Expert: layer_21[4.7-7.6% std=0.81]
Step     343 | Loss: 3.8687 | LR: 2.14e-04 | GradNorm: 0.46 | Tok/s: 2648 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.59] | layer_5[5.8-7.1% std=0.40] | layer_11[5.2-7.2% std=0.61] | layer_17[4.6-8.5% std=0.84] | layer_22[1.1-8.2% std=1.51]
  Expert: layer_1[6.0-7.0% std=0.23]
  Expert: layer_2[6.0-6.5% std=0.17]
  Expert: layer_3[4.6-11.9% std=1.54]
  Expert: layer_4[5.6-7.4% std=0.55]
  Expert: layer_6[4.3-8.8% std=0.99]
  Expert: layer_7[2.2-15.0% std=2.51]
  Expert: layer_8[4.0-11.2% std=1.50]
  Expert: layer_9[5.2-7.5% std=0.69]
  Expert: layer_10[4.3-10.2% std=1.43]
  Expert: layer_12[3.8-10.1% std=1.35]
  Expert: layer_13[4.1-7.8% std=0.97]
  Expert: layer_14[4.8-8.3% std=0.97]
  Expert: layer_15[3.2-8.4% std=1.23]
  Expert: layer_16[4.7-8.6% std=0.92]
  Expert: layer_18[4.9-7.4% std=0.77]
  Expert: layer_19[4.4-7.8% std=1.01]
  Expert: layer_20[4.9-7.7% std=0.69]
  Expert: layer_21[3.8-7.5% std=0.97]
Step     344 | Loss: 3.7998 | LR: 2.15e-04 | GradNorm: 0.45 | Tok/s: 2651 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.59] | layer_5[5.7-7.2% std=0.42] | layer_11[5.2-8.0% std=0.83] | layer_17[4.4-7.8% std=0.85] | layer_22[1.1-8.0% std=1.46]
  Expert: layer_1[6.0-7.0% std=0.22]
  Expert: layer_2[6.0-6.5% std=0.16]
  Expert: layer_3[4.6-12.0% std=1.60]
  Expert: layer_4[5.7-7.3% std=0.52]
  Expert: layer_6[4.4-8.9% std=1.02]
  Expert: layer_7[2.3-15.4% std=2.59]
  Expert: layer_8[4.0-11.0% std=1.49]
  Expert: layer_9[5.0-7.4% std=0.62]
  Expert: layer_10[4.1-10.3% std=1.43]
  Expert: layer_12[4.8-10.3% std=1.31]
  Expert: layer_13[4.2-7.4% std=0.87]
  Expert: layer_14[4.6-8.6% std=1.03]
  Expert: layer_15[3.1-8.1% std=1.14]
  Expert: layer_16[5.3-9.0% std=0.97]
  Expert: layer_18[4.6-8.3% std=0.98]
  Expert: layer_19[4.7-7.7% std=0.94]
  Expert: layer_20[5.1-7.6% std=0.62]
  Expert: layer_21[3.3-8.8% std=1.30]
Step     345 | Loss: 3.7924 | LR: 2.16e-04 | GradNorm: 0.43 | Tok/s: 2646 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.61] | layer_5[5.6-7.1% std=0.47] | layer_11[4.9-7.5% std=0.86] | layer_17[4.8-7.4% std=0.78] | layer_22[1.2-8.2% std=1.45]
  Expert: layer_1[6.0-7.0% std=0.24]
  Expert: layer_2[6.0-6.6% std=0.18]
  Expert: layer_3[4.3-12.1% std=1.62]
  Expert: layer_4[5.4-7.4% std=0.62]
  Expert: layer_6[4.3-8.8% std=0.99]
  Expert: layer_7[2.5-14.6% std=2.41]
  Expert: layer_8[4.2-11.3% std=1.50]
  Expert: layer_9[5.2-7.6% std=0.64]
  Expert: layer_10[4.5-9.8% std=1.28]
  Expert: layer_12[4.5-9.2% std=1.13]
  Expert: layer_13[4.7-7.4% std=0.64]
  Expert: layer_14[5.1-7.9% std=0.76]
  Expert: layer_15[2.9-7.9% std=1.21]
  Expert: layer_16[5.1-8.3% std=0.74]
  Expert: layer_18[5.0-7.3% std=0.73]
  Expert: layer_19[4.6-7.3% std=0.83]
  Expert: layer_20[5.2-7.6% std=0.76]
  Expert: layer_21[4.0-8.2% std=0.88]
Step     346 | Loss: 3.8788 | LR: 2.16e-04 | GradNorm: 0.47 | Tok/s: 2632 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.59] | layer_5[5.7-7.3% std=0.46] | layer_11[4.7-7.6% std=0.91] | layer_17[4.9-7.1% std=0.71] | layer_22[1.0-7.7% std=1.55]
  Expert: layer_1[5.9-7.0% std=0.25]
  Expert: layer_2[6.0-6.6% std=0.15]
  Expert: layer_3[4.4-12.2% std=1.65]
  Expert: layer_4[5.2-7.3% std=0.63]
  Expert: layer_6[4.3-9.0% std=1.03]
  Expert: layer_7[2.5-14.8% std=2.48]
  Expert: layer_8[4.0-11.3% std=1.51]
  Expert: layer_9[4.8-7.9% std=0.72]
  Expert: layer_10[4.4-10.0% std=1.40]
  Expert: layer_12[4.6-9.2% std=1.09]
  Expert: layer_13[4.6-7.3% std=0.74]
  Expert: layer_14[4.9-7.5% std=0.82]
  Expert: layer_15[2.7-7.8% std=1.21]
  Expert: layer_16[5.3-7.9% std=0.61]
  Expert: layer_18[4.9-7.5% std=0.77]
  Expert: layer_19[4.8-7.8% std=0.93]
  Expert: layer_20[5.0-8.6% std=0.78]
  Expert: layer_21[3.8-7.6% std=0.82]
Step     347 | Loss: 3.8299 | LR: 2.17e-04 | GradNorm: 0.69 | Tok/s: 2646 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.59] | layer_5[5.4-7.3% std=0.49] | layer_11[4.6-8.2% std=1.04] | layer_17[4.0-8.6% std=1.08] | layer_22[0.9-7.8% std=1.62]
  Expert: layer_1[5.9-7.0% std=0.26]
  Expert: layer_2[6.0-6.5% std=0.13]
  Expert: layer_3[4.5-12.4% std=1.68]
  Expert: layer_4[5.1-7.6% std=0.69]
  Expert: layer_6[4.4-9.1% std=1.04]
  Expert: layer_7[2.5-14.5% std=2.45]
  Expert: layer_8[3.8-11.8% std=1.70]
  Expert: layer_9[4.8-7.9% std=0.77]
  Expert: layer_10[4.5-9.4% std=1.24]
  Expert: layer_12[4.3-9.1% std=1.12]
  Expert: layer_13[4.6-7.0% std=0.65]
  Expert: layer_14[5.1-8.1% std=0.85]
  Expert: layer_15[3.4-8.7% std=1.36]
  Expert: layer_16[4.7-8.9% std=0.92]
  Expert: layer_18[3.9-7.7% std=0.98]
  Expert: layer_19[4.4-7.7% std=0.91]
  Expert: layer_20[4.9-8.4% std=0.90]
  Expert: layer_21[4.4-8.4% std=0.85]
Step     348 | Loss: 3.8170 | LR: 2.18e-04 | GradNorm: 0.88 | Tok/s: 2651 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.9% std=0.57] | layer_5[5.6-7.0% std=0.41] | layer_11[4.6-8.5% std=1.02] | layer_17[4.5-8.7% std=1.17] | layer_22[1.1-8.7% std=1.83]
  Expert: layer_1[5.9-6.9% std=0.26]
  Expert: layer_2[6.0-6.6% std=0.15]
  Expert: layer_3[4.8-12.1% std=1.61]
  Expert: layer_4[5.1-7.3% std=0.63]
  Expert: layer_6[4.5-8.7% std=0.95]
  Expert: layer_7[2.4-15.0% std=2.57]
  Expert: layer_8[3.8-11.7% std=1.61]
  Expert: layer_9[5.3-7.5% std=0.61]
  Expert: layer_10[4.1-10.1% std=1.49]
  Expert: layer_12[4.2-10.0% std=1.31]
  Expert: layer_13[4.2-8.9% std=1.16]
  Expert: layer_14[4.1-8.8% std=1.27]
  Expert: layer_15[2.1-8.9% std=1.61]
  Expert: layer_16[4.6-7.5% std=0.92]
  Expert: layer_18[4.4-8.7% std=1.35]
  Expert: layer_19[4.1-10.3% std=1.78]
  Expert: layer_20[4.6-8.7% std=1.23]
  Expert: layer_21[2.2-8.6% std=1.52]
Step     349 | Loss: 3.8186 | LR: 2.18e-04 | GradNorm: 0.75 | Tok/s: 2646 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.9% std=0.59] | layer_5[5.4-7.3% std=0.53] | layer_11[4.2-8.5% std=1.15] | layer_17[4.7-8.6% std=1.23] | layer_22[1.3-8.5% std=1.72]
  Expert: layer_1[5.9-7.0% std=0.26]
  Expert: layer_2[6.0-6.6% std=0.15]
  Expert: layer_3[4.7-12.2% std=1.64]
  Expert: layer_4[5.0-7.4% std=0.61]
  Expert: layer_6[4.5-8.9% std=1.05]
  Expert: layer_7[3.0-13.9% std=2.22]
  Expert: layer_8[3.5-11.0% std=1.59]
  Expert: layer_9[4.8-7.8% std=0.76]
  Expert: layer_10[4.4-8.9% std=1.20]
  Expert: layer_12[4.5-8.5% std=1.03]
  Expert: layer_13[4.4-8.3% std=0.94]
  Expert: layer_14[5.0-8.6% std=0.88]
  Expert: layer_15[2.7-10.2% std=1.99]
  Expert: layer_16[4.6-9.0% std=1.20]
  Expert: layer_18[4.7-8.7% std=1.06]
  Expert: layer_19[4.3-8.1% std=1.12]
  Expert: layer_20[4.9-8.2% std=0.83]
  Expert: layer_21[4.3-9.1% std=1.26]
Step     350 | Loss: 3.8297 | LR: 2.19e-04 | GradNorm: 0.69 | Tok/s: 2653 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.56] | layer_5[5.4-6.9% std=0.41] | layer_11[4.7-8.3% std=1.19] | layer_17[3.4-7.9% std=1.24] | layer_22[0.8-7.9% std=1.61]
  Expert: layer_1[6.0-6.9% std=0.23]
  Expert: layer_2[6.0-6.6% std=0.14]
  Expert: layer_3[4.8-12.0% std=1.58]
  Expert: layer_4[4.9-7.5% std=0.66]
  Expert: layer_6[4.4-8.8% std=1.04]
  Expert: layer_7[2.7-12.7% std=2.09]
  Expert: layer_8[4.1-10.9% std=1.50]
  Expert: layer_9[4.9-7.3% std=0.77]
  Expert: layer_10[4.4-8.2% std=1.00]
  Expert: layer_12[4.9-8.8% std=1.12]
  Expert: layer_13[5.5-7.2% std=0.55]
  Expert: layer_14[4.9-7.8% std=0.77]
  Expert: layer_15[2.5-9.5% std=1.44]
  Expert: layer_16[4.9-8.1% std=1.01]
  Expert: layer_18[4.6-9.2% std=1.18]
  Expert: layer_19[4.7-9.0% std=1.12]
  Expert: layer_20[4.8-8.6% std=0.83]
  Expert: layer_21[4.7-9.0% std=1.06]
Step     351 | Loss: 3.8086 | LR: 2.19e-04 | GradNorm: 0.59 | Tok/s: 2631 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.60] | layer_5[5.3-6.9% std=0.42] | layer_11[5.2-8.6% std=0.99] | layer_17[4.0-7.9% std=0.91] | layer_22[1.1-9.1% std=1.72]
  Expert: layer_1[5.8-7.0% std=0.26]
  Expert: layer_2[6.0-6.6% std=0.14]
  Expert: layer_3[5.1-12.2% std=1.64]
  Expert: layer_4[5.2-7.2% std=0.52]
  Expert: layer_6[4.5-8.7% std=1.04]
  Expert: layer_7[2.8-12.6% std=1.99]
  Expert: layer_8[3.9-10.5% std=1.46]
  Expert: layer_9[4.9-7.7% std=0.84]
  Expert: layer_10[4.5-8.5% std=1.03]
  Expert: layer_12[4.5-9.5% std=1.25]
  Expert: layer_13[4.8-8.3% std=1.00]
  Expert: layer_14[4.7-7.8% std=0.94]
  Expert: layer_15[2.9-7.6% std=1.01]
  Expert: layer_16[4.8-7.9% std=0.82]
  Expert: layer_18[5.1-7.6% std=0.84]
  Expert: layer_19[4.3-8.1% std=1.20]
  Expert: layer_20[4.6-9.6% std=1.30]
  Expert: layer_21[2.6-8.4% std=1.31]
Step     352 | Loss: 3.8472 | LR: 2.20e-04 | GradNorm: 0.52 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.59] | layer_5[5.3-7.1% std=0.45] | layer_11[5.0-7.6% std=0.75] | layer_17[4.2-8.2% std=0.94] | layer_22[1.7-8.3% std=1.44]
  Expert: layer_1[5.9-6.9% std=0.21]
  Expert: layer_2[6.1-6.6% std=0.13]
  Expert: layer_3[4.7-11.8% std=1.57]
  Expert: layer_4[5.2-7.2% std=0.53]
  Expert: layer_6[4.5-8.6% std=0.98]
  Expert: layer_7[2.9-12.8% std=2.02]
  Expert: layer_8[3.9-9.9% std=1.34]
  Expert: layer_9[5.0-7.4% std=0.74]
  Expert: layer_10[4.6-8.8% std=1.06]
  Expert: layer_12[4.8-8.8% std=1.07]
  Expert: layer_13[4.9-8.0% std=0.93]
  Expert: layer_14[5.0-8.2% std=0.95]
  Expert: layer_15[2.6-8.3% std=1.33]
  Expert: layer_16[4.8-8.3% std=1.01]
  Expert: layer_18[4.3-8.0% std=0.95]
  Expert: layer_19[4.5-8.0% std=0.91]
  Expert: layer_20[4.8-7.9% std=0.88]
  Expert: layer_21[3.7-7.5% std=0.87]
Step     353 | Loss: 3.7888 | LR: 2.21e-04 | GradNorm: 0.47 | Tok/s: 2653 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.9% std=0.57] | layer_5[5.4-7.0% std=0.47] | layer_11[4.6-8.5% std=1.01] | layer_17[3.9-7.7% std=1.14] | layer_22[1.3-8.1% std=1.51]
  Expert: layer_1[5.9-6.9% std=0.22]
  Expert: layer_2[6.0-6.6% std=0.14]
  Expert: layer_3[4.9-11.9% std=1.56]
  Expert: layer_4[5.3-7.1% std=0.48]
  Expert: layer_6[4.4-8.6% std=0.94]
  Expert: layer_7[2.9-12.7% std=2.02]
  Expert: layer_8[3.7-10.0% std=1.48]
  Expert: layer_9[5.2-7.7% std=0.68]
  Expert: layer_10[4.3-8.6% std=1.04]
  Expert: layer_12[4.9-9.3% std=1.19]
  Expert: layer_13[5.1-8.2% std=0.80]
  Expert: layer_14[5.0-7.8% std=0.73]
  Expert: layer_15[2.6-8.1% std=1.32]
  Expert: layer_16[5.0-8.1% std=0.84]
  Expert: layer_18[4.2-7.1% std=0.82]
  Expert: layer_19[4.4-8.1% std=1.00]
  Expert: layer_20[5.2-7.9% std=0.82]
  Expert: layer_21[4.4-8.8% std=1.06]
Step     354 | Loss: 3.7460 | LR: 2.21e-04 | GradNorm: 0.45 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.59] | layer_5[5.4-7.0% std=0.49] | layer_11[4.8-7.8% std=0.81] | layer_17[4.3-8.0% std=1.07] | layer_22[1.2-8.0% std=1.56]
  Expert: layer_1[6.1-6.9% std=0.20]
  Expert: layer_2[6.0-6.7% std=0.14]
  Expert: layer_3[5.0-11.7% std=1.49]
  Expert: layer_4[5.3-7.0% std=0.44]
  Expert: layer_6[4.4-8.6% std=0.92]
  Expert: layer_7[2.5-12.9% std=2.09]
  Expert: layer_8[3.7-10.1% std=1.48]
  Expert: layer_9[5.4-7.7% std=0.66]
  Expert: layer_10[4.4-8.5% std=0.99]
  Expert: layer_12[4.9-9.3% std=1.14]
  Expert: layer_13[5.2-7.9% std=0.72]
  Expert: layer_14[4.6-8.4% std=0.88]
  Expert: layer_15[3.0-8.3% std=1.22]
  Expert: layer_16[5.4-8.0% std=0.72]
  Expert: layer_18[4.9-8.7% std=0.94]
  Expert: layer_19[4.6-8.3% std=0.90]
  Expert: layer_20[5.2-8.4% std=0.89]
  Expert: layer_21[3.6-8.3% std=1.04]
Step     355 | Loss: 3.7692 | LR: 2.22e-04 | GradNorm: 0.47 | Tok/s: 2649 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.9% std=0.58] | layer_5[5.5-7.1% std=0.47] | layer_11[5.2-8.0% std=0.69] | layer_17[3.9-8.2% std=1.20] | layer_22[1.2-8.3% std=1.49]
  Expert: layer_1[6.0-6.9% std=0.20]
  Expert: layer_2[6.0-6.6% std=0.13]
  Expert: layer_3[5.1-11.6% std=1.48]
  Expert: layer_4[5.1-7.1% std=0.52]
  Expert: layer_6[4.5-8.7% std=0.94]
  Expert: layer_7[2.3-12.2% std=2.03]
  Expert: layer_8[4.0-9.7% std=1.32]
  Expert: layer_9[5.0-8.1% std=0.74]
  Expert: layer_10[4.7-8.0% std=0.87]
  Expert: layer_12[4.3-8.9% std=1.12]
  Expert: layer_13[5.4-7.5% std=0.72]
  Expert: layer_14[4.5-7.9% std=0.94]
  Expert: layer_15[3.1-8.0% std=1.17]
  Expert: layer_16[5.1-8.1% std=0.77]
  Expert: layer_18[4.4-7.8% std=1.04]
  Expert: layer_19[4.1-8.0% std=1.07]
  Expert: layer_20[4.4-8.9% std=1.05]
  Expert: layer_21[4.1-9.3% std=1.17]
Per-layer gamma: 0.0 ~ 0.0 (sqrt, 23 MoE layers)

============================================================
학습 시작 (step=355, epoch=0)
============================================================
Resume position: global_idx=131680, block=1, local_idx=31840

=== Epoch 1/1 ===
Step     356 | Loss: 3.6680 | LR: 2.22e-04 | GradNorm: 0.42 | Tok/s: 2723 | Mem: 65.2GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.59] | layer_5[5.6-7.1% std=0.43] | layer_11[5.2-7.4% std=0.60] | layer_17[3.9-7.5% std=1.09] | layer_22[1.3-8.4% std=1.52]
  Expert: layer_1[5.9-6.7% std=0.19]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[5.0-11.4% std=1.41]
  Expert: layer_4[5.3-7.4% std=0.59]
  Expert: layer_6[4.9-8.9% std=0.87]
  Expert: layer_7[2.6-12.2% std=1.98]
  Expert: layer_8[4.0-9.9% std=1.43]
  Expert: layer_9[4.9-8.8% std=0.91]
  Expert: layer_10[4.6-8.5% std=1.05]
  Expert: layer_12[4.9-8.6% std=1.11]
  Expert: layer_13[5.0-7.7% std=0.74]
  Expert: layer_14[4.7-7.2% std=0.67]
  Expert: layer_15[2.7-8.3% std=1.38]
  Expert: layer_16[5.0-7.7% std=0.77]
  Expert: layer_18[4.0-8.0% std=0.96]
  Expert: layer_19[4.5-8.6% std=1.17]
  Expert: layer_20[4.8-8.4% std=0.93]
  Expert: layer_21[4.3-7.6% std=0.94]
Step     357 | Loss: 3.6497 | LR: 2.23e-04 | GradNorm: 0.37 | Tok/s: 2694 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.59] | layer_5[5.6-7.2% std=0.51] | layer_11[5.1-8.5% std=0.89] | layer_17[4.2-9.0% std=1.19] | layer_22[1.7-9.2% std=1.64]
  Expert: layer_1[5.8-6.8% std=0.21]
  Expert: layer_2[6.0-6.5% std=0.20]
  Expert: layer_3[5.0-11.2% std=1.38]
  Expert: layer_4[5.4-7.5% std=0.61]
  Expert: layer_6[5.1-8.9% std=0.86]
  Expert: layer_7[3.4-11.6% std=1.75]
  Expert: layer_8[3.7-10.0% std=1.60]
  Expert: layer_9[4.7-9.4% std=1.03]
  Expert: layer_10[4.5-8.5% std=1.08]
  Expert: layer_12[4.7-9.3% std=1.23]
  Expert: layer_13[4.7-8.6% std=0.99]
  Expert: layer_14[4.2-7.7% std=0.87]
  Expert: layer_15[2.7-8.7% std=1.48]
  Expert: layer_16[4.1-8.2% std=1.02]
  Expert: layer_18[4.6-9.7% std=1.33]
  Expert: layer_19[4.5-8.3% std=1.15]
  Expert: layer_20[4.5-7.9% std=1.04]
  Expert: layer_21[3.5-9.5% std=1.33]
Step     358 | Loss: 3.6788 | LR: 2.24e-04 | GradNorm: 0.39 | Tok/s: 2692 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.59] | layer_5[5.4-7.3% std=0.49] | layer_11[5.3-7.8% std=0.66] | layer_17[4.4-8.1% std=0.95] | layer_22[1.7-9.3% std=1.50]
  Expert: layer_1[5.7-6.8% std=0.23]
  Expert: layer_2[5.9-6.6% std=0.19]
  Expert: layer_3[5.1-10.9% std=1.30]
  Expert: layer_4[5.6-7.6% std=0.63]
  Expert: layer_6[5.2-8.8% std=0.78]
  Expert: layer_7[2.9-12.0% std=1.85]
  Expert: layer_8[4.0-10.5% std=1.46]
  Expert: layer_9[4.9-8.5% std=0.84]
  Expert: layer_10[4.3-9.1% std=1.16]
  Expert: layer_12[5.1-8.5% std=0.92]
  Expert: layer_13[4.9-7.9% std=0.89]
  Expert: layer_14[4.6-8.6% std=0.99]
  Expert: layer_15[2.3-7.6% std=1.26]
  Expert: layer_16[4.6-8.0% std=0.90]
  Expert: layer_18[4.8-8.3% std=0.95]
  Expert: layer_19[4.6-8.1% std=0.94]
  Expert: layer_20[5.2-8.6% std=0.94]
  Expert: layer_21[3.7-8.0% std=0.99]
Step     359 | Loss: 3.7249 | LR: 2.24e-04 | GradNorm: 0.41 | Tok/s: 2647 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.60] | layer_5[5.2-7.3% std=0.52] | layer_11[5.3-7.7% std=0.67] | layer_17[4.6-8.6% std=1.03] | layer_22[1.5-8.8% std=1.64]
  Expert: layer_1[5.9-6.7% std=0.21]
  Expert: layer_2[5.9-6.5% std=0.19]
  Expert: layer_3[5.1-11.0% std=1.29]
  Expert: layer_4[5.6-7.8% std=0.65]
  Expert: layer_6[5.0-8.7% std=0.77]
  Expert: layer_7[3.1-12.1% std=1.85]
  Expert: layer_8[4.0-10.9% std=1.57]
  Expert: layer_9[4.9-8.8% std=1.00]
  Expert: layer_10[4.5-8.8% std=1.07]
  Expert: layer_12[5.1-8.8% std=1.05]
  Expert: layer_13[4.8-8.0% std=0.83]
  Expert: layer_14[4.5-7.7% std=0.73]
  Expert: layer_15[2.8-8.2% std=1.34]
  Expert: layer_16[4.6-8.2% std=0.97]
  Expert: layer_18[4.6-9.1% std=1.15]
  Expert: layer_19[4.2-8.5% std=1.06]
  Expert: layer_20[5.3-8.7% std=0.82]
  Expert: layer_21[4.1-7.4% std=0.89]
Step     360 | Loss: 3.6726 | LR: 2.25e-04 | GradNorm: 0.38 | Tok/s: 2644 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.59] | layer_5[5.3-7.4% std=0.54] | layer_11[5.0-8.7% std=0.82] | layer_17[4.5-8.4% std=1.16] | layer_22[1.6-8.1% std=1.52]
  Expert: layer_1[6.0-6.6% std=0.22]
  Expert: layer_2[6.0-6.6% std=0.18]
  Expert: layer_3[5.1-11.3% std=1.37]
  Expert: layer_4[5.6-7.8% std=0.68]
  Expert: layer_6[5.1-8.7% std=0.81]
  Expert: layer_7[3.0-12.2% std=1.90]
  Expert: layer_8[3.6-11.2% std=1.63]
  Expert: layer_9[5.0-8.5% std=1.02]
  Expert: layer_10[4.2-8.6% std=1.08]
  Expert: layer_12[4.5-9.1% std=1.07]
  Expert: layer_13[4.5-8.2% std=0.88]
  Expert: layer_14[4.3-8.2% std=0.98]
  Expert: layer_15[2.7-8.3% std=1.22]
  Expert: layer_16[4.7-8.2% std=0.93]
  Expert: layer_18[4.9-7.8% std=1.00]
  Expert: layer_19[4.5-8.6% std=1.19]
  Expert: layer_20[4.9-7.7% std=0.86]
  Expert: layer_21[4.3-8.1% std=0.83]
Step     361 | Loss: 3.6915 | LR: 2.26e-04 | GradNorm: 0.41 | Tok/s: 2627 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.58] | layer_5[5.1-7.6% std=0.57] | layer_11[4.9-8.0% std=0.83] | layer_17[4.6-8.2% std=1.14] | layer_22[1.4-7.7% std=1.49]
  Expert: layer_1[6.0-6.7% std=0.24]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[5.2-11.4% std=1.39]
  Expert: layer_4[5.3-8.0% std=0.72]
  Expert: layer_6[5.0-8.7% std=0.80]
  Expert: layer_7[2.8-12.0% std=1.85]
  Expert: layer_8[3.6-11.3% std=1.64]
  Expert: layer_9[5.0-8.8% std=1.05]
  Expert: layer_10[4.1-8.1% std=1.13]
  Expert: layer_12[4.8-9.1% std=1.06]
  Expert: layer_13[4.2-7.9% std=0.86]
  Expert: layer_14[4.7-8.1% std=0.82]
  Expert: layer_15[2.3-8.3% std=1.25]
  Expert: layer_16[5.3-7.6% std=0.57]
  Expert: layer_18[4.9-9.3% std=1.27]
  Expert: layer_19[4.1-8.1% std=1.24]
  Expert: layer_20[5.2-7.7% std=0.68]
  Expert: layer_21[3.9-7.9% std=0.89]
Step     362 | Loss: 3.7533 | LR: 2.26e-04 | GradNorm: 0.43 | Tok/s: 2644 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.60] | layer_5[5.0-7.3% std=0.52] | layer_11[5.0-8.5% std=0.85] | layer_17[3.6-8.7% std=1.25] | layer_22[1.0-8.7% std=1.60]
  Expert: layer_1[5.9-6.8% std=0.23]
  Expert: layer_2[5.8-6.6% std=0.18]
  Expert: layer_3[5.1-11.5% std=1.44]
  Expert: layer_4[5.2-7.7% std=0.72]
  Expert: layer_6[4.9-8.6% std=0.83]
  Expert: layer_7[2.8-11.9% std=1.82]
  Expert: layer_8[3.6-11.0% std=1.61]
  Expert: layer_9[4.8-8.6% std=0.98]
  Expert: layer_10[4.4-8.6% std=1.19]
  Expert: layer_12[4.6-8.8% std=1.06]
  Expert: layer_13[4.5-8.3% std=0.96]
  Expert: layer_14[4.7-8.1% std=0.81]
  Expert: layer_15[2.4-8.7% std=1.40]
  Expert: layer_16[5.0-7.8% std=0.72]
  Expert: layer_18[4.6-8.8% std=1.01]
  Expert: layer_19[4.2-8.6% std=1.14]
  Expert: layer_20[5.2-7.5% std=0.77]
  Expert: layer_21[4.5-7.3% std=0.87]
Step     363 | Loss: 3.7349 | LR: 2.27e-04 | GradNorm: 0.50 | Tok/s: 2650 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.8% std=0.57] | layer_5[5.0-7.2% std=0.53] | layer_11[4.4-8.5% std=1.03] | layer_17[4.2-9.1% std=1.20] | layer_22[1.3-8.6% std=1.67]
  Expert: layer_1[5.9-6.6% std=0.21]
  Expert: layer_2[5.9-6.5% std=0.19]
  Expert: layer_3[5.3-11.5% std=1.42]
  Expert: layer_4[4.9-7.9% std=0.79]
  Expert: layer_6[4.8-8.7% std=0.83]
  Expert: layer_7[3.0-11.6% std=1.77]
  Expert: layer_8[3.5-10.8% std=1.66]
  Expert: layer_9[4.5-8.8% std=1.12]
  Expert: layer_10[4.3-8.5% std=1.20]
  Expert: layer_12[4.9-9.3% std=1.09]
  Expert: layer_13[4.1-8.7% std=1.05]
  Expert: layer_14[4.6-7.9% std=0.94]
  Expert: layer_15[2.3-9.1% std=1.50]
  Expert: layer_16[4.5-7.8% std=0.78]
  Expert: layer_18[5.0-9.2% std=1.21]
  Expert: layer_19[4.4-8.3% std=1.16]
  Expert: layer_20[4.5-7.7% std=0.94]
  Expert: layer_21[3.8-7.8% std=1.08]
Step     364 | Loss: 3.6885 | LR: 2.28e-04 | GradNorm: 0.64 | Tok/s: 2635 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.61] | layer_5[5.1-7.3% std=0.52] | layer_11[4.9-9.1% std=0.91] | layer_17[3.1-8.7% std=1.33] | layer_22[1.1-10.1% std=1.82]
  Expert: layer_1[5.9-6.6% std=0.20]
  Expert: layer_2[6.0-6.6% std=0.15]
  Expert: layer_3[5.4-11.4% std=1.45]
  Expert: layer_4[5.0-7.8% std=0.70]
  Expert: layer_6[5.0-8.8% std=0.84]
  Expert: layer_7[2.9-11.7% std=1.75]
  Expert: layer_8[3.6-10.1% std=1.46]
  Expert: layer_9[4.3-8.3% std=0.98]
  Expert: layer_10[4.2-8.5% std=1.22]
  Expert: layer_12[4.9-8.8% std=1.13]
  Expert: layer_13[4.2-8.0% std=1.02]
  Expert: layer_14[4.1-7.6% std=0.88]
  Expert: layer_15[2.5-8.4% std=1.17]
  Expert: layer_16[4.9-7.3% std=0.63]
  Expert: layer_18[4.4-8.7% std=1.05]
  Expert: layer_19[4.3-8.9% std=1.29]
  Expert: layer_20[5.0-8.2% std=0.92]
  Expert: layer_21[4.1-8.5% std=1.24]
Step     365 | Loss: 3.6805 | LR: 2.28e-04 | GradNorm: 0.77 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.62] | layer_5[5.1-7.5% std=0.58] | layer_11[3.9-8.8% std=1.11] | layer_17[4.4-10.9% std=1.54] | layer_22[1.3-9.7% std=1.81]
  Expert: layer_1[5.8-6.5% std=0.20]
  Expert: layer_2[5.9-6.6% std=0.22]
  Expert: layer_3[5.3-11.5% std=1.45]
  Expert: layer_4[4.8-7.9% std=0.79]
  Expert: layer_6[5.0-9.0% std=0.90]
  Expert: layer_7[2.9-11.0% std=1.74]
  Expert: layer_8[3.6-10.9% std=1.77]
  Expert: layer_9[4.8-8.5% std=1.09]
  Expert: layer_10[4.5-8.2% std=1.00]
  Expert: layer_12[4.1-9.6% std=1.20]
  Expert: layer_13[4.2-9.1% std=1.19]
  Expert: layer_14[4.6-9.4% std=1.10]
  Expert: layer_15[2.1-10.2% std=1.74]
  Expert: layer_16[4.4-10.5% std=1.54]
  Expert: layer_18[4.2-10.4% std=1.57]
  Expert: layer_19[3.6-8.8% std=1.49]
  Expert: layer_20[4.1-9.4% std=1.46]
  Expert: layer_21[3.8-8.8% std=1.27]
Step     366 | Loss: 3.6376 | LR: 2.29e-04 | GradNorm: 0.74 | Tok/s: 2614 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.59] | layer_5[5.1-7.3% std=0.50] | layer_11[4.8-8.9% std=1.00] | layer_17[3.5-9.4% std=1.35] | layer_22[1.9-8.1% std=1.66]
  Expert: layer_1[5.7-6.6% std=0.24]
  Expert: layer_2[5.9-6.6% std=0.23]
  Expert: layer_3[5.2-11.7% std=1.52]
  Expert: layer_4[5.1-8.0% std=0.74]
  Expert: layer_6[5.1-9.2% std=0.91]
  Expert: layer_7[3.2-11.4% std=1.63]
  Expert: layer_8[3.8-9.7% std=1.53]
  Expert: layer_9[4.2-8.8% std=1.18]
  Expert: layer_10[4.2-8.7% std=1.27]
  Expert: layer_12[4.0-9.0% std=1.34]
  Expert: layer_13[4.2-8.5% std=1.15]
  Expert: layer_14[4.6-8.3% std=1.07]
  Expert: layer_15[2.7-9.1% std=1.27]
  Expert: layer_16[4.2-8.5% std=1.08]
  Expert: layer_18[4.0-9.5% std=1.30]
  Expert: layer_19[4.2-9.4% std=1.25]
  Expert: layer_20[4.3-8.6% std=1.29]
  Expert: layer_21[3.6-9.2% std=1.44]
Step     367 | Loss: 3.6876 | LR: 2.29e-04 | GradNorm: 0.60 | Tok/s: 2576 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-8.0% std=0.60] | layer_5[5.2-7.5% std=0.48] | layer_11[4.9-8.6% std=0.82] | layer_17[3.4-8.0% std=0.98] | layer_22[0.9-10.2% std=2.01]
  Expert: layer_1[6.0-6.7% std=0.20]
  Expert: layer_2[5.9-6.6% std=0.18]
  Expert: layer_3[5.1-11.7% std=1.53]
  Expert: layer_4[5.4-8.4% std=0.78]
  Expert: layer_6[5.0-9.0% std=0.89]
  Expert: layer_7[2.7-11.1% std=1.69]
  Expert: layer_8[4.0-9.7% std=1.40]
  Expert: layer_9[4.7-7.9% std=0.90]
  Expert: layer_10[4.3-8.5% std=1.15]
  Expert: layer_12[4.6-9.0% std=1.06]
  Expert: layer_13[4.3-8.4% std=1.12]
  Expert: layer_14[4.4-7.4% std=0.68]
  Expert: layer_15[2.3-8.3% std=1.36]
  Expert: layer_16[5.2-8.4% std=0.81]
  Expert: layer_18[4.5-8.0% std=1.10]
  Expert: layer_19[4.0-8.4% std=1.36]
  Expert: layer_20[5.1-7.4% std=0.78]
  Expert: layer_21[3.5-8.8% std=1.36]
Step     368 | Loss: 3.7199 | LR: 2.30e-04 | GradNorm: 0.50 | Tok/s: 2597 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.59] | layer_5[5.5-7.4% std=0.49] | layer_11[4.8-7.7% std=0.73] | layer_17[4.5-7.7% std=0.83] | layer_22[1.4-8.2% std=1.58]
  Expert: layer_1[6.0-6.7% std=0.18]
  Expert: layer_2[6.0-6.5% std=0.15]
  Expert: layer_3[5.2-11.7% std=1.50]
  Expert: layer_4[5.2-8.2% std=0.82]
  Expert: layer_6[4.9-9.0% std=0.89]
  Expert: layer_7[2.7-11.0% std=1.68]
  Expert: layer_8[4.1-9.2% std=1.40]
  Expert: layer_9[5.1-7.9% std=0.86]
  Expert: layer_10[4.3-7.9% std=1.07]
  Expert: layer_12[5.0-8.8% std=0.96]
  Expert: layer_13[4.4-7.8% std=0.86]
  Expert: layer_14[4.7-7.1% std=0.56]
  Expert: layer_15[2.2-7.9% std=1.37]
  Expert: layer_16[4.9-8.1% std=0.75]
  Expert: layer_18[5.0-8.3% std=0.95]
  Expert: layer_19[4.5-8.3% std=1.20]
  Expert: layer_20[4.7-7.8% std=0.97]
  Expert: layer_21[3.6-8.8% std=1.31]
Step     369 | Loss: 3.6401 | LR: 2.31e-04 | GradNorm: 0.52 | Tok/s: 2641 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-8.0% std=0.61] | layer_5[5.4-7.6% std=0.56] | layer_11[4.8-7.3% std=0.66] | layer_17[4.6-8.4% std=1.15] | layer_22[1.3-8.3% std=1.68]
  Expert: layer_1[5.9-6.8% std=0.22]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[5.2-11.7% std=1.52]
  Expert: layer_4[5.2-8.1% std=0.81]
  Expert: layer_6[5.0-9.0% std=0.90]
  Expert: layer_7[2.7-11.2% std=1.77]
  Expert: layer_8[3.9-9.6% std=1.54]
  Expert: layer_9[4.9-8.0% std=0.90]
  Expert: layer_10[4.5-7.8% std=1.04]
  Expert: layer_12[4.8-8.4% std=1.03]
  Expert: layer_13[4.7-7.7% std=0.75]
  Expert: layer_14[4.4-7.7% std=0.75]
  Expert: layer_15[2.5-8.3% std=1.27]
  Expert: layer_16[4.8-7.8% std=0.74]
  Expert: layer_18[4.3-9.3% std=1.23]
  Expert: layer_19[4.5-8.3% std=1.22]
  Expert: layer_20[4.1-8.2% std=1.03]
  Expert: layer_21[3.6-8.7% std=1.28]
Step     370 | Loss: 3.7148 | LR: 2.31e-04 | GradNorm: 0.48 | Tok/s: 2648 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-8.0% std=0.62] | layer_5[5.3-7.7% std=0.59] | layer_11[4.9-8.1% std=0.81] | layer_17[3.9-8.3% std=0.98] | layer_22[1.1-8.6% std=1.75]
  Expert: layer_1[6.0-6.8% std=0.23]
  Expert: layer_2[5.9-6.8% std=0.22]
  Expert: layer_3[5.0-12.1% std=1.61]
  Expert: layer_4[5.1-7.9% std=0.77]
  Expert: layer_6[5.0-9.0% std=0.93]
  Expert: layer_7[2.8-12.2% std=1.88]
  Expert: layer_8[3.9-9.3% std=1.46]
  Expert: layer_9[4.8-8.2% std=0.90]
  Expert: layer_10[4.6-8.6% std=1.08]
  Expert: layer_12[4.7-9.3% std=1.14]
  Expert: layer_13[4.7-8.2% std=0.89]
  Expert: layer_14[4.7-7.8% std=0.74]
  Expert: layer_15[2.6-9.2% std=1.40]
  Expert: layer_16[5.5-7.4% std=0.63]
  Expert: layer_18[4.7-8.0% std=0.95]
  Expert: layer_19[4.7-8.0% std=1.15]
  Expert: layer_20[4.9-8.9% std=1.02]
  Expert: layer_21[3.7-7.9% std=1.07]
Step     371 | Loss: 3.6439 | LR: 2.32e-04 | GradNorm: 0.46 | Tok/s: 2588 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.60] | layer_5[5.3-7.5% std=0.57] | layer_11[4.9-8.3% std=0.70] | layer_17[3.1-8.4% std=1.25] | layer_22[1.1-10.5% std=1.92]
  Expert: layer_1[6.0-6.8% std=0.22]
  Expert: layer_2[5.9-6.7% std=0.20]
  Expert: layer_3[5.0-11.9% std=1.60]
  Expert: layer_4[5.0-7.8% std=0.75]
  Expert: layer_6[5.0-9.0% std=0.92]
  Expert: layer_7[2.8-12.3% std=1.87]
  Expert: layer_8[4.1-9.3% std=1.35]
  Expert: layer_9[5.0-8.0% std=0.82]
  Expert: layer_10[4.7-8.4% std=1.02]
  Expert: layer_12[4.6-8.5% std=1.12]
  Expert: layer_13[4.6-7.8% std=0.86]
  Expert: layer_14[4.5-7.6% std=0.91]
  Expert: layer_15[2.1-8.4% std=1.45]
  Expert: layer_16[4.7-8.0% std=0.90]
  Expert: layer_18[4.3-7.7% std=0.92]
  Expert: layer_19[4.2-8.1% std=1.19]
  Expert: layer_20[4.9-7.3% std=0.83]
  Expert: layer_21[3.7-8.4% std=1.32]
Step     372 | Loss: 3.6493 | LR: 2.32e-04 | GradNorm: 0.46 | Tok/s: 2609 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-8.0% std=0.62] | layer_5[5.4-7.4% std=0.59] | layer_11[5.4-7.6% std=0.65] | layer_17[4.0-9.1% std=1.30] | layer_22[1.5-8.6% std=1.64]
  Expert: layer_1[6.0-6.8% std=0.21]
  Expert: layer_2[5.9-6.7% std=0.24]
  Expert: layer_3[5.0-11.9% std=1.60]
  Expert: layer_4[4.8-7.7% std=0.76]
  Expert: layer_6[4.7-8.9% std=0.94]
  Expert: layer_7[2.7-12.1% std=1.85]
  Expert: layer_8[4.1-9.6% std=1.47]
  Expert: layer_9[5.1-7.7% std=0.75]
  Expert: layer_10[5.0-7.5% std=0.73]
  Expert: layer_12[4.6-8.6% std=1.04]
  Expert: layer_13[4.6-7.4% std=0.72]
  Expert: layer_14[5.1-7.6% std=0.65]
  Expert: layer_15[2.2-8.1% std=1.31]
  Expert: layer_16[5.1-8.2% std=0.78]
  Expert: layer_18[4.0-8.5% std=1.17]
  Expert: layer_19[4.2-7.9% std=1.14]
  Expert: layer_20[4.8-8.1% std=0.92]
  Expert: layer_21[3.3-8.0% std=1.22]
Step     373 | Loss: 3.7136 | LR: 2.33e-04 | GradNorm: 0.50 | Tok/s: 2597 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.59] | layer_5[5.4-7.4% std=0.62] | layer_11[5.1-8.5% std=1.02] | layer_17[5.0-8.5% std=1.02] | layer_22[1.3-8.2% std=1.60]
  Expert: layer_1[6.0-6.9% std=0.21]
  Expert: layer_2[5.9-6.8% std=0.24]
  Expert: layer_3[4.9-11.7% std=1.57]
  Expert: layer_4[4.9-7.8% std=0.77]
  Expert: layer_6[4.8-8.8% std=0.96]
  Expert: layer_7[2.6-12.4% std=1.98]
  Expert: layer_8[3.7-10.0% std=1.59]
  Expert: layer_9[5.0-8.0% std=0.81]
  Expert: layer_10[4.9-8.0% std=0.90]
  Expert: layer_12[4.6-8.6% std=1.06]
  Expert: layer_13[4.6-7.7% std=0.83]
  Expert: layer_14[5.0-7.1% std=0.60]
  Expert: layer_15[2.2-9.0% std=1.55]
  Expert: layer_16[5.0-7.9% std=0.88]
  Expert: layer_18[4.3-8.3% std=1.04]
  Expert: layer_19[4.5-8.2% std=1.19]
  Expert: layer_20[4.2-7.7% std=0.95]
  Expert: layer_21[3.5-8.2% std=1.12]
Step     374 | Loss: 3.6253 | LR: 2.34e-04 | GradNorm: 0.40 | Tok/s: 2631 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.58] | layer_5[5.2-7.5% std=0.63] | layer_11[5.0-8.7% std=0.98] | layer_17[3.8-8.6% std=1.18] | layer_22[1.2-9.1% std=1.69]
  Expert: layer_1[6.0-6.8% std=0.19]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[4.9-11.6% std=1.54]
  Expert: layer_4[5.0-7.9% std=0.78]
  Expert: layer_6[4.8-8.9% std=0.96]
  Expert: layer_7[2.5-12.6% std=1.95]
  Expert: layer_8[3.7-10.3% std=1.61]
  Expert: layer_9[4.9-8.0% std=0.88]
  Expert: layer_10[4.7-8.2% std=0.92]
  Expert: layer_12[4.6-8.5% std=1.13]
  Expert: layer_13[4.9-7.5% std=0.70]
  Expert: layer_14[5.2-7.0% std=0.48]
  Expert: layer_15[2.0-8.3% std=1.48]
  Expert: layer_16[5.1-7.7% std=0.78]
  Expert: layer_18[4.1-7.8% std=1.00]
  Expert: layer_19[4.4-8.7% std=1.23]
  Expert: layer_20[4.5-8.1% std=0.99]
  Expert: layer_21[3.3-8.5% std=1.16]
Step     375 | Loss: 3.6428 | LR: 2.34e-04 | GradNorm: 0.37 | Tok/s: 2678 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.56] | layer_5[5.2-7.6% std=0.66] | layer_11[5.0-8.8% std=1.03] | layer_17[3.9-8.1% std=1.01] | layer_22[1.1-9.3% std=1.79]
  Expert: layer_1[6.0-6.8% std=0.18]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[4.9-11.6% std=1.59]
  Expert: layer_4[4.8-7.9% std=0.83]
  Expert: layer_6[4.7-9.0% std=1.00]
  Expert: layer_7[2.7-12.4% std=1.90]
  Expert: layer_8[3.6-10.6% std=1.64]
  Expert: layer_9[4.7-8.0% std=0.91]
  Expert: layer_10[4.5-8.3% std=0.95]
  Expert: layer_12[4.6-8.7% std=1.15]
  Expert: layer_13[4.7-8.5% std=0.87]
  Expert: layer_14[5.1-7.2% std=0.66]
  Expert: layer_15[2.2-8.4% std=1.48]
  Expert: layer_16[4.8-7.5% std=0.90]
  Expert: layer_18[4.6-7.9% std=0.88]
  Expert: layer_19[4.3-8.5% std=1.33]
  Expert: layer_20[4.6-8.3% std=1.00]
  Expert: layer_21[3.0-9.1% std=1.27]
Step     376 | Loss: 3.6493 | LR: 2.35e-04 | GradNorm: 0.41 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.59] | layer_5[5.3-7.7% std=0.68] | layer_11[4.8-7.8% std=1.06] | layer_17[4.0-8.6% std=1.10] | layer_22[1.3-10.1% std=1.79]
  Expert: layer_1[6.0-6.9% std=0.20]
  Expert: layer_2[5.8-6.7% std=0.24]
  Expert: layer_3[4.9-11.8% std=1.61]
  Expert: layer_4[4.7-7.8% std=0.81]
  Expert: layer_6[4.8-8.9% std=0.97]
  Expert: layer_7[2.6-12.6% std=1.98]
  Expert: layer_8[3.6-10.4% std=1.54]
  Expert: layer_9[4.9-7.6% std=0.77]
  Expert: layer_10[4.5-8.7% std=1.00]
  Expert: layer_12[4.8-9.3% std=1.10]
  Expert: layer_13[4.8-8.0% std=0.83]
  Expert: layer_14[4.8-7.5% std=0.81]
  Expert: layer_15[2.1-8.9% std=1.55]
  Expert: layer_16[4.8-7.9% std=0.94]
  Expert: layer_18[4.4-7.8% std=0.87]
  Expert: layer_19[4.3-8.9% std=1.35]
  Expert: layer_20[4.7-9.0% std=1.16]
  Expert: layer_21[3.1-7.8% std=1.15]
Step     377 | Loss: 3.5545 | LR: 2.36e-04 | GradNorm: 0.44 | Tok/s: 2639 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.57] | layer_5[5.2-7.6% std=0.75] | layer_11[4.9-8.1% std=0.98] | layer_17[4.2-10.1% std=1.34] | layer_22[1.2-9.5% std=2.01]
  Expert: layer_1[5.9-6.8% std=0.22]
  Expert: layer_2[5.9-6.7% std=0.24]
  Expert: layer_3[5.0-11.4% std=1.46]
  Expert: layer_4[4.7-8.2% std=0.83]
  Expert: layer_6[5.1-9.1% std=0.96]
  Expert: layer_7[2.8-11.9% std=1.82]
  Expert: layer_8[3.6-10.8% std=1.63]
  Expert: layer_9[4.9-8.1% std=0.86]
  Expert: layer_10[4.5-7.9% std=0.84]
  Expert: layer_12[4.2-8.3% std=1.17]
  Expert: layer_13[4.9-8.1% std=0.91]
  Expert: layer_14[4.6-8.1% std=0.82]
  Expert: layer_15[1.9-9.0% std=1.78]
  Expert: layer_16[4.5-8.8% std=1.06]
  Expert: layer_18[3.7-10.3% std=1.64]
  Expert: layer_19[3.9-8.9% std=1.54]
  Expert: layer_20[4.3-8.6% std=1.21]
  Expert: layer_21[2.9-8.5% std=1.31]
Step     378 | Loss: 3.6341 | LR: 2.36e-04 | GradNorm: 0.42 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.57] | layer_5[5.3-7.7% std=0.68] | layer_11[5.2-7.6% std=0.82] | layer_17[4.1-8.2% std=1.00] | layer_22[1.4-10.8% std=1.91]
  Expert: layer_1[5.9-6.9% std=0.21]
  Expert: layer_2[6.0-6.6% std=0.20]
  Expert: layer_3[5.1-11.4% std=1.49]
  Expert: layer_4[4.8-7.8% std=0.80]
  Expert: layer_6[5.0-9.1% std=0.98]
  Expert: layer_7[2.7-12.8% std=2.00]
  Expert: layer_8[3.6-10.7% std=1.65]
  Expert: layer_9[4.9-7.6% std=0.75]
  Expert: layer_10[4.4-8.3% std=0.98]
  Expert: layer_12[4.5-8.0% std=0.99]
  Expert: layer_13[4.8-7.8% std=0.80]
  Expert: layer_14[4.8-7.4% std=0.69]
  Expert: layer_15[1.7-9.1% std=1.69]
  Expert: layer_16[4.8-7.5% std=0.75]
  Expert: layer_18[4.4-8.9% std=1.21]
  Expert: layer_19[4.5-9.3% std=1.30]
  Expert: layer_20[4.3-8.7% std=1.25]
  Expert: layer_21[3.0-8.3% std=1.21]
Step     379 | Loss: 3.5986 | LR: 2.37e-04 | GradNorm: 0.42 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.61] | layer_5[5.4-7.7% std=0.63] | layer_11[5.1-8.0% std=0.91] | layer_17[4.2-8.4% std=0.99] | layer_22[1.7-10.4% std=1.84]
  Expert: layer_1[6.0-6.9% std=0.20]
  Expert: layer_2[6.0-6.7% std=0.21]
  Expert: layer_3[5.1-11.2% std=1.43]
  Expert: layer_4[4.7-7.8% std=0.86]
  Expert: layer_6[5.1-9.2% std=0.99]
  Expert: layer_7[2.5-12.3% std=1.97]
  Expert: layer_8[3.6-10.9% std=1.61]
  Expert: layer_9[5.1-7.7% std=0.70]
  Expert: layer_10[4.6-8.4% std=0.97]
  Expert: layer_12[4.3-8.0% std=1.03]
  Expert: layer_13[4.9-8.0% std=0.90]
  Expert: layer_14[4.8-7.7% std=0.75]
  Expert: layer_15[1.5-8.2% std=1.57]
  Expert: layer_16[4.5-7.9% std=0.96]
  Expert: layer_18[3.8-8.1% std=1.01]
  Expert: layer_19[4.2-8.2% std=1.23]
  Expert: layer_20[4.1-8.2% std=1.19]
  Expert: layer_21[3.1-8.4% std=1.27]
Step     380 | Loss: 3.6454 | LR: 2.37e-04 | GradNorm: 0.45 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.58] | layer_5[5.3-7.3% std=0.54] | layer_11[4.7-8.0% std=0.97] | layer_17[3.9-8.7% std=1.17] | layer_22[1.1-11.4% std=2.14]
  Expert: layer_1[6.0-6.8% std=0.18]
  Expert: layer_2[6.0-6.6% std=0.19]
  Expert: layer_3[5.1-11.2% std=1.43]
  Expert: layer_4[4.6-8.2% std=0.94]
  Expert: layer_6[5.1-9.2% std=0.97]
  Expert: layer_7[2.3-12.3% std=2.15]
  Expert: layer_8[3.4-11.5% std=1.75]
  Expert: layer_9[5.0-8.0% std=0.82]
  Expert: layer_10[4.6-8.4% std=0.98]
  Expert: layer_12[3.9-8.7% std=1.22]
  Expert: layer_13[4.5-7.8% std=0.91]
  Expert: layer_14[4.4-7.6% std=0.77]
  Expert: layer_15[1.8-9.2% std=1.81]
  Expert: layer_16[5.4-8.4% std=0.75]
  Expert: layer_18[3.4-8.5% std=1.25]
  Expert: layer_19[4.4-8.5% std=1.34]
  Expert: layer_20[4.4-7.7% std=1.05]
  Expert: layer_21[3.1-7.9% std=1.13]
Step     381 | Loss: 3.6071 | LR: 2.38e-04 | GradNorm: 0.51 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.61] | layer_5[5.4-7.1% std=0.46] | layer_11[4.4-8.4% std=1.04] | layer_17[3.7-10.9% std=1.50] | layer_22[1.6-10.6% std=1.86]
  Expert: layer_1[6.0-6.9% std=0.20]
  Expert: layer_2[6.0-6.5% std=0.20]
  Expert: layer_3[5.0-11.2% std=1.44]
  Expert: layer_4[4.5-8.2% std=0.99]
  Expert: layer_6[5.2-9.2% std=0.94]
  Expert: layer_7[2.3-12.8% std=2.14]
  Expert: layer_8[3.8-10.8% std=1.54]
  Expert: layer_9[5.1-7.8% std=0.79]
  Expert: layer_10[4.5-8.5% std=0.99]
  Expert: layer_12[4.7-8.3% std=0.88]
  Expert: layer_13[4.7-8.3% std=1.08]
  Expert: layer_14[4.8-7.7% std=0.94]
  Expert: layer_15[1.4-8.7% std=1.58]
  Expert: layer_16[4.1-8.7% std=1.18]
  Expert: layer_18[4.3-9.0% std=1.23]
  Expert: layer_19[4.0-8.9% std=1.40]
  Expert: layer_20[4.0-8.6% std=1.35]
  Expert: layer_21[4.1-7.9% std=1.02]
Step     382 | Loss: 3.6392 | LR: 2.39e-04 | GradNorm: 0.52 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.59] | layer_5[5.6-7.0% std=0.46] | layer_11[4.8-7.5% std=0.86] | layer_17[4.4-8.0% std=0.98] | layer_22[0.9-10.7% std=1.88]
  Expert: layer_1[5.9-7.0% std=0.22]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[4.7-11.4% std=1.54]
  Expert: layer_4[4.4-8.2% std=1.01]
  Expert: layer_6[5.1-9.1% std=0.93]
  Expert: layer_7[2.3-12.0% std=2.07]
  Expert: layer_8[3.6-10.9% std=1.65]
  Expert: layer_9[5.0-8.0% std=0.88]
  Expert: layer_10[4.7-7.7% std=0.98]
  Expert: layer_12[3.9-9.2% std=1.25]
  Expert: layer_13[4.5-8.0% std=1.00]
  Expert: layer_14[4.6-7.6% std=0.78]
  Expert: layer_15[2.1-9.7% std=1.55]
  Expert: layer_16[5.0-8.7% std=0.90]
  Expert: layer_18[3.9-8.4% std=1.10]
  Expert: layer_19[4.2-9.4% std=1.37]
  Expert: layer_20[4.6-8.8% std=1.15]
  Expert: layer_21[2.6-8.1% std=1.36]
Step     383 | Loss: 3.6538 | LR: 2.39e-04 | GradNorm: 0.59 | Tok/s: 2676 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.60] | layer_5[5.4-7.3% std=0.54] | layer_11[4.5-8.6% std=0.95] | layer_17[4.0-9.4% std=1.40] | layer_22[0.9-10.3% std=2.15]
  Expert: layer_1[6.0-7.0% std=0.24]
  Expert: layer_2[5.9-6.6% std=0.18]
  Expert: layer_3[4.5-11.3% std=1.53]
  Expert: layer_4[4.4-8.4% std=0.98]
  Expert: layer_6[5.1-9.2% std=0.96]
  Expert: layer_7[2.6-10.6% std=1.74]
  Expert: layer_8[4.2-10.9% std=1.57]
  Expert: layer_9[5.2-7.9% std=0.81]
  Expert: layer_10[4.7-7.6% std=0.83]
  Expert: layer_12[4.6-8.8% std=1.16]
  Expert: layer_13[4.8-8.1% std=1.05]
  Expert: layer_14[4.4-9.1% std=1.21]
  Expert: layer_15[1.7-8.6% std=1.55]
  Expert: layer_16[4.8-8.0% std=1.04]
  Expert: layer_18[4.0-8.8% std=1.42]
  Expert: layer_19[4.0-9.0% std=1.40]
  Expert: layer_20[4.2-7.9% std=1.25]
  Expert: layer_21[2.6-8.6% std=1.41]
Step     384 | Loss: 3.5520 | LR: 2.40e-04 | GradNorm: 0.54 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.63] | layer_5[5.5-6.9% std=0.47] | layer_11[4.8-7.4% std=0.70] | layer_17[4.5-8.0% std=0.97] | layer_22[1.4-10.2% std=1.75]
  Expert: layer_1[5.9-7.0% std=0.24]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[4.6-11.2% std=1.46]
  Expert: layer_4[4.3-8.5% std=1.04]
  Expert: layer_6[5.1-8.9% std=0.90]
  Expert: layer_7[2.6-11.3% std=1.84]
  Expert: layer_8[3.6-9.7% std=1.44]
  Expert: layer_9[4.8-7.9% std=0.95]
  Expert: layer_10[4.8-7.4% std=0.82]
  Expert: layer_12[4.0-8.6% std=1.05]
  Expert: layer_13[4.8-7.7% std=1.02]
  Expert: layer_14[4.4-8.7% std=1.01]
  Expert: layer_15[1.3-8.8% std=1.59]
  Expert: layer_16[5.0-7.7% std=0.78]
  Expert: layer_18[4.3-8.1% std=1.16]
  Expert: layer_19[4.4-9.4% std=1.25]
  Expert: layer_20[4.7-7.5% std=0.89]
  Expert: layer_21[3.0-8.1% std=1.25]
Step     385 | Loss: 3.5644 | LR: 2.41e-04 | GradNorm: 0.46 | Tok/s: 2674 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.61] | layer_5[5.6-7.0% std=0.42] | layer_11[4.4-8.6% std=1.05] | layer_17[3.4-9.6% std=1.49] | layer_22[1.2-11.5% std=2.12]
  Expert: layer_1[6.0-6.8% std=0.21]
  Expert: layer_2[5.9-6.7% std=0.20]
  Expert: layer_3[4.7-11.2% std=1.42]
  Expert: layer_4[4.4-8.7% std=1.09]
  Expert: layer_6[5.1-8.9% std=0.93]
  Expert: layer_7[2.9-11.0% std=1.81]
  Expert: layer_8[3.4-9.5% std=1.50]
  Expert: layer_9[4.9-8.2% std=1.00]
  Expert: layer_10[4.5-7.5% std=0.81]
  Expert: layer_12[4.3-9.8% std=1.29]
  Expert: layer_13[4.6-9.0% std=1.16]
  Expert: layer_14[4.2-9.0% std=1.08]
  Expert: layer_15[1.4-9.4% std=1.70]
  Expert: layer_16[4.6-8.4% std=1.18]
  Expert: layer_18[4.4-8.1% std=1.10]
  Expert: layer_19[4.6-9.4% std=1.24]
  Expert: layer_20[3.2-7.9% std=1.31]
  Expert: layer_21[3.4-10.4% std=1.53]
Step     386 | Loss: 3.6105 | LR: 2.41e-04 | GradNorm: 0.54 | Tok/s: 2644 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.62] | layer_5[5.5-7.0% std=0.44] | layer_11[4.7-8.2% std=0.95] | layer_17[4.4-7.7% std=0.95] | layer_22[0.8-9.8% std=1.96]
  Expert: layer_1[5.9-6.9% std=0.23]
  Expert: layer_2[5.8-6.5% std=0.19]
  Expert: layer_3[4.7-11.0% std=1.37]
  Expert: layer_4[4.6-8.3% std=0.99]
  Expert: layer_6[5.2-8.7% std=0.86]
  Expert: layer_7[2.4-10.6% std=1.79]
  Expert: layer_8[3.8-9.6% std=1.32]
  Expert: layer_9[5.1-7.7% std=0.78]
  Expert: layer_10[4.8-7.9% std=0.84]
  Expert: layer_12[4.1-8.4% std=1.03]
  Expert: layer_13[4.7-8.5% std=1.29]
  Expert: layer_14[4.6-8.8% std=1.15]
  Expert: layer_15[1.5-8.4% std=1.53]
  Expert: layer_16[4.7-9.6% std=1.26]
  Expert: layer_18[4.0-9.0% std=1.35]
  Expert: layer_19[4.0-8.9% std=1.54]
  Expert: layer_20[4.1-8.3% std=1.18]
  Expert: layer_21[2.6-8.2% std=1.36]
Step     387 | Loss: 3.6157 | LR: 2.42e-04 | GradNorm: 0.51 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.9% std=0.63] | layer_5[5.5-6.9% std=0.43] | layer_11[4.8-9.2% std=1.01] | layer_17[3.3-8.3% std=1.17] | layer_22[1.2-12.3% std=2.24]
  Expert: layer_1[5.8-6.9% std=0.25]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[4.8-11.0% std=1.36]
  Expert: layer_4[4.9-8.4% std=1.03]
  Expert: layer_6[5.2-8.6% std=0.89]
  Expert: layer_7[2.6-11.1% std=1.86]
  Expert: layer_8[3.7-9.2% std=1.43]
  Expert: layer_9[5.1-7.9% std=0.89]
  Expert: layer_10[4.6-7.4% std=0.81]
  Expert: layer_12[4.6-8.5% std=1.05]
  Expert: layer_13[4.4-8.2% std=1.10]
  Expert: layer_14[4.4-7.8% std=0.89]
  Expert: layer_15[1.8-9.9% std=1.62]
  Expert: layer_16[5.1-8.0% std=0.82]
  Expert: layer_18[4.4-8.3% std=1.22]
  Expert: layer_19[4.2-8.5% std=1.15]
  Expert: layer_20[4.2-9.8% std=1.52]
  Expert: layer_21[2.9-9.7% std=1.52]
Step     388 | Loss: 3.6116 | LR: 2.43e-04 | GradNorm: 0.50 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.60] | layer_5[5.5-7.2% std=0.47] | layer_11[4.6-7.7% std=0.93] | layer_17[4.5-8.1% std=1.08] | layer_22[1.2-10.8% std=1.95]
  Expert: layer_1[5.8-6.8% std=0.22]
  Expert: layer_2[5.9-6.6% std=0.21]
  Expert: layer_3[4.8-11.0% std=1.38]
  Expert: layer_4[5.0-8.3% std=1.00]
  Expert: layer_6[5.1-8.5% std=0.94]
  Expert: layer_7[2.6-11.0% std=1.88]
  Expert: layer_8[3.8-9.0% std=1.26]
  Expert: layer_9[5.0-7.6% std=0.89]
  Expert: layer_10[4.5-7.7% std=0.86]
  Expert: layer_12[4.5-8.0% std=1.06]
  Expert: layer_13[4.4-8.2% std=1.13]
  Expert: layer_14[4.3-8.0% std=1.14]
  Expert: layer_15[2.0-9.1% std=1.61]
  Expert: layer_16[5.1-8.5% std=0.88]
  Expert: layer_18[3.6-8.2% std=1.25]
  Expert: layer_19[4.6-8.9% std=1.19]
  Expert: layer_20[3.6-8.3% std=1.25]
  Expert: layer_21[3.6-9.5% std=1.36]
Step     389 | Loss: 3.6462 | LR: 2.43e-04 | GradNorm: 0.46 | Tok/s: 2704 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.60] | layer_5[5.4-6.9% std=0.48] | layer_11[4.1-8.6% std=1.22] | layer_17[4.1-8.2% std=1.15] | layer_22[1.2-10.3% std=1.92]
  Expert: layer_1[5.8-6.8% std=0.23]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[4.8-11.0% std=1.38]
  Expert: layer_4[5.1-8.4% std=0.97]
  Expert: layer_6[4.9-8.6% std=0.96]
  Expert: layer_7[2.8-10.7% std=1.86]
  Expert: layer_8[3.6-8.9% std=1.27]
  Expert: layer_9[5.2-7.7% std=0.80]
  Expert: layer_10[4.4-7.3% std=0.82]
  Expert: layer_12[4.7-8.3% std=0.97]
  Expert: layer_13[4.6-8.4% std=1.20]
  Expert: layer_14[4.3-8.3% std=1.10]
  Expert: layer_15[1.8-8.4% std=1.53]
  Expert: layer_16[5.3-8.2% std=0.89]
  Expert: layer_18[4.3-8.5% std=1.26]
  Expert: layer_19[4.2-8.4% std=1.33]
  Expert: layer_20[3.8-8.1% std=1.16]
  Expert: layer_21[3.7-9.6% std=1.35]
Step     390 | Loss: 3.6114 | LR: 2.44e-04 | GradNorm: 0.46 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.60] | layer_5[5.4-7.5% std=0.54] | layer_11[4.6-8.9% std=1.04] | layer_17[3.4-9.2% std=1.44] | layer_22[1.0-10.4% std=2.27]
  Expert: layer_1[5.8-6.7% std=0.22]
  Expert: layer_2[5.9-6.6% std=0.19]
  Expert: layer_3[4.9-10.6% std=1.29]
  Expert: layer_4[5.0-8.7% std=1.01]
  Expert: layer_6[4.9-8.8% std=1.04]
  Expert: layer_7[2.9-10.7% std=1.84]
  Expert: layer_8[3.5-8.9% std=1.32]
  Expert: layer_9[4.7-7.7% std=0.85]
  Expert: layer_10[4.4-7.6% std=0.83]
  Expert: layer_12[4.8-8.7% std=1.20]
  Expert: layer_13[4.2-8.0% std=1.16]
  Expert: layer_14[4.0-8.5% std=1.13]
  Expert: layer_15[2.1-9.2% std=1.74]
  Expert: layer_16[4.6-9.8% std=1.23]
  Expert: layer_18[4.3-9.8% std=1.40]
  Expert: layer_19[4.0-8.6% std=1.28]
  Expert: layer_20[3.8-8.1% std=1.20]
  Expert: layer_21[3.4-9.4% std=1.37]
Step     391 | Loss: 3.6198 | LR: 2.44e-04 | GradNorm: 0.46 | Tok/s: 2642 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.9% std=0.60] | layer_5[5.2-7.2% std=0.49] | layer_11[4.6-9.1% std=1.10] | layer_17[3.4-7.6% std=1.20] | layer_22[1.1-9.9% std=1.89]
  Expert: layer_1[5.7-6.8% std=0.23]
  Expert: layer_2[5.8-6.6% std=0.22]
  Expert: layer_3[4.8-10.8% std=1.36]
  Expert: layer_4[5.0-8.7% std=1.00]
  Expert: layer_6[5.0-8.4% std=0.94]
  Expert: layer_7[2.9-10.9% std=1.89]
  Expert: layer_8[3.9-8.8% std=1.20]
  Expert: layer_9[4.8-7.5% std=0.79]
  Expert: layer_10[4.5-8.0% std=0.93]
  Expert: layer_12[4.7-8.2% std=1.05]
  Expert: layer_13[4.3-7.9% std=1.08]
  Expert: layer_14[4.3-8.3% std=1.00]
  Expert: layer_15[1.9-8.0% std=1.34]
  Expert: layer_16[4.6-8.2% std=0.92]
  Expert: layer_18[3.7-8.6% std=1.39]
  Expert: layer_19[4.4-8.1% std=1.16]
  Expert: layer_20[3.7-7.7% std=1.22]
  Expert: layer_21[3.3-8.3% std=1.17]
Step     392 | Loss: 3.5663 | LR: 2.45e-04 | GradNorm: 0.44 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.60] | layer_5[5.2-7.2% std=0.56] | layer_11[4.5-8.6% std=1.13] | layer_17[4.1-9.8% std=1.33] | layer_22[1.2-9.9% std=1.92]
  Expert: layer_1[5.6-6.8% std=0.25]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[5.0-10.9% std=1.40]
  Expert: layer_4[5.0-8.9% std=0.99]
  Expert: layer_6[4.7-8.5% std=0.99]
  Expert: layer_7[2.9-10.7% std=1.75]
  Expert: layer_8[3.7-8.8% std=1.27]
  Expert: layer_9[4.5-7.7% std=0.90]
  Expert: layer_10[4.8-7.8% std=0.81]
  Expert: layer_12[4.5-8.7% std=1.16]
  Expert: layer_13[4.2-8.6% std=1.30]
  Expert: layer_14[4.1-8.1% std=1.06]
  Expert: layer_15[1.9-9.9% std=1.71]
  Expert: layer_16[3.9-9.2% std=1.33]
  Expert: layer_18[4.0-9.6% std=1.49]
  Expert: layer_19[4.7-9.8% std=1.21]
  Expert: layer_20[3.5-9.0% std=1.31]
  Expert: layer_21[3.0-10.7% std=1.50]
Step     393 | Loss: 3.5056 | LR: 2.46e-04 | GradNorm: 0.43 | Tok/s: 2674 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.58] | layer_5[5.2-7.3% std=0.57] | layer_11[4.0-8.8% std=1.22] | layer_17[3.6-8.9% std=1.39] | layer_22[1.2-11.1% std=2.16]
  Expert: layer_1[5.7-6.8% std=0.23]
  Expert: layer_2[5.9-6.6% std=0.21]
  Expert: layer_3[5.0-10.9% std=1.39]
  Expert: layer_4[4.8-8.7% std=0.98]
  Expert: layer_6[5.0-8.6% std=0.94]
  Expert: layer_7[2.9-10.8% std=1.80]
  Expert: layer_8[3.7-9.0% std=1.27]
  Expert: layer_9[4.8-7.7% std=0.84]
  Expert: layer_10[4.6-7.6% std=0.87]
  Expert: layer_12[4.6-8.1% std=1.08]
  Expert: layer_13[4.3-8.1% std=1.19]
  Expert: layer_14[4.1-9.5% std=1.23]
  Expert: layer_15[1.6-7.9% std=1.40]
  Expert: layer_16[4.6-9.9% std=1.22]
  Expert: layer_18[3.8-8.6% std=1.33]
  Expert: layer_19[4.5-8.7% std=1.24]
  Expert: layer_20[3.9-8.7% std=1.26]
  Expert: layer_21[3.1-8.7% std=1.24]
Step     394 | Loss: 3.5256 | LR: 2.46e-04 | GradNorm: 0.39 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.6% std=0.59] | layer_5[5.2-7.3% std=0.62] | layer_11[4.2-9.8% std=1.39] | layer_17[3.8-8.8% std=1.32] | layer_22[1.9-10.7% std=1.90]
  Expert: layer_1[5.6-6.7% std=0.23]
  Expert: layer_2[6.0-6.7% std=0.24]
  Expert: layer_3[5.0-10.9% std=1.44]
  Expert: layer_4[5.0-8.8% std=0.99]
  Expert: layer_6[4.6-8.5% std=0.98]
  Expert: layer_7[3.3-10.0% std=1.60]
  Expert: layer_8[4.0-9.1% std=1.33]
  Expert: layer_9[4.7-7.9% std=0.96]
  Expert: layer_10[4.5-8.3% std=0.91]
  Expert: layer_12[4.8-9.0% std=1.03]
  Expert: layer_13[4.4-8.7% std=1.35]
  Expert: layer_14[4.1-9.0% std=1.13]
  Expert: layer_15[1.7-10.4% std=1.76]
  Expert: layer_16[4.4-7.9% std=1.02]
  Expert: layer_18[4.2-8.9% std=1.49]
  Expert: layer_19[4.3-8.9% std=1.24]
  Expert: layer_20[3.5-8.0% std=1.34]
  Expert: layer_21[3.6-9.1% std=1.24]
Step     395 | Loss: 3.5294 | LR: 2.47e-04 | GradNorm: 0.39 | Tok/s: 2674 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.7% std=0.58] | layer_5[5.4-7.2% std=0.53] | layer_11[3.9-9.2% std=1.38] | layer_17[3.6-9.6% std=1.37] | layer_22[1.3-11.4% std=2.06]
  Expert: layer_1[5.7-6.8% std=0.25]
  Expert: layer_2[6.0-6.7% std=0.22]
  Expert: layer_3[5.0-10.7% std=1.42]
  Expert: layer_4[4.9-8.6% std=0.93]
  Expert: layer_6[4.6-8.4% std=0.99]
  Expert: layer_7[3.3-10.6% std=1.67]
  Expert: layer_8[4.2-8.8% std=1.19]
  Expert: layer_9[4.8-8.1% std=0.97]
  Expert: layer_10[4.3-8.9% std=1.06]
  Expert: layer_12[4.9-8.5% std=1.01]
  Expert: layer_13[4.3-8.9% std=1.34]
  Expert: layer_14[3.8-8.2% std=1.07]
  Expert: layer_15[1.7-9.6% std=1.68]
  Expert: layer_16[4.1-8.9% std=1.12]
  Expert: layer_18[4.4-8.1% std=1.11]
  Expert: layer_19[4.3-9.5% std=1.34]
  Expert: layer_20[3.6-8.9% std=1.36]
  Expert: layer_21[3.2-9.8% std=1.45]
Step     396 | Loss: 3.5536 | LR: 2.47e-04 | GradNorm: 0.44 | Tok/s: 2644 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.7% std=0.57] | layer_5[5.4-7.2% std=0.51] | layer_11[4.4-8.1% std=1.22] | layer_17[4.0-8.9% std=1.37] | layer_22[1.2-10.4% std=1.99]
  Expert: layer_1[5.7-6.7% std=0.25]
  Expert: layer_2[5.9-6.6% std=0.23]
  Expert: layer_3[5.0-10.6% std=1.33]
  Expert: layer_4[4.8-8.6% std=0.92]
  Expert: layer_6[4.6-8.6% std=0.98]
  Expert: layer_7[3.1-10.8% std=1.81]
  Expert: layer_8[3.8-9.4% std=1.35]
  Expert: layer_9[4.5-8.0% std=1.05]
  Expert: layer_10[4.8-8.0% std=0.84]
  Expert: layer_12[4.2-8.0% std=1.03]
  Expert: layer_13[4.4-8.3% std=1.20]
  Expert: layer_14[4.5-8.5% std=1.01]
  Expert: layer_15[1.6-9.4% std=1.69]
  Expert: layer_16[5.0-10.1% std=1.17]
  Expert: layer_18[3.8-8.7% std=1.40]
  Expert: layer_19[4.3-8.4% std=1.41]
  Expert: layer_20[4.2-8.3% std=1.30]
  Expert: layer_21[3.4-8.2% std=1.31]
Step     397 | Loss: 3.5682 | LR: 2.48e-04 | GradNorm: 0.39 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.51] | layer_5[5.3-7.4% std=0.56] | layer_11[4.2-9.1% std=1.46] | layer_17[3.5-9.2% std=1.39] | layer_22[1.6-10.1% std=1.95]
  Expert: layer_1[5.6-6.8% std=0.26]
  Expert: layer_2[5.9-6.8% std=0.23]
  Expert: layer_3[5.0-10.5% std=1.29]
  Expert: layer_4[4.9-8.5% std=0.91]
  Expert: layer_6[4.7-8.7% std=0.98]
  Expert: layer_7[2.9-10.8% std=1.79]
  Expert: layer_8[4.3-9.0% std=1.27]
  Expert: layer_9[4.5-7.9% std=1.01]
  Expert: layer_10[4.5-8.4% std=0.98]
  Expert: layer_12[5.1-9.1% std=0.96]
  Expert: layer_13[4.5-8.7% std=1.32]
  Expert: layer_14[4.1-8.3% std=1.01]
  Expert: layer_15[1.6-9.3% std=1.66]
  Expert: layer_16[4.1-9.2% std=1.18]
  Expert: layer_18[4.0-8.3% std=1.23]
  Expert: layer_19[4.1-9.0% std=1.21]
  Expert: layer_20[3.3-7.8% std=1.33]
  Expert: layer_21[3.5-9.7% std=1.51]
Step     398 | Loss: 3.5296 | LR: 2.49e-04 | GradNorm: 0.38 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.55] | layer_5[5.1-7.1% std=0.56] | layer_11[4.5-8.5% std=1.34] | layer_17[3.9-9.2% std=1.40] | layer_22[1.2-11.9% std=2.27]
  Expert: layer_1[5.7-6.7% std=0.26]
  Expert: layer_2[5.9-6.6% std=0.25]
  Expert: layer_3[5.0-10.4% std=1.25]
  Expert: layer_4[4.8-8.4% std=0.90]
  Expert: layer_6[4.7-8.7% std=0.94]
  Expert: layer_7[2.7-11.1% std=1.87]
  Expert: layer_8[4.1-9.2% std=1.34]
  Expert: layer_9[4.2-8.1% std=1.11]
  Expert: layer_10[4.7-8.3% std=0.94]
  Expert: layer_12[4.3-8.8% std=1.21]
  Expert: layer_13[4.4-8.4% std=1.29]
  Expert: layer_14[3.7-8.4% std=1.07]
  Expert: layer_15[2.0-10.8% std=1.84]
  Expert: layer_16[4.5-8.0% std=0.97]
  Expert: layer_18[3.3-8.6% std=1.38]
  Expert: layer_19[3.8-10.2% std=1.64]
  Expert: layer_20[4.1-8.9% std=1.31]
  Expert: layer_21[3.4-10.1% std=1.50]
Step     399 | Loss: 3.6214 | LR: 2.49e-04 | GradNorm: 0.41 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.6% std=0.54] | layer_5[5.0-7.0% std=0.60] | layer_11[4.1-8.6% std=1.45] | layer_17[3.5-9.5% std=1.43] | layer_22[1.2-10.9% std=1.95]
  Expert: layer_1[5.8-6.8% std=0.26]
  Expert: layer_2[5.9-6.8% std=0.28]
  Expert: layer_3[4.6-11.0% std=1.42]
  Expert: layer_4[5.0-8.1% std=0.88]
  Expert: layer_6[4.7-8.6% std=0.96]
  Expert: layer_7[2.6-11.7% std=2.07]
  Expert: layer_8[4.0-9.4% std=1.36]
  Expert: layer_9[4.0-8.2% std=1.18]
  Expert: layer_10[4.7-8.1% std=0.95]
  Expert: layer_12[4.7-8.2% std=1.02]
  Expert: layer_13[4.1-8.4% std=1.22]
  Expert: layer_14[4.3-7.8% std=0.88]
  Expert: layer_15[2.1-8.4% std=1.40]
  Expert: layer_16[4.6-8.5% std=1.03]
  Expert: layer_18[3.8-8.2% std=1.30]
  Expert: layer_19[4.1-8.9% std=1.50]
  Expert: layer_20[3.9-9.4% std=1.58]
  Expert: layer_21[3.7-9.5% std=1.34]
Step     400 | Loss: 3.4765 | LR: 2.50e-04 | GradNorm: 0.40 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.52] | layer_5[4.8-7.0% std=0.59] | layer_11[4.2-9.4% std=1.43] | layer_17[3.7-9.6% std=1.40] | layer_22[1.4-10.3% std=1.91]
  Expert: layer_1[5.7-6.7% std=0.26]
  Expert: layer_2[5.8-6.9% std=0.32]
  Expert: layer_3[4.6-10.9% std=1.41]
  Expert: layer_4[5.0-8.0% std=0.86]
  Expert: layer_6[4.7-8.6% std=0.93]
  Expert: layer_7[2.8-11.3% std=1.88]
  Expert: layer_8[4.2-9.4% std=1.34]
  Expert: layer_9[4.2-8.0% std=1.17]
  Expert: layer_10[4.8-8.7% std=0.95]
  Expert: layer_12[4.6-9.0% std=1.04]
  Expert: layer_13[4.2-9.2% std=1.50]
  Expert: layer_14[4.2-8.0% std=0.94]
  Expert: layer_15[1.9-9.6% std=1.75]
  Expert: layer_16[4.3-8.4% std=1.08]
  Expert: layer_18[4.0-8.1% std=1.30]
  Expert: layer_19[4.1-8.9% std=1.44]
  Expert: layer_20[4.3-8.5% std=1.36]
  Expert: layer_21[3.3-8.3% std=1.34]
Step     401 | Loss: 3.5299 | LR: 2.50e-04 | GradNorm: 0.43 | Tok/s: 2639 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.54] | layer_5[4.8-7.0% std=0.61] | layer_11[4.1-8.9% std=1.30] | layer_17[3.6-9.5% std=1.32] | layer_22[1.4-10.5% std=1.98]
  Expert: layer_1[5.6-6.8% std=0.27]
  Expert: layer_2[5.8-6.9% std=0.35]
  Expert: layer_3[4.9-10.9% std=1.38]
  Expert: layer_4[5.1-8.0% std=0.86]
  Expert: layer_6[4.8-8.6% std=0.91]
  Expert: layer_7[3.1-11.4% std=1.88]
  Expert: layer_8[4.5-9.2% std=1.29]
  Expert: layer_9[4.3-7.9% std=1.17]
  Expert: layer_10[4.6-8.4% std=0.93]
  Expert: layer_12[4.6-8.5% std=1.00]
  Expert: layer_13[4.0-8.9% std=1.49]
  Expert: layer_14[3.8-7.9% std=1.01]
  Expert: layer_15[1.8-8.9% std=1.69]
  Expert: layer_16[4.3-8.7% std=1.12]
  Expert: layer_18[3.8-9.0% std=1.30]
  Expert: layer_19[3.9-10.2% std=1.58]
  Expert: layer_20[4.7-8.9% std=1.26]
  Expert: layer_21[3.3-9.0% std=1.51]
Step     402 | Loss: 3.5294 | LR: 2.50e-04 | GradNorm: 0.44 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.53] | layer_5[4.7-7.1% std=0.64] | layer_11[4.4-8.5% std=1.25] | layer_17[3.9-9.8% std=1.46] | layer_22[1.5-10.5% std=2.03]
  Expert: layer_1[5.7-6.7% std=0.27]
  Expert: layer_2[5.8-6.9% std=0.35]
  Expert: layer_3[4.9-11.1% std=1.42]
  Expert: layer_4[5.2-8.1% std=0.86]
  Expert: layer_6[4.6-8.5% std=0.94]
  Expert: layer_7[3.2-10.9% std=1.89]
  Expert: layer_8[4.6-9.7% std=1.42]
  Expert: layer_9[4.1-7.9% std=1.20]
  Expert: layer_10[4.5-7.5% std=0.80]
  Expert: layer_12[4.4-8.4% std=1.13]
  Expert: layer_13[4.1-8.6% std=1.32]
  Expert: layer_14[4.0-8.4% std=0.91]
  Expert: layer_15[1.8-9.9% std=1.71]
  Expert: layer_16[5.1-8.5% std=0.97]
  Expert: layer_18[3.6-8.6% std=1.51]
  Expert: layer_19[3.7-9.7% std=1.75]
  Expert: layer_20[4.3-9.3% std=1.47]
  Expert: layer_21[3.1-9.0% std=1.43]
Step     403 | Loss: 3.5738 | LR: 2.50e-04 | GradNorm: 0.45 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.51] | layer_5[4.7-7.4% std=0.62] | layer_11[4.4-8.4% std=1.16] | layer_17[3.6-10.2% std=1.48] | layer_22[1.0-13.0% std=2.52]
  Expert: layer_1[5.7-6.6% std=0.27]
  Expert: layer_2[5.8-7.0% std=0.34]
  Expert: layer_3[4.8-11.0% std=1.42]
  Expert: layer_4[5.2-8.2% std=0.88]
  Expert: layer_6[4.6-8.6% std=0.97]
  Expert: layer_7[2.9-10.8% std=1.86]
  Expert: layer_8[4.6-9.5% std=1.30]
  Expert: layer_9[4.3-8.1% std=1.16]
  Expert: layer_10[4.1-7.8% std=0.93]
  Expert: layer_12[4.7-8.4% std=1.02]
  Expert: layer_13[4.2-8.9% std=1.28]
  Expert: layer_14[4.2-8.2% std=0.97]
  Expert: layer_15[1.7-8.3% std=1.66]
  Expert: layer_16[4.0-8.9% std=1.31]
  Expert: layer_18[3.1-8.6% std=1.32]
  Expert: layer_19[4.1-8.5% std=1.47]
  Expert: layer_20[3.9-8.0% std=1.42]
  Expert: layer_21[3.5-9.9% std=1.73]
Step     404 | Loss: 3.4972 | LR: 2.50e-04 | GradNorm: 0.45 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.54] | layer_5[4.6-7.4% std=0.63] | layer_11[4.2-8.2% std=1.22] | layer_17[4.3-9.3% std=1.24] | layer_22[1.3-11.4% std=2.21]
  Expert: layer_1[5.8-6.7% std=0.26]
  Expert: layer_2[5.8-6.9% std=0.35]
  Expert: layer_3[5.0-11.1% std=1.44]
  Expert: layer_4[5.0-8.0% std=0.83]
  Expert: layer_6[4.8-8.5% std=0.93]
  Expert: layer_7[2.9-10.3% std=1.76]
  Expert: layer_8[4.4-9.2% std=1.32]
  Expert: layer_9[4.0-8.3% std=1.28]
  Expert: layer_10[4.0-7.4% std=0.88]
  Expert: layer_12[4.3-8.1% std=1.23]
  Expert: layer_13[4.1-8.8% std=1.39]
  Expert: layer_14[3.8-7.5% std=0.97]
  Expert: layer_15[1.5-9.9% std=1.65]
  Expert: layer_16[4.7-8.9% std=1.17]
  Expert: layer_18[3.6-9.9% std=1.68]
  Expert: layer_19[4.0-10.3% std=1.71]
  Expert: layer_20[4.3-9.4% std=1.41]
  Expert: layer_21[2.9-8.1% std=1.41]
Step     405 | Loss: 3.5955 | LR: 2.50e-04 | GradNorm: 0.49 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.53] | layer_5[4.8-7.4% std=0.63] | layer_11[3.9-8.8% std=1.22] | layer_17[4.4-10.4% std=1.53] | layer_22[1.0-11.2% std=2.31]
  Expert: layer_1[5.8-6.7% std=0.26]
  Expert: layer_2[5.8-7.0% std=0.33]
  Expert: layer_3[4.9-11.1% std=1.44]
  Expert: layer_4[5.3-8.1% std=0.86]
  Expert: layer_6[4.8-8.6% std=0.95]
  Expert: layer_7[2.8-10.5% std=1.85]
  Expert: layer_8[4.2-9.8% std=1.34]
  Expert: layer_9[4.2-8.3% std=1.30]
  Expert: layer_10[4.2-7.5% std=0.95]
  Expert: layer_12[4.2-7.7% std=0.97]
  Expert: layer_13[4.5-8.8% std=1.33]
  Expert: layer_14[4.3-8.0% std=0.86]
  Expert: layer_15[1.4-9.2% std=1.85]
  Expert: layer_16[4.8-8.2% std=1.01]
  Expert: layer_18[3.2-10.3% std=1.80]
  Expert: layer_19[4.0-8.8% std=1.65]
  Expert: layer_20[3.7-8.2% std=1.28]
  Expert: layer_21[4.3-8.3% std=1.19]
Step     406 | Loss: 3.5097 | LR: 2.50e-04 | GradNorm: 0.51 | Tok/s: 2640 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.52] | layer_5[4.7-7.4% std=0.63] | layer_11[4.0-9.4% std=1.35] | layer_17[3.6-10.3% std=1.54] | layer_22[1.3-13.0% std=2.52]
  Expert: layer_1[5.8-6.7% std=0.26]
  Expert: layer_2[5.7-7.1% std=0.34]
  Expert: layer_3[4.8-11.0% std=1.45]
  Expert: layer_4[5.2-8.0% std=0.90]
  Expert: layer_6[4.9-8.7% std=1.00]
  Expert: layer_7[3.1-10.7% std=1.72]
  Expert: layer_8[4.2-8.9% std=1.22]
  Expert: layer_9[3.9-8.9% std=1.37]
  Expert: layer_10[3.9-7.6% std=1.06]
  Expert: layer_12[4.3-8.8% std=1.26]
  Expert: layer_13[4.3-8.9% std=1.30]
  Expert: layer_14[3.9-7.6% std=1.02]
  Expert: layer_15[1.5-9.2% std=1.66]
  Expert: layer_16[4.6-9.8% std=1.39]
  Expert: layer_18[3.3-8.7% std=1.47]
  Expert: layer_19[4.0-9.7% std=1.47]
  Expert: layer_20[3.6-9.9% std=1.67]
  Expert: layer_21[3.0-8.9% std=1.49]
Step     407 | Loss: 3.4932 | LR: 2.50e-04 | GradNorm: 0.44 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.53] | layer_5[5.0-7.3% std=0.62] | layer_11[3.8-9.9% std=1.36] | layer_17[3.7-10.1% std=1.64] | layer_22[1.2-11.7% std=2.33]
  Expert: layer_1[5.8-6.8% std=0.23]
  Expert: layer_2[5.8-7.0% std=0.33]
  Expert: layer_3[4.9-11.0% std=1.38]
  Expert: layer_4[5.3-7.8% std=0.87]
  Expert: layer_6[4.9-8.7% std=0.94]
  Expert: layer_7[3.2-10.4% std=1.74]
  Expert: layer_8[4.1-9.2% std=1.34]
  Expert: layer_9[4.2-8.4% std=1.30]
  Expert: layer_10[3.7-7.4% std=1.04]
  Expert: layer_12[4.1-9.2% std=1.20]
  Expert: layer_13[4.5-9.2% std=1.16]
  Expert: layer_14[4.0-8.9% std=1.15]
  Expert: layer_15[1.5-8.2% std=1.62]
  Expert: layer_16[4.4-9.4% std=1.26]
  Expert: layer_18[3.3-9.5% std=1.61]
  Expert: layer_19[3.8-8.2% std=1.48]
  Expert: layer_20[3.7-8.4% std=1.40]
  Expert: layer_21[3.9-9.1% std=1.45]
Step     408 | Loss: 3.5853 | LR: 2.50e-04 | GradNorm: 0.49 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.57] | layer_5[5.2-7.4% std=0.60] | layer_11[3.8-8.2% std=1.19] | layer_17[4.1-8.1% std=1.00] | layer_22[0.9-12.7% std=2.37]
  Expert: layer_1[5.7-6.8% std=0.24]
  Expert: layer_2[5.8-7.0% std=0.34]
  Expert: layer_3[4.7-10.9% std=1.43]
  Expert: layer_4[5.2-8.2% std=0.93]
  Expert: layer_6[4.6-8.4% std=0.90]
  Expert: layer_7[2.8-10.2% std=1.83]
  Expert: layer_8[4.1-9.1% std=1.18]
  Expert: layer_9[4.4-8.5% std=1.19]
  Expert: layer_10[4.2-7.9% std=1.01]
  Expert: layer_12[3.7-8.4% std=1.23]
  Expert: layer_13[4.7-8.5% std=1.04]
  Expert: layer_14[4.3-8.4% std=1.05]
  Expert: layer_15[2.0-9.1% std=1.61]
  Expert: layer_16[4.9-9.1% std=1.05]
  Expert: layer_18[3.4-10.1% std=1.71]
  Expert: layer_19[3.5-9.0% std=1.65]
  Expert: layer_20[4.1-9.0% std=1.43]
  Expert: layer_21[2.9-8.9% std=1.58]
Step     409 | Loss: 3.5043 | LR: 2.50e-04 | GradNorm: 0.52 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.54] | layer_5[5.0-7.7% std=0.64] | layer_11[4.3-10.3% std=1.51] | layer_17[3.9-11.2% std=1.82] | layer_22[1.1-14.0% std=2.66]
  Expert: layer_1[5.7-6.7% std=0.22]
  Expert: layer_2[5.9-6.8% std=0.30]
  Expert: layer_3[4.8-10.9% std=1.47]
  Expert: layer_4[5.0-7.8% std=0.90]
  Expert: layer_6[4.9-8.6% std=0.95]
  Expert: layer_7[3.4-10.0% std=1.66]
  Expert: layer_8[4.1-8.8% std=1.26]
  Expert: layer_9[4.1-8.6% std=1.25]
  Expert: layer_10[3.9-8.2% std=1.06]
  Expert: layer_12[4.3-8.9% std=1.37]
  Expert: layer_13[4.2-9.3% std=1.19]
  Expert: layer_14[4.0-8.6% std=1.18]
  Expert: layer_15[1.9-8.4% std=1.47]
  Expert: layer_16[4.4-10.1% std=1.45]
  Expert: layer_18[3.0-9.1% std=1.71]
  Expert: layer_19[4.0-8.5% std=1.34]
  Expert: layer_20[3.3-10.2% std=1.78]
  Expert: layer_21[3.3-9.3% std=1.58]
Step     410 | Loss: 3.5104 | LR: 2.50e-04 | GradNorm: 0.54 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.7% std=0.59] | layer_5[5.1-7.3% std=0.62] | layer_11[4.0-9.7% std=1.44] | layer_17[3.3-9.8% std=1.41] | layer_22[1.1-14.1% std=2.63]
  Expert: layer_1[5.8-6.7% std=0.21]
  Expert: layer_2[5.8-6.7% std=0.27]
  Expert: layer_3[4.7-10.9% std=1.48]
  Expert: layer_4[4.8-7.9% std=1.02]
  Expert: layer_6[4.9-8.5% std=0.98]
  Expert: layer_7[3.2-9.5% std=1.56]
  Expert: layer_8[4.1-8.7% std=1.35]
  Expert: layer_9[4.4-8.5% std=1.12]
  Expert: layer_10[4.2-7.8% std=1.01]
  Expert: layer_12[3.9-8.9% std=1.25]
  Expert: layer_13[4.7-8.8% std=1.16]
  Expert: layer_14[3.9-8.5% std=1.14]
  Expert: layer_15[2.2-8.5% std=1.48]
  Expert: layer_16[4.5-8.3% std=1.10]
  Expert: layer_18[3.3-9.2% std=1.77]
  Expert: layer_19[3.4-10.1% std=1.73]
  Expert: layer_20[4.1-9.7% std=1.70]
  Expert: layer_21[2.8-8.9% std=1.64]
Step     411 | Loss: 3.4980 | LR: 2.50e-04 | GradNorm: 0.55 | Tok/s: 2641 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.58] | layer_5[5.0-7.4% std=0.63] | layer_11[4.5-9.2% std=1.21] | layer_17[4.3-9.4% std=1.40] | layer_22[1.2-11.4% std=2.24]
  Expert: layer_1[5.8-6.9% std=0.26]
  Expert: layer_2[5.9-6.8% std=0.27]
  Expert: layer_3[4.7-10.8% std=1.49]
  Expert: layer_4[4.7-8.1% std=1.09]
  Expert: layer_6[5.0-8.3% std=0.87]
  Expert: layer_7[3.0-9.7% std=1.71]
  Expert: layer_8[4.2-8.5% std=1.20]
  Expert: layer_9[4.7-8.8% std=1.13]
  Expert: layer_10[4.1-8.1% std=1.09]
  Expert: layer_12[3.8-8.6% std=1.34]
  Expert: layer_13[4.8-8.9% std=1.17]
  Expert: layer_14[3.9-7.9% std=1.28]
  Expert: layer_15[1.6-9.1% std=1.71]
  Expert: layer_16[4.7-11.5% std=1.65]
  Expert: layer_18[3.3-8.4% std=1.36]
  Expert: layer_19[4.3-9.3% std=1.55]
  Expert: layer_20[3.7-9.4% std=1.64]
  Expert: layer_21[3.3-10.0% std=1.66]
Step     412 | Loss: 3.5212 | LR: 2.50e-04 | GradNorm: 0.49 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.58] | layer_5[5.1-7.5% std=0.63] | layer_11[4.4-10.0% std=1.43] | layer_17[4.1-9.7% std=1.50] | layer_22[1.2-14.6% std=2.70]
  Expert: layer_1[5.8-6.8% std=0.22]
  Expert: layer_2[5.8-6.7% std=0.27]
  Expert: layer_3[4.8-10.9% std=1.53]
  Expert: layer_4[4.8-8.2% std=1.06]
  Expert: layer_6[4.9-8.4% std=0.92]
  Expert: layer_7[3.7-8.9% std=1.45]
  Expert: layer_8[4.2-9.0% std=1.41]
  Expert: layer_9[4.4-8.4% std=1.26]
  Expert: layer_10[3.7-7.8% std=1.13]
  Expert: layer_12[4.1-9.7% std=1.48]
  Expert: layer_13[4.5-8.7% std=1.09]
  Expert: layer_14[4.1-8.2% std=1.18]
  Expert: layer_15[1.5-8.5% std=1.56]
  Expert: layer_16[4.2-10.2% std=1.39]
  Expert: layer_18[3.8-9.3% std=1.50]
  Expert: layer_19[4.0-10.0% std=1.42]
  Expert: layer_20[3.2-8.3% std=1.54]
  Expert: layer_21[3.0-10.0% std=1.83]
Step     413 | Loss: 3.4902 | LR: 2.50e-04 | GradNorm: 0.50 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.57] | layer_5[5.2-7.3% std=0.61] | layer_11[4.1-10.1% std=1.41] | layer_17[4.2-10.3% std=1.35] | layer_22[1.1-14.2% std=2.67]
  Expert: layer_1[5.9-6.8% std=0.20]
  Expert: layer_2[5.8-6.6% std=0.24]
  Expert: layer_3[4.6-10.8% std=1.54]
  Expert: layer_4[4.8-8.3% std=1.11]
  Expert: layer_6[4.8-8.5% std=0.98]
  Expert: layer_7[3.3-9.0% std=1.48]
  Expert: layer_8[4.0-8.7% std=1.27]
  Expert: layer_9[4.4-8.3% std=1.19]
  Expert: layer_10[4.0-7.9% std=1.05]
  Expert: layer_12[3.7-8.6% std=1.26]
  Expert: layer_13[4.9-9.0% std=1.15]
  Expert: layer_14[4.1-8.0% std=1.13]
  Expert: layer_15[1.5-9.9% std=1.84]
  Expert: layer_16[4.4-8.8% std=1.10]
  Expert: layer_18[3.8-10.2% std=1.84]
  Expert: layer_19[3.7-8.4% std=1.42]
  Expert: layer_20[4.0-9.8% std=1.73]
  Expert: layer_21[3.3-9.4% std=1.75]
Step     414 | Loss: 3.5397 | LR: 2.50e-04 | GradNorm: 0.50 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.8% std=0.59] | layer_5[5.2-7.3% std=0.60] | layer_11[4.2-9.4% std=1.23] | layer_17[3.2-11.0% std=1.58] | layer_22[1.1-15.4% std=2.85]
  Expert: layer_1[5.9-6.7% std=0.20]
  Expert: layer_2[5.9-6.7% std=0.25]
  Expert: layer_3[4.6-10.7% std=1.58]
  Expert: layer_4[4.7-8.2% std=1.11]
  Expert: layer_6[4.8-8.4% std=0.92]
  Expert: layer_7[3.0-9.6% std=1.62]
  Expert: layer_8[4.0-8.2% std=1.17]
  Expert: layer_9[4.6-8.5% std=1.13]
  Expert: layer_10[3.9-7.9% std=1.12]
  Expert: layer_12[3.8-8.2% std=1.30]
  Expert: layer_13[4.6-8.6% std=1.15]
  Expert: layer_14[3.8-8.8% std=1.21]
  Expert: layer_15[1.7-8.7% std=1.62]
  Expert: layer_16[4.8-9.2% std=1.16]
  Expert: layer_18[2.9-8.5% std=1.50]
  Expert: layer_19[4.3-8.5% std=1.30]
  Expert: layer_20[3.8-9.3% std=1.58]
  Expert: layer_21[3.3-9.9% std=1.88]
Step     415 | Loss: 3.5102 | LR: 2.50e-04 | GradNorm: 0.52 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.59] | layer_5[5.1-7.4% std=0.68] | layer_11[4.1-9.2% std=1.22] | layer_17[3.9-9.8% std=1.49] | layer_22[1.0-11.4% std=2.31]
  Expert: layer_1[5.8-6.8% std=0.21]
  Expert: layer_2[5.9-6.7% std=0.25]
  Expert: layer_3[4.7-10.7% std=1.51]
  Expert: layer_4[4.4-8.6% std=1.12]
  Expert: layer_6[4.9-8.5% std=0.93]
  Expert: layer_7[3.1-8.7% std=1.48]
  Expert: layer_8[4.4-8.2% std=1.21]
  Expert: layer_9[4.2-8.6% std=1.12]
  Expert: layer_10[4.2-7.8% std=0.96]
  Expert: layer_12[3.8-8.4% std=1.10]
  Expert: layer_13[4.7-9.0% std=1.09]
  Expert: layer_14[3.9-8.0% std=1.10]
  Expert: layer_15[1.8-10.2% std=1.88]
  Expert: layer_16[4.6-9.8% std=1.29]
  Expert: layer_18[3.5-10.5% std=1.86]
  Expert: layer_19[4.1-9.2% std=1.29]
  Expert: layer_20[3.7-8.6% std=1.55]
  Expert: layer_21[3.5-12.1% std=1.95]
Step     416 | Loss: 3.4934 | LR: 2.50e-04 | GradNorm: 0.47 | Tok/s: 2638 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.58] | layer_5[5.0-7.3% std=0.65] | layer_11[3.7-9.0% std=1.33] | layer_17[3.7-9.8% std=1.31] | layer_22[1.0-13.2% std=2.48]
  Expert: layer_1[5.9-6.7% std=0.20]
  Expert: layer_2[5.9-6.7% std=0.25]
  Expert: layer_3[4.5-10.2% std=1.46]
  Expert: layer_4[4.5-8.7% std=1.10]
  Expert: layer_6[4.8-8.4% std=0.95]
  Expert: layer_7[3.0-9.3% std=1.59]
  Expert: layer_8[4.5-7.9% std=1.11]
  Expert: layer_9[4.0-8.5% std=1.08]
  Expert: layer_10[3.9-8.0% std=1.09]
  Expert: layer_12[3.7-8.5% std=1.23]
  Expert: layer_13[4.6-8.7% std=1.22]
  Expert: layer_14[4.0-8.0% std=1.23]
  Expert: layer_15[1.0-8.7% std=1.75]
  Expert: layer_16[4.2-10.0% std=1.38]
  Expert: layer_18[3.7-9.2% std=1.50]
  Expert: layer_19[3.6-9.8% std=1.70]
  Expert: layer_20[3.9-9.7% std=1.57]
  Expert: layer_21[3.5-9.4% std=1.59]
Step     417 | Loss: 3.4933 | LR: 2.50e-04 | GradNorm: 0.43 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.56] | layer_5[5.2-7.4% std=0.65] | layer_11[4.2-9.2% std=1.28] | layer_17[3.2-10.2% std=1.47] | layer_22[0.8-16.3% std=3.12]
  Expert: layer_1[5.9-6.6% std=0.18]
  Expert: layer_2[5.9-7.0% std=0.30]
  Expert: layer_3[4.6-10.1% std=1.45]
  Expert: layer_4[4.5-9.0% std=1.14]
  Expert: layer_6[4.6-8.6% std=1.01]
  Expert: layer_7[3.1-9.3% std=1.53]
  Expert: layer_8[3.9-8.8% std=1.23]
  Expert: layer_9[4.3-8.2% std=1.17]
  Expert: layer_10[4.1-8.1% std=1.09]
  Expert: layer_12[3.3-9.0% std=1.46]
  Expert: layer_13[4.4-9.3% std=1.21]
  Expert: layer_14[3.9-7.6% std=1.04]
  Expert: layer_15[1.4-9.7% std=1.91]
  Expert: layer_16[4.6-8.8% std=1.16]
  Expert: layer_18[3.2-9.2% std=1.48]
  Expert: layer_19[3.4-9.4% std=1.63]
  Expert: layer_20[4.1-8.5% std=1.51]
  Expert: layer_21[3.7-10.0% std=1.79]
Step     418 | Loss: 3.4009 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.9% std=0.60] | layer_5[5.3-7.2% std=0.65] | layer_11[3.9-9.0% std=1.50] | layer_17[3.3-11.1% std=1.75] | layer_22[1.1-13.6% std=2.58]
  Expert: layer_1[5.8-6.6% std=0.20]
  Expert: layer_2[5.8-7.1% std=0.36]
  Expert: layer_3[4.6-10.1% std=1.42]
  Expert: layer_4[4.3-9.1% std=1.14]
  Expert: layer_6[4.9-8.5% std=0.97]
  Expert: layer_7[2.8-9.3% std=1.66]
  Expert: layer_8[3.8-8.6% std=1.24]
  Expert: layer_9[4.2-8.2% std=1.19]
  Expert: layer_10[4.2-7.8% std=1.05]
  Expert: layer_12[3.5-8.3% std=1.37]
  Expert: layer_13[4.5-9.6% std=1.36]
  Expert: layer_14[3.9-7.8% std=1.19]
  Expert: layer_15[1.4-10.2% std=1.85]
  Expert: layer_16[4.8-9.4% std=1.23]
  Expert: layer_18[3.1-9.0% std=1.79]
  Expert: layer_19[3.7-9.4% std=1.81]
  Expert: layer_20[3.6-9.0% std=1.63]
  Expert: layer_21[3.7-9.3% std=1.60]
Step     419 | Loss: 3.4772 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2674 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.9% std=0.57] | layer_5[5.3-7.2% std=0.60] | layer_11[3.8-8.6% std=1.44] | layer_17[3.8-9.9% std=1.37] | layer_22[1.2-12.9% std=2.30]
  Expert: layer_1[5.8-6.7% std=0.21]
  Expert: layer_2[5.9-7.1% std=0.29]
  Expert: layer_3[4.6-10.0% std=1.42]
  Expert: layer_4[4.4-8.7% std=1.09]
  Expert: layer_6[4.9-8.4% std=0.95]
  Expert: layer_7[2.9-9.6% std=1.67]
  Expert: layer_8[3.9-8.1% std=1.12]
  Expert: layer_9[4.3-8.5% std=1.08]
  Expert: layer_10[4.2-8.2% std=1.10]
  Expert: layer_12[3.8-8.2% std=1.19]
  Expert: layer_13[4.6-9.3% std=1.32]
  Expert: layer_14[4.1-8.2% std=1.23]
  Expert: layer_15[1.4-9.1% std=1.69]
  Expert: layer_16[4.7-9.2% std=1.17]
  Expert: layer_18[3.4-9.2% std=1.62]
  Expert: layer_19[3.9-9.7% std=1.72]
  Expert: layer_20[3.8-9.3% std=1.60]
  Expert: layer_21[3.5-9.5% std=1.62]
Step     420 | Loss: 3.5042 | LR: 2.50e-04 | GradNorm: 0.40 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.54] | layer_5[5.2-7.3% std=0.67] | layer_11[4.2-8.8% std=1.38] | layer_17[4.2-10.1% std=1.51] | layer_22[1.2-12.5% std=2.40]
  Expert: layer_1[5.9-6.5% std=0.18]
  Expert: layer_2[5.8-7.1% std=0.33]
  Expert: layer_3[4.6-9.9% std=1.37]
  Expert: layer_4[4.3-9.0% std=1.16]
  Expert: layer_6[4.8-8.6% std=1.01]
  Expert: layer_7[2.8-9.4% std=1.69]
  Expert: layer_8[3.8-8.7% std=1.23]
  Expert: layer_9[4.5-8.4% std=1.05]
  Expert: layer_10[4.2-8.3% std=1.08]
  Expert: layer_12[3.7-8.0% std=1.25]
  Expert: layer_13[4.5-9.3% std=1.30]
  Expert: layer_14[4.0-8.4% std=1.24]
  Expert: layer_15[1.5-10.4% std=2.02]
  Expert: layer_16[4.6-9.4% std=1.20]
  Expert: layer_18[3.6-9.0% std=1.58]
  Expert: layer_19[3.6-10.7% std=1.92]
  Expert: layer_20[3.7-9.6% std=1.57]
  Expert: layer_21[3.4-8.9% std=1.52]
Step     421 | Loss: 3.4594 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2644 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.55] | layer_5[5.2-7.6% std=0.70] | layer_11[3.8-8.8% std=1.35] | layer_17[4.2-8.9% std=1.34] | layer_22[1.2-12.4% std=2.44]
  Expert: layer_1[5.8-6.6% std=0.21]
  Expert: layer_2[5.8-7.1% std=0.34]
  Expert: layer_3[4.5-9.8% std=1.36]
  Expert: layer_4[4.2-9.2% std=1.25]
  Expert: layer_6[5.0-8.3% std=0.93]
  Expert: layer_7[2.5-9.2% std=1.68]
  Expert: layer_8[3.9-8.8% std=1.14]
  Expert: layer_9[4.4-8.2% std=0.97]
  Expert: layer_10[4.4-8.0% std=1.01]
  Expert: layer_12[3.5-8.5% std=1.33]
  Expert: layer_13[4.7-8.4% std=1.19]
  Expert: layer_14[4.3-7.6% std=1.05]
  Expert: layer_15[1.6-9.6% std=1.86]
  Expert: layer_16[4.7-9.6% std=1.15]
  Expert: layer_18[3.4-9.3% std=1.71]
  Expert: layer_19[3.5-11.1% std=1.79]
  Expert: layer_20[3.5-9.6% std=1.67]
  Expert: layer_21[3.7-8.7% std=1.57]
Step     422 | Loss: 3.4126 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2674 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.57] | layer_5[5.1-7.9% std=0.71] | layer_11[3.7-8.6% std=1.43] | layer_17[4.4-10.1% std=1.36] | layer_22[1.3-12.9% std=2.47]
  Expert: layer_1[5.8-6.7% std=0.24]
  Expert: layer_2[5.7-7.1% std=0.32]
  Expert: layer_3[4.6-10.0% std=1.40]
  Expert: layer_4[4.3-9.4% std=1.26]
  Expert: layer_6[5.0-8.0% std=0.89]
  Expert: layer_7[2.6-9.1% std=1.64]
  Expert: layer_8[3.7-8.8% std=1.19]
  Expert: layer_9[4.4-8.0% std=1.01]
  Expert: layer_10[4.6-7.9% std=0.97]
  Expert: layer_12[3.4-8.6% std=1.45]
  Expert: layer_13[4.5-8.9% std=1.29]
  Expert: layer_14[4.0-8.2% std=1.16]
  Expert: layer_15[1.6-10.2% std=1.89]
  Expert: layer_16[4.7-10.2% std=1.37]
  Expert: layer_18[3.5-9.8% std=1.64]
  Expert: layer_19[3.7-11.0% std=1.82]
  Expert: layer_20[2.8-9.3% std=1.76]
  Expert: layer_21[3.5-9.3% std=1.56]
Step     423 | Loss: 3.3941 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.55] | layer_5[5.1-8.3% std=0.77] | layer_11[3.8-9.4% std=1.53] | layer_17[3.8-10.5% std=1.45] | layer_22[1.1-14.5% std=2.83]
  Expert: layer_1[5.9-6.6% std=0.23]
  Expert: layer_2[5.8-7.0% std=0.30]
  Expert: layer_3[4.3-10.1% std=1.46]
  Expert: layer_4[4.4-9.5% std=1.31]
  Expert: layer_6[4.7-8.4% std=0.94]
  Expert: layer_7[2.8-9.0% std=1.59]
  Expert: layer_8[3.5-9.0% std=1.28]
  Expert: layer_9[4.5-8.1% std=1.10]
  Expert: layer_10[4.6-8.3% std=1.00]
  Expert: layer_12[3.8-8.8% std=1.29]
  Expert: layer_13[4.4-9.3% std=1.43]
  Expert: layer_14[4.0-9.5% std=1.23]
  Expert: layer_15[1.7-10.1% std=1.99]
  Expert: layer_16[4.3-9.1% std=1.35]
  Expert: layer_18[3.8-9.0% std=1.50]
  Expert: layer_19[3.7-10.8% std=1.77]
  Expert: layer_20[3.3-9.6% std=1.71]
  Expert: layer_21[3.2-10.0% std=1.81]
Step     424 | Loss: 3.4312 | LR: 2.50e-04 | GradNorm: 0.42 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.58] | layer_5[5.2-8.3% std=0.79] | layer_11[3.8-9.0% std=1.38] | layer_17[3.9-9.3% std=1.46] | layer_22[1.2-14.5% std=2.89]
  Expert: layer_1[5.8-6.7% std=0.27]
  Expert: layer_2[5.7-7.1% std=0.32]
  Expert: layer_3[4.3-10.2% std=1.47]
  Expert: layer_4[4.3-9.4% std=1.29]
  Expert: layer_6[4.9-8.2% std=0.85]
  Expert: layer_7[2.7-9.3% std=1.76]
  Expert: layer_8[3.6-9.1% std=1.36]
  Expert: layer_9[4.2-8.3% std=1.08]
  Expert: layer_10[4.6-7.7% std=0.94]
  Expert: layer_12[3.6-8.3% std=1.42]
  Expert: layer_13[4.5-9.0% std=1.36]
  Expert: layer_14[4.0-9.0% std=1.32]
  Expert: layer_15[1.7-9.2% std=1.74]
  Expert: layer_16[4.6-10.1% std=1.38]
  Expert: layer_18[3.2-9.5% std=1.81]
  Expert: layer_19[3.5-11.1% std=1.91]
  Expert: layer_20[3.7-10.1% std=1.90]
  Expert: layer_21[2.7-9.8% std=1.90]
Step     425 | Loss: 3.4584 | LR: 2.50e-04 | GradNorm: 0.42 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.55] | layer_5[5.0-8.9% std=0.89] | layer_11[3.9-10.0% std=1.54] | layer_17[4.4-11.0% std=1.70] | layer_22[1.3-12.0% std=2.47]
  Expert: layer_1[5.9-6.7% std=0.26]
  Expert: layer_2[5.7-6.8% std=0.26]
  Expert: layer_3[4.4-10.5% std=1.56]
  Expert: layer_4[4.5-9.2% std=1.26]
  Expert: layer_6[4.7-8.1% std=0.92]
  Expert: layer_7[2.8-9.3% std=1.79]
  Expert: layer_8[3.4-8.6% std=1.37]
  Expert: layer_9[4.8-8.3% std=1.10]
  Expert: layer_10[4.6-7.9% std=0.99]
  Expert: layer_12[3.9-9.2% std=1.38]
  Expert: layer_13[4.3-9.4% std=1.42]
  Expert: layer_14[4.4-8.7% std=1.23]
  Expert: layer_15[1.6-10.9% std=1.84]
  Expert: layer_16[4.4-9.5% std=1.28]
  Expert: layer_18[3.8-10.0% std=1.79]
  Expert: layer_19[4.1-10.1% std=1.67]
  Expert: layer_20[3.1-10.3% std=2.00]
  Expert: layer_21[3.3-10.5% std=1.82]
Step     426 | Loss: 3.4444 | LR: 2.50e-04 | GradNorm: 0.46 | Tok/s: 2641 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.8% std=0.55] | layer_5[5.3-9.1% std=0.91] | layer_11[4.1-9.3% std=1.23] | layer_17[4.0-10.3% std=1.40] | layer_22[0.8-15.1% std=2.99]
  Expert: layer_1[5.8-6.7% std=0.28]
  Expert: layer_2[5.8-6.7% std=0.23]
  Expert: layer_3[4.4-10.5% std=1.56]
  Expert: layer_4[4.4-9.2% std=1.31]
  Expert: layer_6[4.6-8.2% std=0.93]
  Expert: layer_7[2.7-9.5% std=1.78]
  Expert: layer_8[3.5-8.5% std=1.30]
  Expert: layer_9[4.9-8.4% std=1.04]
  Expert: layer_10[4.1-8.3% std=1.07]
  Expert: layer_12[3.3-8.8% std=1.54]
  Expert: layer_13[4.5-9.3% std=1.33]
  Expert: layer_14[3.8-8.8% std=1.28]
  Expert: layer_15[1.5-10.1% std=1.78]
  Expert: layer_16[4.1-10.0% std=1.38]
  Expert: layer_18[3.4-8.8% std=1.54]
  Expert: layer_19[3.5-11.5% std=1.93]
  Expert: layer_20[3.7-8.8% std=1.66]
  Expert: layer_21[2.8-10.5% std=2.05]
Step     427 | Loss: 3.4809 | LR: 2.50e-04 | GradNorm: 0.44 | Tok/s: 2675 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.53] | layer_5[5.2-8.8% std=0.91] | layer_11[3.1-10.2% std=1.63] | layer_17[4.1-10.5% std=1.40] | layer_22[0.8-13.5% std=2.68]
  Expert: layer_1[5.9-6.7% std=0.26]
  Expert: layer_2[5.6-6.8% std=0.28]
  Expert: layer_3[4.3-10.5% std=1.54]
  Expert: layer_4[4.3-9.3% std=1.36]
  Expert: layer_6[4.6-8.3% std=0.94]
  Expert: layer_7[2.7-9.0% std=1.85]
  Expert: layer_8[3.6-9.3% std=1.42]
  Expert: layer_9[4.7-7.9% std=1.01]
  Expert: layer_10[4.2-8.0% std=1.05]
  Expert: layer_12[3.8-8.8% std=1.37]
  Expert: layer_13[4.4-9.2% std=1.42]
  Expert: layer_14[3.9-8.6% std=1.20]
  Expert: layer_15[1.6-9.3% std=1.81]
  Expert: layer_16[4.1-9.9% std=1.42]
  Expert: layer_18[3.5-9.3% std=1.69]
  Expert: layer_19[4.0-11.2% std=1.82]
  Expert: layer_20[3.3-9.1% std=1.72]
  Expert: layer_21[3.0-9.4% std=1.78]
Step     428 | Loss: 3.4690 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.54] | layer_5[5.1-8.7% std=0.89] | layer_11[4.0-9.4% std=1.39] | layer_17[3.9-10.0% std=1.46] | layer_22[0.8-15.3% std=3.04]
  Expert: layer_1[5.9-6.7% std=0.26]
  Expert: layer_2[5.5-6.8% std=0.30]
  Expert: layer_3[4.3-10.8% std=1.61]
  Expert: layer_4[4.6-8.9% std=1.24]
  Expert: layer_6[4.8-8.1% std=0.92]
  Expert: layer_7[2.5-8.9% std=1.71]
  Expert: layer_8[3.4-9.3% std=1.50]
  Expert: layer_9[4.4-8.3% std=1.11]
  Expert: layer_10[4.4-8.0% std=0.94]
  Expert: layer_12[3.6-9.3% std=1.51]
  Expert: layer_13[4.0-8.6% std=1.28]
  Expert: layer_14[3.6-8.7% std=1.23]
  Expert: layer_15[1.8-10.6% std=1.80]
  Expert: layer_16[4.3-10.3% std=1.46]
  Expert: layer_18[3.3-9.9% std=1.95]
  Expert: layer_19[3.8-11.6% std=1.94]
  Expert: layer_20[3.5-10.2% std=1.91]
  Expert: layer_21[2.7-9.6% std=1.80]
Step     429 | Loss: 3.4188 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.54] | layer_5[5.0-8.5% std=0.86] | layer_11[3.8-9.8% std=1.44] | layer_17[4.3-10.0% std=1.32] | layer_22[0.9-16.7% std=3.27]
  Expert: layer_1[5.8-6.7% std=0.26]
  Expert: layer_2[5.6-6.7% std=0.28]
  Expert: layer_3[4.3-10.9% std=1.65]
  Expert: layer_4[4.8-8.9% std=1.20]
  Expert: layer_6[4.7-8.1% std=0.89]
  Expert: layer_7[2.6-8.8% std=1.74]
  Expert: layer_8[3.8-9.3% std=1.40]
  Expert: layer_9[4.4-8.7% std=1.19]
  Expert: layer_10[4.2-7.8% std=0.95]
  Expert: layer_12[3.6-9.1% std=1.58]
  Expert: layer_13[4.0-9.3% std=1.37]
  Expert: layer_14[4.0-9.3% std=1.24]
  Expert: layer_15[1.6-9.8% std=1.67]
  Expert: layer_16[3.9-9.8% std=1.54]
  Expert: layer_18[3.3-9.9% std=1.67]
  Expert: layer_19[4.1-11.9% std=1.96]
  Expert: layer_20[3.2-10.1% std=1.92]
  Expert: layer_21[2.8-11.1% std=2.11]
Step     430 | Loss: 3.4615 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.6% std=0.49] | layer_5[5.0-8.3% std=0.84] | layer_11[3.4-9.8% std=1.47] | layer_17[4.0-9.4% std=1.30] | layer_22[0.7-16.5% std=3.31]
  Expert: layer_1[5.7-6.6% std=0.26]
  Expert: layer_2[5.6-6.7% std=0.28]
  Expert: layer_3[4.1-10.8% std=1.64]
  Expert: layer_4[4.6-8.7% std=1.15]
  Expert: layer_6[4.6-7.9% std=0.87]
  Expert: layer_7[2.4-8.7% std=1.82]
  Expert: layer_8[3.6-9.5% std=1.46]
  Expert: layer_9[4.5-8.7% std=1.16]
  Expert: layer_10[4.3-7.8% std=0.94]
  Expert: layer_12[3.5-8.5% std=1.40]
  Expert: layer_13[4.0-8.6% std=1.27]
  Expert: layer_14[4.2-8.4% std=1.09]
  Expert: layer_15[1.7-9.9% std=1.67]
  Expert: layer_16[4.5-8.8% std=1.16]
  Expert: layer_18[3.0-9.7% std=1.87]
  Expert: layer_19[3.9-11.7% std=2.00]
  Expert: layer_20[3.6-11.3% std=1.99]
  Expert: layer_21[2.8-9.6% std=1.81]
Step     431 | Loss: 3.3818 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.51] | layer_5[5.0-8.2% std=0.83] | layer_11[3.5-9.1% std=1.53] | layer_17[4.0-8.4% std=1.20] | layer_22[0.8-16.0% std=3.14]
  Expert: layer_1[5.7-6.8% std=0.28]
  Expert: layer_2[5.6-6.7% std=0.26]
  Expert: layer_3[4.1-10.6% std=1.62]
  Expert: layer_4[4.6-8.6% std=1.14]
  Expert: layer_6[4.6-7.9% std=0.89]
  Expert: layer_7[2.6-8.8% std=1.80]
  Expert: layer_8[3.8-9.1% std=1.47]
  Expert: layer_9[4.2-8.8% std=1.24]
  Expert: layer_10[4.3-8.0% std=0.96]
  Expert: layer_12[3.2-8.9% std=1.53]
  Expert: layer_13[3.9-8.3% std=1.29]
  Expert: layer_14[4.0-8.1% std=1.19]
  Expert: layer_15[1.9-9.3% std=1.49]
  Expert: layer_16[4.5-9.5% std=1.43]
  Expert: layer_18[3.1-10.7% std=1.99]
  Expert: layer_19[3.8-11.1% std=1.80]
  Expert: layer_20[4.0-11.3% std=2.21]
  Expert: layer_21[2.9-10.4% std=1.97]
Step     432 | Loss: 3.4265 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[5.0-8.0% std=0.79] | layer_11[3.4-9.3% std=1.58] | layer_17[3.9-10.0% std=1.44] | layer_22[0.8-16.3% std=3.19]
  Expert: layer_1[5.9-6.7% std=0.24]
  Expert: layer_2[5.7-6.6% std=0.24]
  Expert: layer_3[4.1-10.6% std=1.68]
  Expert: layer_4[4.5-8.5% std=1.13]
  Expert: layer_6[4.3-7.8% std=0.92]
  Expert: layer_7[2.9-9.5% std=1.89]
  Expert: layer_8[4.0-8.4% std=1.36]
  Expert: layer_9[4.5-8.8% std=1.21]
  Expert: layer_10[4.0-7.8% std=1.03]
  Expert: layer_12[3.4-10.6% std=1.84]
  Expert: layer_13[4.0-8.5% std=1.36]
  Expert: layer_14[4.2-9.3% std=1.34]
  Expert: layer_15[1.6-8.1% std=1.53]
  Expert: layer_16[4.1-10.9% std=1.74]
  Expert: layer_18[3.3-9.1% std=1.60]
  Expert: layer_19[3.6-11.1% std=1.82]
  Expert: layer_20[3.3-9.4% std=2.03]
  Expert: layer_21[3.1-10.9% std=2.08]
Step     433 | Loss: 3.3939 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.8-7.9% std=0.87] | layer_11[3.9-9.4% std=1.57] | layer_17[4.0-9.1% std=1.43] | layer_22[0.8-15.0% std=3.02]
  Expert: layer_1[5.8-6.8% std=0.29]
  Expert: layer_2[5.6-6.8% std=0.29]
  Expert: layer_3[3.9-10.6% std=1.70]
  Expert: layer_4[4.4-8.4% std=1.11]
  Expert: layer_6[4.5-7.7% std=0.92]
  Expert: layer_7[2.5-9.4% std=1.87]
  Expert: layer_8[4.2-8.9% std=1.37]
  Expert: layer_9[4.4-9.2% std=1.28]
  Expert: layer_10[4.2-7.8% std=1.00]
  Expert: layer_12[3.3-9.8% std=1.61]
  Expert: layer_13[4.0-8.7% std=1.33]
  Expert: layer_14[3.9-8.9% std=1.31]
  Expert: layer_15[1.4-10.5% std=1.88]
  Expert: layer_16[4.4-9.7% std=1.53]
  Expert: layer_18[3.2-10.0% std=2.06]
  Expert: layer_19[4.1-11.5% std=1.81]
  Expert: layer_20[3.8-9.5% std=1.87]
  Expert: layer_21[2.5-10.6% std=2.00]
Step     434 | Loss: 3.3526 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2674 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.52] | layer_5[4.7-7.9% std=0.87] | layer_11[4.0-9.7% std=1.64] | layer_17[3.7-9.9% std=1.53] | layer_22[1.2-15.9% std=3.13]
  Expert: layer_1[5.8-6.8% std=0.26]
  Expert: layer_2[5.6-6.6% std=0.26]
  Expert: layer_3[4.1-10.6% std=1.73]
  Expert: layer_4[4.5-8.6% std=1.21]
  Expert: layer_6[4.4-7.6% std=0.96]
  Expert: layer_7[2.7-9.3% std=1.88]
  Expert: layer_8[4.0-9.4% std=1.45]
  Expert: layer_9[4.3-9.1% std=1.38]
  Expert: layer_10[4.2-7.7% std=0.95]
  Expert: layer_12[3.5-9.8% std=1.67]
  Expert: layer_13[4.1-8.2% std=1.27]
  Expert: layer_14[3.7-8.7% std=1.34]
  Expert: layer_15[1.4-9.0% std=1.70]
  Expert: layer_16[4.5-10.5% std=1.57]
  Expert: layer_18[3.2-10.8% std=1.97]
  Expert: layer_19[3.7-11.5% std=1.88]
  Expert: layer_20[3.4-9.6% std=2.06]
  Expert: layer_21[2.1-10.7% std=2.15]
Step     435 | Loss: 3.4415 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.4% std=0.49] | layer_5[4.8-7.7% std=0.83] | layer_11[4.0-10.1% std=1.67] | layer_17[3.6-10.1% std=1.51] | layer_22[1.0-16.9% std=3.30]
  Expert: layer_1[5.8-6.6% std=0.21]
  Expert: layer_2[5.6-6.7% std=0.26]
  Expert: layer_3[4.2-10.8% std=1.82]
  Expert: layer_4[4.4-8.6% std=1.25]
  Expert: layer_6[4.2-7.7% std=1.01]
  Expert: layer_7[3.0-9.4% std=1.78]
  Expert: layer_8[3.9-8.5% std=1.39]
  Expert: layer_9[4.1-9.5% std=1.51]
  Expert: layer_10[4.1-7.5% std=0.96]
  Expert: layer_12[3.2-10.0% std=1.76]
  Expert: layer_13[4.0-9.1% std=1.40]
  Expert: layer_14[3.8-9.8% std=1.51]
  Expert: layer_15[1.3-8.7% std=1.78]
  Expert: layer_16[3.9-10.3% std=1.61]
  Expert: layer_18[3.4-10.0% std=1.68]
  Expert: layer_19[3.8-11.4% std=1.83]
  Expert: layer_20[2.8-10.5% std=2.12]
  Expert: layer_21[2.5-11.0% std=2.32]
Step     436 | Loss: 3.3729 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2640 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.53] | layer_5[4.6-7.5% std=0.87] | layer_11[3.3-9.9% std=1.74] | layer_17[4.7-8.6% std=1.12] | layer_22[1.1-16.9% std=3.22]
  Expert: layer_1[5.9-6.7% std=0.22]
  Expert: layer_2[5.7-6.6% std=0.24]
  Expert: layer_3[4.1-10.8% std=1.83]
  Expert: layer_4[4.5-9.1% std=1.26]
  Expert: layer_6[4.6-7.5% std=0.95]
  Expert: layer_7[3.0-9.6% std=1.89]
  Expert: layer_8[3.9-8.6% std=1.44]
  Expert: layer_9[4.1-9.6% std=1.43]
  Expert: layer_10[4.4-7.9% std=0.98]
  Expert: layer_12[3.5-9.4% std=1.66]
  Expert: layer_13[4.2-9.0% std=1.37]
  Expert: layer_14[4.2-9.8% std=1.47]
  Expert: layer_15[1.0-10.7% std=1.94]
  Expert: layer_16[4.5-11.3% std=1.68]
  Expert: layer_18[3.5-9.9% std=1.72]
  Expert: layer_19[3.7-11.6% std=1.95]
  Expert: layer_20[3.7-10.1% std=1.96]
  Expert: layer_21[2.5-9.7% std=1.85]
Step     437 | Loss: 3.3718 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.54] | layer_5[4.8-7.7% std=0.90] | layer_11[3.9-10.3% std=1.71] | layer_17[3.5-8.8% std=1.46] | layer_22[1.3-17.0% std=3.32]
  Expert: layer_1[6.0-6.7% std=0.19]
  Expert: layer_2[5.7-6.7% std=0.25]
  Expert: layer_3[4.1-10.6% std=1.77]
  Expert: layer_4[4.5-8.9% std=1.23]
  Expert: layer_6[4.5-7.6% std=0.99]
  Expert: layer_7[3.1-9.4% std=1.72]
  Expert: layer_8[3.8-8.6% std=1.49]
  Expert: layer_9[3.9-9.3% std=1.51]
  Expert: layer_10[4.1-7.6% std=1.03]
  Expert: layer_12[3.6-11.0% std=1.87]
  Expert: layer_13[4.4-9.2% std=1.38]
  Expert: layer_14[3.8-9.3% std=1.44]
  Expert: layer_15[1.3-10.6% std=1.88]
  Expert: layer_16[4.1-10.0% std=1.71]
  Expert: layer_18[3.1-10.4% std=2.06]
  Expert: layer_19[3.7-12.1% std=1.95]
  Expert: layer_20[3.7-10.9% std=2.12]
  Expert: layer_21[2.7-10.4% std=2.06]
Step     438 | Loss: 3.3626 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.54] | layer_5[4.8-7.6% std=0.92] | layer_11[3.5-10.0% std=1.84] | layer_17[4.2-9.7% std=1.34] | layer_22[1.2-15.6% std=3.01]
  Expert: layer_1[5.9-6.6% std=0.23]
  Expert: layer_2[5.6-6.7% std=0.30]
  Expert: layer_3[4.0-10.5% std=1.79]
  Expert: layer_4[4.4-8.9% std=1.21]
  Expert: layer_6[4.5-7.4% std=0.98]
  Expert: layer_7[3.3-9.2% std=1.69]
  Expert: layer_8[4.0-8.8% std=1.42]
  Expert: layer_9[3.6-9.8% std=1.57]
  Expert: layer_10[4.2-7.6% std=1.01]
  Expert: layer_12[3.3-9.6% std=1.79]
  Expert: layer_13[4.0-8.6% std=1.42]
  Expert: layer_14[3.9-9.3% std=1.48]
  Expert: layer_15[1.1-12.1% std=2.15]
  Expert: layer_16[4.5-9.9% std=1.57]
  Expert: layer_18[3.1-10.7% std=1.98]
  Expert: layer_19[3.4-11.9% std=2.12]
  Expert: layer_20[3.5-11.0% std=2.01]
  Expert: layer_21[2.4-9.9% std=1.94]
Step     439 | Loss: 3.4414 | LR: 2.50e-04 | GradNorm: 0.39 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.52] | layer_5[4.8-7.6% std=0.86] | layer_11[3.3-11.0% std=1.92] | layer_17[3.3-11.6% std=1.85] | layer_22[1.0-17.2% std=3.35]
  Expert: layer_1[5.9-6.6% std=0.21]
  Expert: layer_2[5.7-6.6% std=0.24]
  Expert: layer_3[4.1-10.6% std=1.84]
  Expert: layer_4[4.7-9.1% std=1.21]
  Expert: layer_6[4.5-7.6% std=0.96]
  Expert: layer_7[3.4-9.2% std=1.77]
  Expert: layer_8[3.7-8.3% std=1.42]
  Expert: layer_9[4.0-9.9% std=1.50]
  Expert: layer_10[4.1-7.9% std=1.03]
  Expert: layer_12[4.3-9.8% std=1.48]
  Expert: layer_13[4.3-9.0% std=1.34]
  Expert: layer_14[4.0-9.7% std=1.64]
  Expert: layer_15[1.0-8.5% std=1.80]
  Expert: layer_16[4.1-10.4% std=1.68]
  Expert: layer_18[3.2-10.2% std=1.75]
  Expert: layer_19[3.6-10.9% std=1.99]
  Expert: layer_20[3.0-11.6% std=2.13]
  Expert: layer_21[3.0-10.7% std=1.99]
Step     440 | Loss: 3.4051 | LR: 2.50e-04 | GradNorm: 0.44 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.4% std=0.51] | layer_5[4.8-7.9% std=0.89] | layer_11[4.0-11.0% std=1.80] | layer_17[3.5-10.2% std=1.46] | layer_22[1.1-17.3% std=3.46]
  Expert: layer_1[5.9-6.6% std=0.23]
  Expert: layer_2[5.6-6.7% std=0.27]
  Expert: layer_3[4.1-10.4% std=1.80]
  Expert: layer_4[4.6-9.0% std=1.20]
  Expert: layer_6[4.4-7.8% std=0.97]
  Expert: layer_7[3.9-9.1% std=1.62]
  Expert: layer_8[3.7-8.9% std=1.46]
  Expert: layer_9[3.6-10.4% std=1.59]
  Expert: layer_10[3.7-8.2% std=1.11]
  Expert: layer_12[3.6-11.1% std=1.97]
  Expert: layer_13[4.1-9.0% std=1.41]
  Expert: layer_14[3.9-9.5% std=1.56]
  Expert: layer_15[1.0-11.5% std=2.07]
  Expert: layer_16[4.0-9.6% std=1.47]
  Expert: layer_18[2.5-9.8% std=1.81]
  Expert: layer_19[3.7-11.4% std=1.99]
  Expert: layer_20[3.1-9.9% std=1.96]
  Expert: layer_21[2.5-10.0% std=2.03]
Step     441 | Loss: 3.3624 | LR: 2.50e-04 | GradNorm: 0.45 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.9-7.7% std=0.87] | layer_11[3.1-10.0% std=1.70] | layer_17[3.7-11.3% std=1.64] | layer_22[0.8-16.6% std=3.25]
  Expert: layer_1[5.9-6.6% std=0.23]
  Expert: layer_2[5.8-6.7% std=0.24]
  Expert: layer_3[4.1-10.2% std=1.77]
  Expert: layer_4[4.6-8.8% std=1.22]
  Expert: layer_6[4.6-7.6% std=0.93]
  Expert: layer_7[3.4-9.7% std=1.79]
  Expert: layer_8[3.8-8.5% std=1.40]
  Expert: layer_9[3.7-10.4% std=1.55]
  Expert: layer_10[4.1-8.2% std=1.04]
  Expert: layer_12[3.9-9.2% std=1.28]
  Expert: layer_13[4.6-8.5% std=1.21]
  Expert: layer_14[3.9-9.9% std=1.64]
  Expert: layer_15[1.0-8.6% std=1.88]
  Expert: layer_16[4.3-10.2% std=1.67]
  Expert: layer_18[2.4-10.3% std=2.02]
  Expert: layer_19[3.9-9.7% std=1.82]
  Expert: layer_20[3.9-9.5% std=1.94]
  Expert: layer_21[3.1-12.3% std=2.05]
Step     442 | Loss: 3.4044 | LR: 2.50e-04 | GradNorm: 0.53 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.4% std=0.50] | layer_5[4.8-7.7% std=0.87] | layer_11[3.6-10.5% std=1.92] | layer_17[3.6-12.1% std=1.88] | layer_22[1.1-18.4% std=3.65]
  Expert: layer_1[5.9-6.8% std=0.25]
  Expert: layer_2[5.8-6.7% std=0.27]
  Expert: layer_3[4.0-10.2% std=1.75]
  Expert: layer_4[4.8-9.1% std=1.23]
  Expert: layer_6[4.5-7.6% std=0.93]
  Expert: layer_7[3.7-9.0% std=1.69]
  Expert: layer_8[3.8-9.1% std=1.49]
  Expert: layer_9[3.8-10.2% std=1.52]
  Expert: layer_10[3.7-8.3% std=1.12]
  Expert: layer_12[3.8-10.9% std=1.93]
  Expert: layer_13[3.9-8.6% std=1.32]
  Expert: layer_14[3.6-9.1% std=1.51]
  Expert: layer_15[0.9-10.0% std=1.89]
  Expert: layer_16[3.9-10.3% std=1.78]
  Expert: layer_18[3.1-10.6% std=1.96]
  Expert: layer_19[3.6-13.0% std=2.17]
  Expert: layer_20[2.6-10.4% std=2.12]
  Expert: layer_21[2.7-10.7% std=2.09]
Step     443 | Loss: 3.3773 | LR: 2.50e-04 | GradNorm: 0.54 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.53] | layer_5[4.7-8.2% std=0.95] | layer_11[3.6-9.8% std=1.51] | layer_17[3.7-9.6% std=1.43] | layer_22[1.0-16.2% std=3.31]
  Expert: layer_1[5.9-6.7% std=0.26]
  Expert: layer_2[5.7-6.7% std=0.28]
  Expert: layer_3[3.9-9.9% std=1.67]
  Expert: layer_4[4.6-9.1% std=1.16]
  Expert: layer_6[4.8-7.7% std=0.93]
  Expert: layer_7[3.6-9.2% std=1.60]
  Expert: layer_8[4.3-8.6% std=1.30]
  Expert: layer_9[3.8-10.5% std=1.54]
  Expert: layer_10[3.8-8.3% std=1.12]
  Expert: layer_12[3.7-9.9% std=1.60]
  Expert: layer_13[4.3-9.3% std=1.48]
  Expert: layer_14[3.9-9.5% std=1.58]
  Expert: layer_15[1.0-10.7% std=2.08]
  Expert: layer_16[3.9-10.6% std=1.75]
  Expert: layer_18[2.5-10.8% std=2.24]
  Expert: layer_19[3.5-11.0% std=1.98]
  Expert: layer_20[3.2-10.8% std=2.22]
  Expert: layer_21[2.8-10.1% std=1.96]
Step     444 | Loss: 3.3756 | LR: 2.50e-04 | GradNorm: 0.48 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.50] | layer_5[4.9-8.2% std=0.93] | layer_11[3.6-10.6% std=1.77] | layer_17[3.0-10.7% std=1.83] | layer_22[1.2-19.2% std=3.84]
  Expert: layer_1[5.9-6.8% std=0.27]
  Expert: layer_2[5.6-6.8% std=0.31]
  Expert: layer_3[4.0-9.8% std=1.62]
  Expert: layer_4[4.6-9.4% std=1.20]
  Expert: layer_6[4.9-7.9% std=0.96]
  Expert: layer_7[3.9-9.2% std=1.52]
  Expert: layer_8[3.8-8.9% std=1.37]
  Expert: layer_9[3.8-9.6% std=1.44]
  Expert: layer_10[3.6-8.6% std=1.19]
  Expert: layer_12[4.0-9.8% std=1.60]
  Expert: layer_13[4.4-8.9% std=1.31]
  Expert: layer_14[3.7-9.7% std=1.56]
  Expert: layer_15[0.9-8.8% std=1.83]
  Expert: layer_16[3.9-10.6% std=1.82]
  Expert: layer_18[2.2-10.9% std=2.12]
  Expert: layer_19[3.5-10.9% std=1.87]
  Expert: layer_20[2.5-10.1% std=2.07]
  Expert: layer_21[3.3-10.8% std=2.03]
Step     445 | Loss: 3.3743 | LR: 2.50e-04 | GradNorm: 0.55 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.50] | layer_5[4.8-8.3% std=0.95] | layer_11[3.4-9.8% std=1.80] | layer_17[3.7-10.8% std=1.65] | layer_22[1.4-18.5% std=3.78]
  Expert: layer_1[5.9-6.8% std=0.25]
  Expert: layer_2[5.6-6.6% std=0.28]
  Expert: layer_3[4.0-9.8% std=1.62]
  Expert: layer_4[4.7-9.2% std=1.23]
  Expert: layer_6[4.8-7.8% std=0.96]
  Expert: layer_7[3.9-9.0% std=1.48]
  Expert: layer_8[3.7-9.0% std=1.44]
  Expert: layer_9[3.9-9.5% std=1.41]
  Expert: layer_10[3.9-8.3% std=1.18]
  Expert: layer_12[3.7-10.3% std=1.86]
  Expert: layer_13[4.2-9.0% std=1.39]
  Expert: layer_14[3.6-9.5% std=1.60]
  Expert: layer_15[0.9-11.9% std=2.22]
  Expert: layer_16[4.5-11.4% std=1.98]
  Expert: layer_18[3.0-12.5% std=2.42]
  Expert: layer_19[3.8-13.4% std=2.30]
  Expert: layer_20[2.8-11.3% std=2.39]
  Expert: layer_21[2.5-10.3% std=2.08]
Step     446 | Loss: 3.3471 | LR: 2.50e-04 | GradNorm: 0.44 | Tok/s: 2640 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.5% std=0.50] | layer_5[4.8-8.4% std=0.91] | layer_11[3.4-9.0% std=1.64] | layer_17[3.2-9.1% std=1.32] | layer_22[0.9-21.7% std=4.34]
  Expert: layer_1[5.8-6.9% std=0.28]
  Expert: layer_2[5.7-6.8% std=0.30]
  Expert: layer_3[4.0-9.1% std=1.50]
  Expert: layer_4[4.6-8.8% std=1.22]
  Expert: layer_6[4.8-8.0% std=0.90]
  Expert: layer_7[3.5-9.3% std=1.53]
  Expert: layer_8[4.0-8.8% std=1.25]
  Expert: layer_9[3.7-10.0% std=1.48]
  Expert: layer_10[3.6-8.5% std=1.23]
  Expert: layer_12[3.9-9.8% std=1.53]
  Expert: layer_13[4.2-8.7% std=1.32]
  Expert: layer_14[3.4-10.8% std=1.72]
  Expert: layer_15[0.7-9.0% std=1.91]
  Expert: layer_16[4.1-11.7% std=1.96]
  Expert: layer_18[2.4-11.2% std=1.99]
  Expert: layer_19[3.7-11.0% std=2.04]
  Expert: layer_20[3.4-10.2% std=2.05]
  Expert: layer_21[3.1-12.4% std=2.33]
Step     447 | Loss: 3.4090 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.6-7.6% std=0.51] | layer_5[4.8-8.6% std=0.90] | layer_11[3.9-9.4% std=1.56] | layer_17[3.1-9.9% std=1.40] | layer_22[0.7-18.8% std=3.70]
  Expert: layer_1[5.8-6.9% std=0.27]
  Expert: layer_2[5.7-6.7% std=0.27]
  Expert: layer_3[4.1-9.2% std=1.56]
  Expert: layer_4[4.7-9.2% std=1.26]
  Expert: layer_6[4.8-8.0% std=1.00]
  Expert: layer_7[3.7-9.3% std=1.50]
  Expert: layer_8[4.1-8.3% std=1.17]
  Expert: layer_9[3.8-10.5% std=1.54]
  Expert: layer_10[3.6-8.4% std=1.27]
  Expert: layer_12[4.0-9.4% std=1.48]
  Expert: layer_13[4.5-9.4% std=1.44]
  Expert: layer_14[3.4-10.1% std=1.69]
  Expert: layer_15[0.9-9.6% std=1.85]
  Expert: layer_16[4.1-10.5% std=1.81]
  Expert: layer_18[2.6-11.3% std=1.91]
  Expert: layer_19[3.8-11.7% std=1.99]
  Expert: layer_20[3.1-9.9% std=2.24]
  Expert: layer_21[2.8-11.9% std=2.30]
Step     448 | Loss: 3.4315 | LR: 2.50e-04 | GradNorm: 0.39 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.5% std=0.48] | layer_5[4.5-8.4% std=0.88] | layer_11[3.6-9.8% std=1.73] | layer_17[3.3-10.4% std=1.72] | layer_22[0.8-16.7% std=3.35]
  Expert: layer_1[5.8-6.8% std=0.25]
  Expert: layer_2[5.6-6.7% std=0.28]
  Expert: layer_3[4.0-9.4% std=1.57]
  Expert: layer_4[4.7-9.1% std=1.29]
  Expert: layer_6[4.8-8.2% std=1.05]
  Expert: layer_7[3.5-9.3% std=1.65]
  Expert: layer_8[4.0-9.3% std=1.37]
  Expert: layer_9[3.8-10.3% std=1.53]
  Expert: layer_10[3.9-8.2% std=1.21]
  Expert: layer_12[3.9-9.7% std=1.72]
  Expert: layer_13[4.6-9.4% std=1.41]
  Expert: layer_14[3.7-9.9% std=1.69]
  Expert: layer_15[0.9-10.5% std=2.02]
  Expert: layer_16[4.3-10.7% std=1.77]
  Expert: layer_18[2.6-11.3% std=2.14]
  Expert: layer_19[3.3-13.4% std=2.36]
  Expert: layer_20[3.0-10.8% std=2.41]
  Expert: layer_21[2.9-11.3% std=2.01]
Step     449 | Loss: 3.3921 | LR: 2.50e-04 | GradNorm: 0.44 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.5% std=0.48] | layer_5[4.3-8.4% std=0.93] | layer_11[3.5-9.5% std=1.72] | layer_17[3.2-10.5% std=1.58] | layer_22[0.8-18.2% std=3.54]
  Expert: layer_1[5.8-6.8% std=0.25]
  Expert: layer_2[5.7-6.6% std=0.26]
  Expert: layer_3[4.1-9.4% std=1.61]
  Expert: layer_4[4.7-9.3% std=1.33]
  Expert: layer_6[4.6-8.3% std=1.11]
  Expert: layer_7[3.7-8.8% std=1.61]
  Expert: layer_8[3.8-9.2% std=1.40]
  Expert: layer_9[3.8-10.8% std=1.61]
  Expert: layer_10[3.7-8.1% std=1.19]
  Expert: layer_12[4.2-9.3% std=1.56]
  Expert: layer_13[4.4-8.8% std=1.32]
  Expert: layer_14[3.5-10.3% std=1.58]
  Expert: layer_15[0.9-9.7% std=1.89]
  Expert: layer_16[4.2-11.3% std=1.92]
  Expert: layer_18[2.9-10.5% std=1.95]
  Expert: layer_19[3.6-14.0% std=2.32]
  Expert: layer_20[3.4-11.5% std=2.41]
  Expert: layer_21[2.7-11.1% std=2.00]
Step     450 | Loss: 3.3314 | LR: 2.50e-04 | GradNorm: 0.47 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.6% std=0.51] | layer_5[4.3-8.3% std=0.95] | layer_11[3.6-10.4% std=1.75] | layer_17[3.3-11.6% std=1.90] | layer_22[0.8-19.6% std=3.94]
  Expert: layer_1[5.8-6.8% std=0.24]
  Expert: layer_2[5.7-6.7% std=0.29]
  Expert: layer_3[4.2-9.4% std=1.58]
  Expert: layer_4[4.8-9.5% std=1.36]
  Expert: layer_6[4.7-8.3% std=1.04]
  Expert: layer_7[3.9-8.9% std=1.62]
  Expert: layer_8[3.6-10.2% std=1.53]
  Expert: layer_9[3.9-10.7% std=1.59]
  Expert: layer_10[3.5-8.0% std=1.23]
  Expert: layer_12[3.8-9.9% std=1.66]
  Expert: layer_13[4.1-9.8% std=1.53]
  Expert: layer_14[3.2-10.2% std=1.76]
  Expert: layer_15[0.9-11.0% std=2.13]
  Expert: layer_16[4.0-11.2% std=1.80]
  Expert: layer_18[2.5-10.3% std=2.04]
  Expert: layer_19[3.1-11.9% std=2.18]
  Expert: layer_20[2.9-10.7% std=2.32]
  Expert: layer_21[2.9-10.9% std=2.13]
Step     451 | Loss: 3.4003 | LR: 2.50e-04 | GradNorm: 0.44 | Tok/s: 2636 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.6% std=0.50] | layer_5[4.5-8.4% std=0.92] | layer_11[3.6-8.9% std=1.65] | layer_17[3.3-10.7% std=1.64] | layer_22[0.9-19.5% std=3.83]
  Expert: layer_1[5.9-6.9% std=0.23]
  Expert: layer_2[5.8-6.6% std=0.29]
  Expert: layer_3[4.3-9.6% std=1.62]
  Expert: layer_4[4.8-9.5% std=1.38]
  Expert: layer_6[4.7-8.2% std=1.06]
  Expert: layer_7[3.7-9.4% std=1.71]
  Expert: layer_8[3.6-9.9% std=1.50]
  Expert: layer_9[3.7-11.6% std=1.82]
  Expert: layer_10[3.5-8.0% std=1.17]
  Expert: layer_12[3.6-8.9% std=1.58]
  Expert: layer_13[4.4-9.3% std=1.32]
  Expert: layer_14[3.6-10.4% std=1.56]
  Expert: layer_15[1.0-10.4% std=1.99]
  Expert: layer_16[4.3-10.3% std=1.73]
  Expert: layer_18[2.8-11.5% std=2.08]
  Expert: layer_19[3.4-11.8% std=2.09]
  Expert: layer_20[3.2-11.5% std=2.49]
  Expert: layer_21[2.7-12.2% std=2.22]
Step     452 | Loss: 3.3807 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.6% std=0.50] | layer_5[4.5-8.5% std=0.94] | layer_11[3.5-9.7% std=1.69] | layer_17[3.2-11.0% std=1.73] | layer_22[0.8-20.4% std=4.11]
  Expert: layer_1[5.9-6.8% std=0.24]
  Expert: layer_2[5.7-6.8% std=0.30]
  Expert: layer_3[4.3-9.2% std=1.52]
  Expert: layer_4[4.8-9.3% std=1.37]
  Expert: layer_6[4.7-8.3% std=1.05]
  Expert: layer_7[3.4-9.2% std=1.69]
  Expert: layer_8[3.4-10.9% std=1.73]
  Expert: layer_9[3.9-10.9% std=1.65]
  Expert: layer_10[3.6-7.8% std=1.21]
  Expert: layer_12[3.8-9.4% std=1.57]
  Expert: layer_13[4.6-9.1% std=1.30]
  Expert: layer_14[3.6-10.7% std=1.63]
  Expert: layer_15[0.8-9.4% std=1.99]
  Expert: layer_16[4.2-11.1% std=1.83]
  Expert: layer_18[2.8-11.2% std=2.04]
  Expert: layer_19[3.4-12.6% std=2.24]
  Expert: layer_20[3.0-11.8% std=2.60]
  Expert: layer_21[2.7-11.7% std=2.27]
Step     453 | Loss: 3.3299 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.5% std=0.49] | layer_5[4.6-8.3% std=0.96] | layer_11[3.8-10.1% std=1.69] | layer_17[3.2-10.8% std=1.91] | layer_22[1.1-19.5% std=3.95]
  Expert: layer_1[5.8-6.8% std=0.23]
  Expert: layer_2[5.7-6.9% std=0.33]
  Expert: layer_3[4.3-9.3% std=1.48]
  Expert: layer_4[4.8-9.3% std=1.37]
  Expert: layer_6[4.7-8.3% std=1.02]
  Expert: layer_7[3.5-8.6% std=1.68]
  Expert: layer_8[3.3-11.3% std=1.83]
  Expert: layer_9[3.9-10.7% std=1.59]
  Expert: layer_10[3.5-7.8% std=1.21]
  Expert: layer_12[3.5-9.7% std=1.77]
  Expert: layer_13[4.6-9.0% std=1.39]
  Expert: layer_14[3.4-9.6% std=1.61]
  Expert: layer_15[0.7-10.4% std=2.14]
  Expert: layer_16[4.2-11.4% std=1.82]
  Expert: layer_18[2.8-10.3% std=2.02]
  Expert: layer_19[3.4-14.2% std=2.54]
  Expert: layer_20[2.9-11.2% std=2.40]
  Expert: layer_21[2.5-11.5% std=2.33]
Step     454 | Loss: 3.3180 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.49] | layer_5[4.5-8.5% std=0.99] | layer_11[3.7-9.3% std=1.70] | layer_17[3.6-10.5% std=1.67] | layer_22[0.9-19.0% std=3.76]
  Expert: layer_1[5.7-6.7% std=0.24]
  Expert: layer_2[5.7-6.7% std=0.30]
  Expert: layer_3[4.3-9.5% std=1.55]
  Expert: layer_4[4.8-9.2% std=1.35]
  Expert: layer_6[4.5-8.4% std=1.09]
  Expert: layer_7[3.7-8.4% std=1.63]
  Expert: layer_8[3.3-11.1% std=1.80]
  Expert: layer_9[3.7-11.0% std=1.73]
  Expert: layer_10[3.4-7.8% std=1.22]
  Expert: layer_12[3.1-9.3% std=1.91]
  Expert: layer_13[4.1-8.7% std=1.43]
  Expert: layer_14[3.3-9.5% std=1.55]
  Expert: layer_15[0.8-11.4% std=2.19]
  Expert: layer_16[4.3-10.4% std=1.72]
  Expert: layer_18[2.8-10.7% std=2.08]
  Expert: layer_19[3.3-14.0% std=2.38]
  Expert: layer_20[3.1-10.8% std=2.39]
  Expert: layer_21[2.6-11.9% std=2.24]
Step     455 | Loss: 3.3723 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.6% std=0.49] | layer_5[4.5-8.6% std=1.01] | layer_11[3.4-8.9% std=1.62] | layer_17[3.1-12.1% std=1.85] | layer_22[0.7-20.2% std=4.04]
  Expert: layer_1[5.7-6.7% std=0.24]
  Expert: layer_2[5.8-6.6% std=0.23]
  Expert: layer_3[4.4-9.5% std=1.56]
  Expert: layer_4[4.8-9.3% std=1.36]
  Expert: layer_6[4.6-8.3% std=1.09]
  Expert: layer_7[3.3-8.9% std=1.75]
  Expert: layer_8[3.5-9.9% std=1.57]
  Expert: layer_9[3.8-11.2% std=1.77]
  Expert: layer_10[3.6-8.0% std=1.22]
  Expert: layer_12[3.3-9.6% std=1.69]
  Expert: layer_13[4.3-9.1% std=1.40]
  Expert: layer_14[3.3-9.6% std=1.70]
  Expert: layer_15[1.0-9.6% std=1.96]
  Expert: layer_16[3.9-11.0% std=1.91]
  Expert: layer_18[2.9-11.2% std=2.09]
  Expert: layer_19[3.6-12.3% std=2.03]
  Expert: layer_20[2.7-11.5% std=2.65]
  Expert: layer_21[2.8-12.2% std=2.30]
Step     456 | Loss: 3.3148 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2641 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.52] | layer_5[4.6-8.7% std=1.03] | layer_11[3.7-9.4% std=1.60] | layer_17[3.2-11.4% std=1.81] | layer_22[0.8-20.0% std=4.00]
  Expert: layer_1[5.7-6.7% std=0.21]
  Expert: layer_2[5.8-6.6% std=0.28]
  Expert: layer_3[4.4-9.4% std=1.49]
  Expert: layer_4[4.8-9.2% std=1.37]
  Expert: layer_6[4.7-8.1% std=1.03]
  Expert: layer_7[3.2-8.9% std=1.75]
  Expert: layer_8[3.4-10.5% std=1.69]
  Expert: layer_9[3.8-10.5% std=1.61]
  Expert: layer_10[3.6-8.2% std=1.19]
  Expert: layer_12[3.4-10.1% std=1.80]
  Expert: layer_13[4.6-8.6% std=1.35]
  Expert: layer_14[3.5-9.9% std=1.74]
  Expert: layer_15[0.9-9.7% std=1.97]
  Expert: layer_16[4.4-11.0% std=1.69]
  Expert: layer_18[2.6-11.6% std=2.26]
  Expert: layer_19[3.1-12.1% std=2.15]
  Expert: layer_20[3.1-10.9% std=2.40]
  Expert: layer_21[2.9-10.6% std=2.02]
Step     457 | Loss: 3.2934 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2672 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.6-8.7% std=1.05] | layer_11[3.4-10.0% std=1.71] | layer_17[3.2-12.2% std=2.00] | layer_22[0.9-19.4% std=3.89]
  Expert: layer_1[5.8-6.7% std=0.21]
  Expert: layer_2[5.8-6.7% std=0.28]
  Expert: layer_3[4.6-9.5% std=1.49]
  Expert: layer_4[4.9-9.3% std=1.36]
  Expert: layer_6[4.5-8.1% std=1.07]
  Expert: layer_7[3.4-8.8% std=1.69]
  Expert: layer_8[3.3-11.0% std=1.79]
  Expert: layer_9[3.8-10.4% std=1.67]
  Expert: layer_10[3.5-7.8% std=1.16]
  Expert: layer_12[3.1-9.7% std=2.04]
  Expert: layer_13[4.4-9.5% std=1.56]
  Expert: layer_14[3.5-10.2% std=1.73]
  Expert: layer_15[0.9-11.7% std=2.22]
  Expert: layer_16[4.2-10.5% std=1.75]
  Expert: layer_18[2.8-11.4% std=2.17]
  Expert: layer_19[3.2-14.1% std=2.49]
  Expert: layer_20[3.1-10.8% std=2.35]
  Expert: layer_21[2.6-11.3% std=2.33]
Step     458 | Loss: 3.3183 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.52] | layer_5[4.7-8.4% std=0.98] | layer_11[3.2-9.4% std=1.65] | layer_17[3.4-11.8% std=1.85] | layer_22[0.9-19.9% std=3.99]
  Expert: layer_1[5.7-6.8% std=0.24]
  Expert: layer_2[5.8-6.6% std=0.27]
  Expert: layer_3[4.5-9.3% std=1.47]
  Expert: layer_4[4.6-9.5% std=1.40]
  Expert: layer_6[4.6-8.1% std=1.01]
  Expert: layer_7[3.1-9.0% std=1.87]
  Expert: layer_8[3.4-10.6% std=1.73]
  Expert: layer_9[3.5-10.5% std=1.67]
  Expert: layer_10[3.8-8.3% std=1.18]
  Expert: layer_12[3.2-10.3% std=1.92]
  Expert: layer_13[4.1-9.1% std=1.46]
  Expert: layer_14[3.7-8.9% std=1.65]
  Expert: layer_15[1.0-9.6% std=1.99]
  Expert: layer_16[4.4-12.0% std=2.09]
  Expert: layer_18[2.7-11.4% std=2.18]
  Expert: layer_19[3.4-13.5% std=2.39]
  Expert: layer_20[3.2-10.7% std=2.43]
  Expert: layer_21[2.6-12.2% std=2.22]
Step     459 | Loss: 3.3511 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.5% std=0.51] | layer_5[4.8-8.5% std=0.98] | layer_11[3.3-10.1% std=1.79] | layer_17[3.2-11.9% std=1.97] | layer_22[0.7-19.1% std=3.91]
  Expert: layer_1[5.7-6.7% std=0.22]
  Expert: layer_2[5.6-6.7% std=0.33]
  Expert: layer_3[4.4-9.4% std=1.47]
  Expert: layer_4[4.7-9.7% std=1.41]
  Expert: layer_6[4.6-8.1% std=1.05]
  Expert: layer_7[3.5-9.1% std=1.87]
  Expert: layer_8[3.2-11.1% std=1.96]
  Expert: layer_9[3.4-9.6% std=1.62]
  Expert: layer_10[3.9-8.1% std=1.16]
  Expert: layer_12[3.3-10.4% std=1.93]
  Expert: layer_13[4.3-8.9% std=1.54]
  Expert: layer_14[3.5-9.2% std=1.52]
  Expert: layer_15[0.9-9.8% std=1.97]
  Expert: layer_16[4.5-11.7% std=1.90]
  Expert: layer_18[2.9-12.6% std=2.38]
  Expert: layer_19[3.4-11.2% std=2.02]
  Expert: layer_20[3.1-12.3% std=2.46]
  Expert: layer_21[2.9-10.6% std=1.98]
Step     460 | Loss: 3.2479 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.5% std=0.51] | layer_5[4.8-8.4% std=0.97] | layer_11[2.9-9.1% std=1.70] | layer_17[3.7-11.1% std=1.62] | layer_22[1.0-20.4% std=4.07]
  Expert: layer_1[5.8-6.6% std=0.21]
  Expert: layer_2[5.7-6.6% std=0.30]
  Expert: layer_3[4.3-9.1% std=1.45]
  Expert: layer_4[4.4-9.5% std=1.46]
  Expert: layer_6[4.6-8.1% std=1.05]
  Expert: layer_7[3.2-9.5% std=1.91]
  Expert: layer_8[3.2-10.4% std=1.83]
  Expert: layer_9[3.2-10.0% std=1.65]
  Expert: layer_10[3.8-8.2% std=1.23]
  Expert: layer_12[3.2-10.5% std=2.00]
  Expert: layer_13[4.2-9.0% std=1.48]
  Expert: layer_14[3.4-9.4% std=1.71]
  Expert: layer_15[0.9-9.9% std=2.04]
  Expert: layer_16[4.3-11.1% std=1.88]
  Expert: layer_18[2.7-11.4% std=2.14]
  Expert: layer_19[3.1-13.0% std=2.34]
  Expert: layer_20[3.3-10.4% std=2.17]
  Expert: layer_21[2.5-11.6% std=2.24]
Step     461 | Loss: 3.3198 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2639 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.7-8.7% std=1.01] | layer_11[3.3-9.4% std=1.73] | layer_17[3.3-12.4% std=2.04] | layer_22[0.8-20.9% std=4.26]
  Expert: layer_1[5.8-6.6% std=0.19]
  Expert: layer_2[5.6-6.8% std=0.34]
  Expert: layer_3[4.3-9.2% std=1.50]
  Expert: layer_4[4.5-10.1% std=1.52]
  Expert: layer_6[4.5-8.1% std=1.11]
  Expert: layer_7[3.4-9.8% std=1.89]
  Expert: layer_8[3.0-10.8% std=1.96]
  Expert: layer_9[3.1-9.9% std=1.73]
  Expert: layer_10[3.5-7.9% std=1.24]
  Expert: layer_12[3.3-11.0% std=2.13]
  Expert: layer_13[4.2-9.3% std=1.59]
  Expert: layer_14[3.6-9.1% std=1.73]
  Expert: layer_15[0.9-10.2% std=2.00]
  Expert: layer_16[4.0-12.5% std=2.15]
  Expert: layer_18[2.6-11.7% std=2.33]
  Expert: layer_19[3.3-13.7% std=2.41]
  Expert: layer_20[2.8-10.5% std=2.36]
  Expert: layer_21[2.6-12.4% std=2.35]
Step     462 | Loss: 3.3558 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.56] | layer_5[4.8-8.9% std=1.02] | layer_11[3.1-10.5% std=1.78] | layer_17[3.5-12.3% std=2.00] | layer_22[0.8-20.1% std=4.11]
  Expert: layer_1[5.8-6.6% std=0.20]
  Expert: layer_2[5.8-6.8% std=0.32]
  Expert: layer_3[4.1-9.0% std=1.54]
  Expert: layer_4[4.3-10.2% std=1.57]
  Expert: layer_6[4.4-7.9% std=1.09]
  Expert: layer_7[3.0-9.8% std=2.03]
  Expert: layer_8[3.2-10.9% std=1.98]
  Expert: layer_9[3.4-9.6% std=1.67]
  Expert: layer_10[3.9-7.9% std=1.17]
  Expert: layer_12[3.4-10.3% std=2.04]
  Expert: layer_13[4.4-9.1% std=1.47]
  Expert: layer_14[3.7-9.6% std=1.62]
  Expert: layer_15[1.0-11.1% std=2.13]
  Expert: layer_16[4.4-11.8% std=1.80]
  Expert: layer_18[2.6-11.6% std=2.29]
  Expert: layer_19[3.4-12.9% std=2.21]
  Expert: layer_20[3.1-11.3% std=2.51]
  Expert: layer_21[2.8-12.4% std=2.23]
Step     463 | Loss: 3.2683 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.7% std=0.59] | layer_5[4.6-8.7% std=1.02] | layer_11[3.4-9.9% std=1.79] | layer_17[3.3-11.7% std=1.82] | layer_22[1.3-19.8% std=4.01]
  Expert: layer_1[5.9-6.6% std=0.24]
  Expert: layer_2[5.7-6.8% std=0.33]
  Expert: layer_3[4.1-8.8% std=1.55]
  Expert: layer_4[4.1-10.1% std=1.56]
  Expert: layer_6[4.4-8.0% std=1.15]
  Expert: layer_7[3.3-9.8% std=1.87]
  Expert: layer_8[3.7-10.6% std=1.83]
  Expert: layer_9[3.2-10.4% std=1.79]
  Expert: layer_10[3.7-7.7% std=1.18]
  Expert: layer_12[3.2-10.3% std=2.05]
  Expert: layer_13[3.7-8.9% std=1.61]
  Expert: layer_14[3.5-8.6% std=1.57]
  Expert: layer_15[0.9-10.8% std=2.06]
  Expert: layer_16[4.1-10.6% std=1.88]
  Expert: layer_18[2.5-11.9% std=2.34]
  Expert: layer_19[3.6-13.7% std=2.31]
  Expert: layer_20[3.0-10.7% std=2.49]
  Expert: layer_21[2.4-11.8% std=2.21]
Step     464 | Loss: 3.2934 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.7% std=0.56] | layer_5[4.5-8.7% std=1.00] | layer_11[3.3-9.7% std=1.83] | layer_17[2.7-11.6% std=1.97] | layer_22[0.8-21.9% std=4.48]
  Expert: layer_1[5.9-6.6% std=0.19]
  Expert: layer_2[5.7-6.7% std=0.32]
  Expert: layer_3[4.1-9.0% std=1.65]
  Expert: layer_4[4.4-9.9% std=1.54]
  Expert: layer_6[4.4-7.7% std=1.20]
  Expert: layer_7[3.4-10.2% std=1.92]
  Expert: layer_8[3.1-10.1% std=1.83]
  Expert: layer_9[3.4-10.0% std=1.79]
  Expert: layer_10[3.7-7.6% std=1.17]
  Expert: layer_12[3.2-10.9% std=2.16]
  Expert: layer_13[4.0-9.0% std=1.46]
  Expert: layer_14[3.4-9.4% std=1.58]
  Expert: layer_15[1.0-10.6% std=2.12]
  Expert: layer_16[4.1-10.7% std=1.90]
  Expert: layer_18[2.7-11.4% std=2.32]
  Expert: layer_19[3.4-14.1% std=2.39]
  Expert: layer_20[3.3-11.1% std=2.41]
  Expert: layer_21[2.7-11.6% std=2.44]
Step     465 | Loss: 3.2536 | LR: 2.50e-04 | GradNorm: 0.40 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.6% std=0.56] | layer_5[4.8-8.5% std=0.94] | layer_11[3.0-10.2% std=2.03] | layer_17[3.4-10.8% std=1.63] | layer_22[1.3-19.3% std=3.79]
  Expert: layer_1[5.9-6.7% std=0.22]
  Expert: layer_2[5.7-6.9% std=0.35]
  Expert: layer_3[4.0-9.0% std=1.71]
  Expert: layer_4[4.2-9.9% std=1.55]
  Expert: layer_6[4.4-7.8% std=1.13]
  Expert: layer_7[3.5-10.2% std=2.12]
  Expert: layer_8[3.7-9.7% std=1.66]
  Expert: layer_9[3.2-10.7% std=1.80]
  Expert: layer_10[3.7-8.0% std=1.25]
  Expert: layer_12[3.2-10.8% std=2.02]
  Expert: layer_13[3.9-10.1% std=1.78]
  Expert: layer_14[4.0-9.6% std=1.77]
  Expert: layer_15[0.6-8.9% std=2.08]
  Expert: layer_16[4.2-12.5% std=2.24]
  Expert: layer_18[2.5-12.0% std=2.13]
  Expert: layer_19[2.8-12.2% std=2.12]
  Expert: layer_20[2.9-10.9% std=2.38]
  Expert: layer_21[2.7-11.2% std=2.28]
Step     466 | Loss: 3.2782 | LR: 2.50e-04 | GradNorm: 0.43 | Tok/s: 2643 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.6% std=0.57] | layer_5[4.7-8.7% std=1.03] | layer_11[3.3-10.0% std=1.92] | layer_17[3.1-10.7% std=1.74] | layer_22[0.8-22.0% std=4.55]
  Expert: layer_1[5.9-6.8% std=0.24]
  Expert: layer_2[5.7-6.7% std=0.32]
  Expert: layer_3[4.0-9.0% std=1.68]
  Expert: layer_4[4.3-9.6% std=1.50]
  Expert: layer_6[4.4-8.0% std=1.17]
  Expert: layer_7[3.4-10.1% std=1.84]
  Expert: layer_8[3.1-10.3% std=1.78]
  Expert: layer_9[3.2-10.4% std=1.81]
  Expert: layer_10[3.8-8.0% std=1.23]
  Expert: layer_12[3.0-10.1% std=2.12]
  Expert: layer_13[4.3-9.2% std=1.61]
  Expert: layer_14[3.6-9.9% std=1.55]
  Expert: layer_15[0.9-11.4% std=2.26]
  Expert: layer_16[4.0-10.8% std=1.98]
  Expert: layer_18[2.4-12.5% std=2.59]
  Expert: layer_19[2.7-14.9% std=2.61]
  Expert: layer_20[2.7-12.2% std=2.68]
  Expert: layer_21[1.9-12.1% std=2.50]
Step     467 | Loss: 3.2944 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.58] | layer_5[4.7-9.0% std=1.11] | layer_11[3.3-11.0% std=2.15] | layer_17[3.7-11.3% std=1.92] | layer_22[0.7-21.2% std=4.41]
  Expert: layer_1[5.8-6.6% std=0.24]
  Expert: layer_2[5.7-6.9% std=0.35]
  Expert: layer_3[4.1-9.0% std=1.71]
  Expert: layer_4[4.3-9.7% std=1.50]
  Expert: layer_6[4.3-7.8% std=1.13]
  Expert: layer_7[3.7-9.9% std=1.85]
  Expert: layer_8[3.2-10.6% std=1.83]
  Expert: layer_9[3.4-10.1% std=1.72]
  Expert: layer_10[3.6-8.1% std=1.27]
  Expert: layer_12[3.1-10.8% std=2.20]
  Expert: layer_13[4.3-9.6% std=1.75]
  Expert: layer_14[3.8-9.2% std=1.64]
  Expert: layer_15[0.7-10.5% std=2.18]
  Expert: layer_16[4.1-10.8% std=1.95]
  Expert: layer_18[2.4-12.4% std=2.58]
  Expert: layer_19[3.0-13.8% std=2.51]
  Expert: layer_20[2.6-11.7% std=2.58]
  Expert: layer_21[2.4-14.0% std=2.68]
Step     468 | Loss: 3.2463 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.6% std=0.56] | layer_5[4.8-9.2% std=1.06] | layer_11[3.5-11.8% std=2.30] | layer_17[3.6-11.8% std=1.86] | layer_22[0.7-19.9% std=3.97]
  Expert: layer_1[5.8-6.6% std=0.23]
  Expert: layer_2[5.7-6.8% std=0.30]
  Expert: layer_3[3.9-9.3% std=1.79]
  Expert: layer_4[4.2-9.5% std=1.51]
  Expert: layer_6[4.2-7.6% std=1.17]
  Expert: layer_7[4.2-9.3% std=1.74]
  Expert: layer_8[3.6-10.0% std=1.69]
  Expert: layer_9[3.5-10.5% std=1.78]
  Expert: layer_10[3.7-8.1% std=1.30]
  Expert: layer_12[3.5-11.2% std=2.29]
  Expert: layer_13[4.1-10.3% std=1.76]
  Expert: layer_14[3.7-9.1% std=1.66]
  Expert: layer_15[0.7-10.4% std=2.14]
  Expert: layer_16[3.6-12.4% std=2.17]
  Expert: layer_18[2.6-12.1% std=2.25]
  Expert: layer_19[3.4-14.0% std=2.41]
  Expert: layer_20[2.8-11.1% std=2.37]
  Expert: layer_21[2.4-11.7% std=2.53]
Step     469 | Loss: 3.3321 | LR: 2.50e-04 | GradNorm: 0.39 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.55] | layer_5[4.8-9.4% std=1.13] | layer_11[3.6-11.5% std=2.17] | layer_17[3.2-10.4% std=1.73] | layer_22[0.7-19.7% std=4.03]
  Expert: layer_1[5.7-6.7% std=0.25]
  Expert: layer_2[5.8-7.0% std=0.33]
  Expert: layer_3[3.9-9.2% std=1.75]
  Expert: layer_4[4.4-9.8% std=1.54]
  Expert: layer_6[4.3-7.5% std=1.13]
  Expert: layer_7[4.0-9.5% std=1.86]
  Expert: layer_8[3.4-10.2% std=1.79]
  Expert: layer_9[3.5-10.5% std=1.77]
  Expert: layer_10[3.8-7.9% std=1.27]
  Expert: layer_12[3.5-12.2% std=2.40]
  Expert: layer_13[4.3-10.2% std=1.70]
  Expert: layer_14[3.6-10.0% std=1.74]
  Expert: layer_15[0.8-8.7% std=1.81]
  Expert: layer_16[4.0-12.0% std=2.10]
  Expert: layer_18[2.6-13.0% std=2.65]
  Expert: layer_19[3.3-13.5% std=2.30]
  Expert: layer_20[3.3-10.7% std=2.35]
  Expert: layer_21[2.4-12.3% std=2.47]
Step     470 | Loss: 3.3242 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.4% std=0.51] | layer_5[4.7-9.5% std=1.16] | layer_11[3.4-11.9% std=2.21] | layer_17[3.5-13.5% std=2.32] | layer_22[0.8-22.1% std=4.59]
  Expert: layer_1[5.7-6.7% std=0.26]
  Expert: layer_2[5.6-7.0% std=0.34]
  Expert: layer_3[4.1-9.2% std=1.72]
  Expert: layer_4[4.3-10.0% std=1.54]
  Expert: layer_6[4.3-7.6% std=1.17]
  Expert: layer_7[3.6-9.4% std=1.80]
  Expert: layer_8[3.2-10.8% std=1.88]
  Expert: layer_9[3.6-10.6% std=1.80]
  Expert: layer_10[3.6-8.0% std=1.31]
  Expert: layer_12[3.0-11.3% std=2.40]
  Expert: layer_13[4.1-10.6% std=1.77]
  Expert: layer_14[3.6-9.5% std=1.72]
  Expert: layer_15[1.0-11.3% std=2.30]
  Expert: layer_16[3.8-10.6% std=1.88]
  Expert: layer_18[2.4-11.4% std=2.31]
  Expert: layer_19[3.3-14.4% std=2.54]
  Expert: layer_20[2.4-10.7% std=2.54]
  Expert: layer_21[2.6-12.8% std=2.56]
Step     471 | Loss: 3.2498 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2635 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.55] | layer_5[4.7-9.0% std=1.12] | layer_11[3.3-12.1% std=2.32] | layer_17[3.4-12.7% std=2.22] | layer_22[1.1-22.4% std=4.60]
  Expert: layer_1[5.6-6.6% std=0.28]
  Expert: layer_2[5.7-7.1% std=0.35]
  Expert: layer_3[3.9-8.9% std=1.70]
  Expert: layer_4[4.2-10.1% std=1.59]
  Expert: layer_6[4.1-7.9% std=1.16]
  Expert: layer_7[3.7-9.0% std=1.76]
  Expert: layer_8[3.6-11.2% std=1.89]
  Expert: layer_9[3.4-11.1% std=1.86]
  Expert: layer_10[3.4-8.3% std=1.33]
  Expert: layer_12[2.9-11.1% std=2.39]
  Expert: layer_13[4.2-10.2% std=1.77]
  Expert: layer_14[3.4-9.9% std=1.89]
  Expert: layer_15[0.8-10.7% std=2.21]
  Expert: layer_16[3.8-11.0% std=1.99]
  Expert: layer_18[2.6-11.8% std=2.32]
  Expert: layer_19[3.2-14.2% std=2.47]
  Expert: layer_20[2.3-10.3% std=2.49]
  Expert: layer_21[2.4-13.5% std=2.73]
Step     472 | Loss: 3.3526 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.51] | layer_5[4.8-9.1% std=1.07] | layer_11[3.4-11.5% std=2.09] | layer_17[3.6-10.9% std=1.83] | layer_22[0.7-21.3% std=4.37]
  Expert: layer_1[5.6-6.7% std=0.26]
  Expert: layer_2[5.7-7.0% std=0.35]
  Expert: layer_3[4.0-9.0% std=1.73]
  Expert: layer_4[4.2-10.5% std=1.66]
  Expert: layer_6[4.2-7.9% std=1.16]
  Expert: layer_7[3.5-9.3% std=1.89]
  Expert: layer_8[3.4-10.9% std=1.90]
  Expert: layer_9[3.5-11.0% std=1.84]
  Expert: layer_10[3.7-8.2% std=1.32]
  Expert: layer_12[2.9-11.5% std=2.22]
  Expert: layer_13[4.2-9.9% std=1.60]
  Expert: layer_14[3.4-11.4% std=1.99]
  Expert: layer_15[0.9-10.0% std=2.09]
  Expert: layer_16[4.3-11.3% std=1.84]
  Expert: layer_18[2.9-11.6% std=2.31]
  Expert: layer_19[3.1-13.4% std=2.47]
  Expert: layer_20[3.0-10.3% std=2.39]
  Expert: layer_21[2.4-12.4% std=2.48]
Step     473 | Loss: 3.2921 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2661 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.5% std=0.53] | layer_5[4.9-9.1% std=1.13] | layer_11[3.3-11.4% std=2.04] | layer_17[3.5-11.9% std=1.95] | layer_22[0.8-22.1% std=4.57]
  Expert: layer_1[5.6-6.8% std=0.27]
  Expert: layer_2[5.8-7.0% std=0.34]
  Expert: layer_3[4.0-8.9% std=1.68]
  Expert: layer_4[4.1-10.8% std=1.73]
  Expert: layer_6[4.2-7.8% std=1.10]
  Expert: layer_7[3.5-9.4% std=1.95]
  Expert: layer_8[3.5-10.8% std=1.88]
  Expert: layer_9[3.3-11.1% std=1.87]
  Expert: layer_10[3.5-8.1% std=1.35]
  Expert: layer_12[2.7-11.7% std=2.26]
  Expert: layer_13[4.3-9.4% std=1.64]
  Expert: layer_14[3.3-10.6% std=1.90]
  Expert: layer_15[1.0-9.7% std=2.01]
  Expert: layer_16[4.2-10.5% std=1.76]
  Expert: layer_18[2.5-11.8% std=2.26]
  Expert: layer_19[3.1-13.6% std=2.42]
  Expert: layer_20[2.9-10.8% std=2.40]
  Expert: layer_21[2.4-11.8% std=2.38]
Step     474 | Loss: 3.2846 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2665 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.54] | layer_5[4.9-8.9% std=1.08] | layer_11[3.1-10.9% std=2.04] | layer_17[3.4-12.0% std=1.88] | layer_22[0.9-23.0% std=4.73]
  Expert: layer_1[5.7-6.8% std=0.27]
  Expert: layer_2[5.7-7.1% std=0.35]
  Expert: layer_3[4.1-8.9% std=1.71]
  Expert: layer_4[4.0-10.9% std=1.78]
  Expert: layer_6[4.1-7.9% std=1.13]
  Expert: layer_7[3.2-9.8% std=2.07]
  Expert: layer_8[3.7-10.3% std=1.77]
  Expert: layer_9[3.1-11.0% std=1.91]
  Expert: layer_10[3.3-8.7% std=1.45]
  Expert: layer_12[2.8-12.4% std=2.29]
  Expert: layer_13[4.1-9.9% std=1.86]
  Expert: layer_14[3.6-10.0% std=1.96]
  Expert: layer_15[0.7-9.1% std=2.08]
  Expert: layer_16[4.3-11.1% std=1.91]
  Expert: layer_18[2.4-13.7% std=2.51]
  Expert: layer_19[2.5-12.7% std=2.40]
  Expert: layer_20[2.8-10.9% std=2.53]
  Expert: layer_21[2.4-11.6% std=2.43]
Step     475 | Loss: 3.3064 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2666 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.57] | layer_5[4.8-9.1% std=1.12] | layer_11[3.4-12.3% std=2.32] | layer_17[3.2-12.7% std=2.12] | layer_22[0.7-23.9% std=4.95]
  Expert: layer_1[5.7-6.8% std=0.27]
  Expert: layer_2[5.8-6.9% std=0.34]
  Expert: layer_3[4.1-9.0% std=1.73]
  Expert: layer_4[4.1-10.8% std=1.74]
  Expert: layer_6[4.1-8.1% std=1.18]
  Expert: layer_7[3.5-9.7% std=1.87]
  Expert: layer_8[3.5-10.5% std=1.84]
  Expert: layer_9[3.1-11.4% std=2.07]
  Expert: layer_10[3.3-8.3% std=1.42]
  Expert: layer_12[3.1-12.4% std=2.37]
  Expert: layer_13[4.1-9.7% std=1.76]
  Expert: layer_14[3.5-10.9% std=1.95]
  Expert: layer_15[0.8-10.3% std=2.13]
  Expert: layer_16[4.2-11.3% std=1.97]
  Expert: layer_18[2.6-12.5% std=2.42]
  Expert: layer_19[3.0-13.5% std=2.38]
  Expert: layer_20[3.0-11.2% std=2.54]
  Expert: layer_21[2.3-12.2% std=2.62]
Step     476 | Loss: 3.3236 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2640 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.2-7.6% std=0.55] | layer_5[4.7-8.8% std=1.12] | layer_11[3.1-12.0% std=2.25] | layer_17[3.5-11.7% std=1.81] | layer_22[1.1-21.2% std=4.33]
  Expert: layer_1[5.7-6.8% std=0.26]
  Expert: layer_2[5.7-7.0% std=0.37]
  Expert: layer_3[4.2-9.1% std=1.70]
  Expert: layer_4[4.0-10.9% std=1.77]
  Expert: layer_6[4.1-8.0% std=1.15]
  Expert: layer_7[3.2-9.5% std=1.99]
  Expert: layer_8[3.5-10.8% std=1.86]
  Expert: layer_9[3.0-11.2% std=1.95]
  Expert: layer_10[3.5-8.5% std=1.44]
  Expert: layer_12[2.9-13.0% std=2.40]
  Expert: layer_13[3.9-10.0% std=1.82]
  Expert: layer_14[3.3-10.4% std=2.00]
  Expert: layer_15[0.8-10.1% std=2.08]
  Expert: layer_16[4.1-11.8% std=2.03]
  Expert: layer_18[2.6-13.2% std=2.55]
  Expert: layer_19[2.6-13.1% std=2.42]
  Expert: layer_20[3.0-11.0% std=2.57]
  Expert: layer_21[2.2-12.2% std=2.48]
Step     477 | Loss: 3.2437 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.6% std=0.55] | layer_5[4.6-8.5% std=1.10] | layer_11[2.6-11.3% std=2.23] | layer_17[3.3-11.7% std=1.95] | layer_22[0.9-22.9% std=4.71]
  Expert: layer_1[5.7-6.7% std=0.27]
  Expert: layer_2[5.7-6.9% std=0.36]
  Expert: layer_3[4.1-9.1% std=1.65]
  Expert: layer_4[4.0-10.7% std=1.76]
  Expert: layer_6[4.2-7.8% std=1.13]
  Expert: layer_7[3.8-9.3% std=1.88]
  Expert: layer_8[3.6-11.2% std=1.89]
  Expert: layer_9[2.8-11.3% std=1.99]
  Expert: layer_10[3.4-8.5% std=1.43]
  Expert: layer_12[2.8-12.7% std=2.47]
  Expert: layer_13[3.7-9.6% std=1.78]
  Expert: layer_14[3.3-9.8% std=1.76]
  Expert: layer_15[1.0-9.5% std=2.07]
  Expert: layer_16[3.7-11.8% std=2.14]
  Expert: layer_18[2.6-12.0% std=2.39]
  Expert: layer_19[2.9-14.6% std=2.62]
  Expert: layer_20[2.8-12.2% std=2.61]
  Expert: layer_21[2.4-13.7% std=2.72]
Step     478 | Loss: 3.2699 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2668 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.4% std=0.54] | layer_5[4.4-8.6% std=1.19] | layer_11[2.6-13.7% std=2.58] | layer_17[2.8-14.2% std=2.36] | layer_22[0.8-23.8% std=4.93]
  Expert: layer_1[5.8-6.7% std=0.25]
  Expert: layer_2[5.7-6.8% std=0.35]
  Expert: layer_3[3.9-9.3% std=1.68]
  Expert: layer_4[4.1-10.6% std=1.75]
  Expert: layer_6[4.1-7.7% std=1.17]
  Expert: layer_7[3.7-9.5% std=1.79]
  Expert: layer_8[3.2-11.3% std=1.97]
  Expert: layer_9[2.9-11.3% std=2.02]
  Expert: layer_10[3.3-8.4% std=1.51]
  Expert: layer_12[2.9-11.3% std=2.17]
  Expert: layer_13[3.8-10.1% std=1.97]
  Expert: layer_14[3.2-11.0% std=1.90]
  Expert: layer_15[0.9-10.9% std=2.34]
  Expert: layer_16[3.9-10.8% std=1.94]
  Expert: layer_18[2.6-12.4% std=2.38]
  Expert: layer_19[2.6-14.5% std=2.69]
  Expert: layer_20[3.0-11.7% std=2.60]
  Expert: layer_21[2.4-12.4% std=2.69]
Step     479 | Loss: 3.2533 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2671 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.4-7.5% std=0.56] | layer_5[4.4-8.9% std=1.19] | layer_11[2.8-13.4% std=2.50] | layer_17[3.0-13.2% std=2.34] | layer_22[1.1-22.9% std=4.71]
  Expert: layer_1[5.8-6.7% std=0.28]
  Expert: layer_2[5.6-6.9% std=0.37]
  Expert: layer_3[4.1-9.3% std=1.65]
  Expert: layer_4[4.2-10.5% std=1.71]
  Expert: layer_6[4.2-7.7% std=1.12]
  Expert: layer_7[3.6-8.9% std=1.80]
  Expert: layer_8[3.3-11.2% std=1.96]
  Expert: layer_9[2.6-11.5% std=2.04]
  Expert: layer_10[3.4-8.9% std=1.52]
  Expert: layer_12[2.9-11.4% std=2.35]
  Expert: layer_13[3.7-9.9% std=1.77]
  Expert: layer_14[3.3-10.0% std=1.88]
  Expert: layer_15[0.8-9.7% std=2.02]
  Expert: layer_16[3.6-11.8% std=2.22]
  Expert: layer_18[2.4-13.2% std=2.55]
  Expert: layer_19[3.0-13.1% std=2.39]
  Expert: layer_20[2.4-11.0% std=2.62]
  Expert: layer_21[2.6-12.9% std=2.70]
Step     480 | Loss: 3.2976 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2670 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.3-7.4% std=0.53] | layer_5[4.4-9.2% std=1.21] | layer_11[3.2-12.4% std=2.36] | layer_17[3.1-12.9% std=2.23] | layer_22[0.9-22.6% std=4.70]
  Expert: layer_1[5.8-6.8% std=0.26]
  Expert: layer_2[5.6-6.9% std=0.37]
  Expert: layer_3[4.1-9.3% std=1.69]
  Expert: layer_4[4.3-10.4% std=1.68]
  Expert: layer_6[4.2-8.0% std=1.13]
  Expert: layer_7[3.6-9.2% std=1.77]
  Expert: layer_8[3.4-11.0% std=1.88]
  Expert: layer_9[2.8-11.9% std=2.09]
  Expert: layer_10[3.5-9.2% std=1.58]
  Expert: layer_12[2.6-10.8% std=2.41]
  Expert: layer_13[3.7-10.4% std=1.87]
  Expert: layer_14[3.4-10.6% std=1.89]
  Expert: layer_15[0.9-12.5% std=2.42]
  Expert: layer_16[3.6-11.4% std=2.23]
  Expert: layer_18[2.5-13.0% std=2.63]
  Expert: layer_19[3.0-15.2% std=2.74]
  Expert: layer_20[2.2-11.5% std=2.91]
  Expert: layer_21[2.5-12.8% std=2.86]
Step     481 | Loss: 3.2593 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2641 | Mem: 65.5GB | Data: train_2k/008.npy
  Expert: layer_0[5.5-7.4% std=0.52] | layer_5[4.5-8.5% std=1.09] | layer_11[2.9-12.0% std=2.28] | layer_17[3.3-12.6% std=2.02] | layer_22[0.7-25.4% std=5.26]
  Expert: layer_1[5.8-7.0% std=0.30]
  Expert: layer_2[5.5-6.9% std=0.39]
  Expert: layer_3[3.9-9.1% std=1.69]
  Expert: layer_4[4.4-10.1% std=1.68]
  Expert: layer_6[4.2-7.9% std=1.12]
  Expert: layer_7[3.5-9.7% std=1.89]
  Expert: layer_8[3.6-11.7% std=1.90]
  Expert: layer_9[2.8-12.2% std=2.11]
  Expert: layer_10[3.7-8.8% std=1.49]
  Expert: layer_12[2.7-10.5% std=2.16]
  Expert: layer_13[4.2-10.2% std=1.71]
  Expert: layer_14[3.6-11.2% std=1.88]
  Expert: layer_15[0.8-11.4% std=2.33]
  Expert: layer_16[4.1-11.8% std=2.29]
  Expert: layer_18[2.4-13.3% std=2.55]
  Expert: layer_19[2.7-14.5% std=2.74]
  Expert: layer_20[3.0-12.0% std=2.90]
  Expert: layer_21[2.6-14.6% std=2.88]
Step     482 | Loss: 3.2799 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2066 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.50] | layer_5[4.6-9.0% std=1.10] | layer_11[3.0-12.3% std=2.32] | layer_17[2.9-12.0% std=2.03] | layer_22[0.8-25.7% std=5.37]
  Expert: layer_1[5.8-6.8% std=0.29]
  Expert: layer_2[5.6-6.8% std=0.37]
  Expert: layer_3[4.0-9.3% std=1.74]
  Expert: layer_4[4.6-10.0% std=1.64]
  Expert: layer_6[4.3-7.7% std=1.08]
  Expert: layer_7[3.4-9.8% std=1.92]
  Expert: layer_8[3.9-11.3% std=1.90]
  Expert: layer_9[2.8-12.5% std=2.19]
  Expert: layer_10[3.6-8.5% std=1.44]
  Expert: layer_12[2.8-11.3% std=2.26]
  Expert: layer_13[4.3-10.3% std=1.79]
  Expert: layer_14[3.5-10.5% std=1.83]
  Expert: layer_15[0.9-10.9% std=2.03]
  Expert: layer_16[4.0-11.7% std=2.16]
  Expert: layer_18[2.4-14.8% std=2.81]
  Expert: layer_19[2.7-13.1% std=2.43]
  Expert: layer_20[2.6-12.5% std=2.83]
  Expert: layer_21[2.1-12.7% std=2.74]
Step     483 | Loss: 3.2394 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2671 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.53] | layer_5[4.6-8.8% std=1.12] | layer_11[2.9-10.7% std=2.04] | layer_17[3.9-11.4% std=1.82] | layer_22[1.0-24.4% std=5.09]
  Expert: layer_1[5.9-6.8% std=0.28]
  Expert: layer_2[5.7-6.8% std=0.38]
  Expert: layer_3[3.8-9.1% std=1.70]
  Expert: layer_4[4.6-9.6% std=1.57]
  Expert: layer_6[4.3-7.8% std=1.06]
  Expert: layer_7[3.4-10.6% std=2.00]
  Expert: layer_8[3.8-11.4% std=1.94]
  Expert: layer_9[2.8-12.8% std=2.24]
  Expert: layer_10[3.9-8.7% std=1.42]
  Expert: layer_12[2.8-10.5% std=2.15]
  Expert: layer_13[4.0-10.4% std=1.78]
  Expert: layer_14[3.3-11.8% std=2.10]
  Expert: layer_15[0.9-12.3% std=2.34]
  Expert: layer_16[3.2-12.5% std=2.43]
  Expert: layer_18[2.4-13.2% std=2.52]
  Expert: layer_19[2.9-14.9% std=2.73]
  Expert: layer_20[2.3-11.6% std=2.69]
  Expert: layer_21[2.6-13.5% std=2.84]
Step     484 | Loss: 3.3294 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2671 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.4% std=0.49] | layer_5[4.5-8.7% std=1.08] | layer_11[2.7-11.0% std=2.15] | layer_17[3.3-11.8% std=1.98] | layer_22[1.0-24.7% std=5.14]
  Expert: layer_1[5.9-6.7% std=0.26]
  Expert: layer_2[5.7-6.8% std=0.39]
  Expert: layer_3[3.9-9.1% std=1.75]
  Expert: layer_4[4.7-9.7% std=1.51]
  Expert: layer_6[4.4-7.5% std=1.05]
  Expert: layer_7[3.3-10.5% std=2.04]
  Expert: layer_8[3.4-10.5% std=1.89]
  Expert: layer_9[3.0-12.4% std=2.17]
  Expert: layer_10[3.8-8.5% std=1.41]
  Expert: layer_12[2.7-10.9% std=2.24]
  Expert: layer_13[4.3-10.6% std=1.73]
  Expert: layer_14[3.5-12.9% std=2.30]
  Expert: layer_15[0.8-11.1% std=2.21]
  Expert: layer_16[3.4-12.0% std=2.35]
  Expert: layer_18[2.5-13.5% std=2.49]
  Expert: layer_19[2.7-14.8% std=2.81]
  Expert: layer_20[2.3-11.3% std=2.73]
  Expert: layer_21[2.8-13.2% std=2.79]
Step     485 | Loss: 3.2755 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2667 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.5% std=0.50] | layer_5[4.4-8.7% std=1.06] | layer_11[2.6-11.4% std=2.37] | layer_17[3.1-12.5% std=1.99] | layer_22[0.7-25.2% std=5.28]
  Expert: layer_1[5.9-6.8% std=0.27]
  Expert: layer_2[5.6-6.8% std=0.39]
  Expert: layer_3[3.8-9.2% std=1.78]
  Expert: layer_4[4.7-9.7% std=1.44]
  Expert: layer_6[4.4-7.8% std=1.08]
  Expert: layer_7[3.1-10.0% std=1.92]
  Expert: layer_8[3.6-10.1% std=1.84]
  Expert: layer_9[3.0-12.6% std=2.26]
  Expert: layer_10[3.8-8.2% std=1.40]
  Expert: layer_12[2.7-10.7% std=2.26]
  Expert: layer_13[3.9-10.6% std=1.83]
  Expert: layer_14[3.3-12.1% std=2.15]
  Expert: layer_15[0.8-12.0% std=2.23]
  Expert: layer_16[3.5-12.4% std=2.49]
  Expert: layer_18[2.5-14.1% std=2.68]
  Expert: layer_19[3.2-14.8% std=2.76]
  Expert: layer_20[2.4-12.5% std=2.98]
  Expert: layer_21[2.3-14.9% std=3.03]
Step     486 | Loss: 3.2729 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2640 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.5% std=0.49] | layer_5[4.5-8.6% std=1.04] | layer_11[2.7-11.1% std=2.21] | layer_17[3.2-11.8% std=2.06] | layer_22[0.8-25.4% std=5.35]
  Expert: layer_1[5.8-6.7% std=0.27]
  Expert: layer_2[5.6-6.7% std=0.36]
  Expert: layer_3[3.7-9.0% std=1.78]
  Expert: layer_4[4.6-9.3% std=1.39]
  Expert: layer_6[4.4-7.9% std=1.08]
  Expert: layer_7[2.7-10.6% std=2.07]
  Expert: layer_8[3.6-10.4% std=1.90]
  Expert: layer_9[2.9-12.7% std=2.25]
  Expert: layer_10[3.6-8.3% std=1.40]
  Expert: layer_12[2.8-11.3% std=2.46]
  Expert: layer_13[4.0-10.0% std=1.62]
  Expert: layer_14[3.5-11.4% std=2.09]
  Expert: layer_15[0.8-11.7% std=2.40]
  Expert: layer_16[3.3-12.1% std=2.40]
  Expert: layer_18[1.9-12.6% std=2.53]
  Expert: layer_19[3.0-14.5% std=2.67]
  Expert: layer_20[2.3-11.0% std=2.67]
  Expert: layer_21[2.7-14.6% std=2.77]
Step     487 | Loss: 3.1862 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2666 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.49] | layer_5[4.4-8.5% std=1.06] | layer_11[2.8-12.9% std=2.65] | layer_17[3.0-12.9% std=2.22] | layer_22[1.0-25.5% std=5.34]
  Expert: layer_1[5.8-6.7% std=0.27]
  Expert: layer_2[5.6-6.8% std=0.36]
  Expert: layer_3[3.7-9.1% std=1.79]
  Expert: layer_4[4.7-9.6% std=1.38]
  Expert: layer_6[4.4-7.9% std=1.11]
  Expert: layer_7[3.2-10.2% std=1.92]
  Expert: layer_8[3.8-10.9% std=1.95]
  Expert: layer_9[2.9-12.6% std=2.26]
  Expert: layer_10[3.5-8.2% std=1.39]
  Expert: layer_12[3.1-11.3% std=2.46]
  Expert: layer_13[4.1-10.9% std=1.82]
  Expert: layer_14[3.2-12.2% std=2.16]
  Expert: layer_15[0.7-11.7% std=2.36]
  Expert: layer_16[3.7-11.0% std=2.22]
  Expert: layer_18[2.3-14.7% std=2.84]
  Expert: layer_19[3.3-16.4% std=2.98]
  Expert: layer_20[2.1-12.5% std=2.84]
  Expert: layer_21[2.4-13.3% std=2.89]
Step     488 | Loss: 3.2049 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2667 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.5% std=0.50] | layer_5[4.4-8.1% std=1.03] | layer_11[2.8-10.9% std=2.26] | layer_17[3.3-11.9% std=1.96] | layer_22[0.8-24.1% std=5.00]
  Expert: layer_1[5.7-6.7% std=0.31]
  Expert: layer_2[5.6-6.8% std=0.41]
  Expert: layer_3[3.7-8.9% std=1.74]
  Expert: layer_4[4.4-9.6% std=1.41]
  Expert: layer_6[4.3-7.8% std=1.07]
  Expert: layer_7[3.1-10.5% std=2.01]
  Expert: layer_8[3.8-11.3% std=1.98]
  Expert: layer_9[2.6-13.4% std=2.42]
  Expert: layer_10[3.7-8.2% std=1.35]
  Expert: layer_12[2.9-11.8% std=2.40]
  Expert: layer_13[4.2-10.6% std=1.75]
  Expert: layer_14[3.1-11.6% std=2.08]
  Expert: layer_15[0.8-10.5% std=2.28]
  Expert: layer_16[3.3-12.9% std=2.48]
  Expert: layer_18[2.3-14.6% std=2.98]
  Expert: layer_19[2.6-16.4% std=3.19]
  Expert: layer_20[2.8-11.4% std=2.69]
  Expert: layer_21[2.0-15.2% std=2.97]
Step     489 | Loss: 3.2282 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2666 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.3% std=0.48] | layer_5[4.4-8.3% std=1.03] | layer_11[2.8-11.8% std=2.28] | layer_17[3.1-11.9% std=1.90] | layer_22[1.0-25.3% std=5.29]
  Expert: layer_1[5.6-6.8% std=0.31]
  Expert: layer_2[5.6-6.8% std=0.36]
  Expert: layer_3[3.8-9.2% std=1.86]
  Expert: layer_4[4.3-9.8% std=1.46]
  Expert: layer_6[4.4-8.1% std=1.12]
  Expert: layer_7[3.1-10.2% std=2.00]
  Expert: layer_8[4.0-10.2% std=1.83]
  Expert: layer_9[2.6-13.4% std=2.45]
  Expert: layer_10[3.6-8.0% std=1.29]
  Expert: layer_12[2.8-11.1% std=2.36]
  Expert: layer_13[4.2-11.2% std=1.74]
  Expert: layer_14[3.4-11.1% std=2.05]
  Expert: layer_15[1.1-10.4% std=2.12]
  Expert: layer_16[3.7-11.2% std=2.16]
  Expert: layer_18[2.2-15.5% std=2.95]
  Expert: layer_19[2.8-15.9% std=2.89]
  Expert: layer_20[2.3-11.5% std=2.81]
  Expert: layer_21[2.0-14.4% std=2.95]
Step     490 | Loss: 3.2760 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2668 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.4% std=0.46] | layer_5[4.5-8.5% std=1.05] | layer_11[2.8-11.4% std=2.34] | layer_17[3.6-11.7% std=1.85] | layer_22[0.6-26.9% std=5.66]
  Expert: layer_1[5.6-6.9% std=0.32]
  Expert: layer_2[5.6-6.8% std=0.36]
  Expert: layer_3[3.8-9.6% std=1.93]
  Expert: layer_4[4.2-9.2% std=1.41]
  Expert: layer_6[4.4-7.8% std=1.14]
  Expert: layer_7[3.2-10.4% std=2.00]
  Expert: layer_8[4.2-9.8% std=1.77]
  Expert: layer_9[2.6-13.9% std=2.52]
  Expert: layer_10[3.5-8.4% std=1.38]
  Expert: layer_12[3.0-12.7% std=2.61]
  Expert: layer_13[4.0-10.7% std=1.69]
  Expert: layer_14[3.5-10.5% std=2.07]
  Expert: layer_15[0.9-9.6% std=2.02]
  Expert: layer_16[3.2-12.7% std=2.50]
  Expert: layer_18[2.1-13.9% std=2.69]
  Expert: layer_19[2.8-17.0% std=3.08]
  Expert: layer_20[2.3-10.6% std=2.70]
  Expert: layer_21[2.0-15.2% std=3.06]
Step     491 | Loss: 3.2773 | LR: 2.50e-04 | GradNorm: 0.43 | Tok/s: 2638 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.48] | layer_5[4.4-8.6% std=1.08] | layer_11[2.9-11.5% std=2.26] | layer_17[3.5-12.1% std=1.97] | layer_22[0.6-24.6% std=5.17]
  Expert: layer_1[5.6-6.8% std=0.30]
  Expert: layer_2[5.6-6.8% std=0.35]
  Expert: layer_3[3.7-10.0% std=1.96]
  Expert: layer_4[4.3-9.5% std=1.49]
  Expert: layer_6[4.3-7.9% std=1.19]
  Expert: layer_7[2.8-11.0% std=2.20]
  Expert: layer_8[3.8-10.4% std=1.85]
  Expert: layer_9[2.8-13.3% std=2.45]
  Expert: layer_10[3.8-8.0% std=1.33]
  Expert: layer_12[3.0-12.2% std=2.40]
  Expert: layer_13[4.4-11.4% std=1.75]
  Expert: layer_14[3.5-12.1% std=2.16]
  Expert: layer_15[0.9-11.2% std=2.26]
  Expert: layer_16[3.6-12.4% std=2.27]
  Expert: layer_18[2.4-15.2% std=2.90]
  Expert: layer_19[2.9-14.8% std=2.78]
  Expert: layer_20[2.8-11.9% std=2.81]
  Expert: layer_21[2.6-14.8% std=2.95]
Step     492 | Loss: 3.2560 | LR: 2.50e-04 | GradNorm: 0.48 | Tok/s: 2666 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.49] | layer_5[4.3-8.3% std=1.05] | layer_11[2.8-11.1% std=2.47] | layer_17[2.3-13.8% std=2.31] | layer_22[0.7-27.3% std=5.77]
  Expert: layer_1[5.6-6.7% std=0.29]
  Expert: layer_2[5.6-6.7% std=0.35]
  Expert: layer_3[3.7-9.7% std=1.92]
  Expert: layer_4[4.1-9.5% std=1.54]
  Expert: layer_6[4.4-8.1% std=1.21]
  Expert: layer_7[3.2-10.8% std=2.06]
  Expert: layer_8[3.5-10.3% std=1.94]
  Expert: layer_9[2.7-13.3% std=2.51]
  Expert: layer_10[3.1-8.3% std=1.48]
  Expert: layer_12[3.0-10.8% std=2.70]
  Expert: layer_13[3.8-10.9% std=1.86]
  Expert: layer_14[3.3-10.9% std=2.02]
  Expert: layer_15[0.9-10.7% std=2.28]
  Expert: layer_16[3.1-13.1% std=2.69]
  Expert: layer_18[2.0-13.6% std=2.65]
  Expert: layer_19[2.9-17.8% std=3.30]
  Expert: layer_20[2.4-11.7% std=2.85]
  Expert: layer_21[2.0-15.0% std=3.34]
Step     493 | Loss: 3.1730 | LR: 2.50e-04 | GradNorm: 0.48 | Tok/s: 2663 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.4% std=0.49] | layer_5[4.4-8.2% std=1.07] | layer_11[2.7-10.6% std=2.15] | layer_17[4.0-11.1% std=1.87] | layer_22[0.6-24.8% std=5.29]
  Expert: layer_1[5.5-6.8% std=0.33]
  Expert: layer_2[5.5-6.9% std=0.37]
  Expert: layer_3[3.5-9.8% std=1.88]
  Expert: layer_4[3.9-9.6% std=1.58]
  Expert: layer_6[4.2-7.6% std=1.13]
  Expert: layer_7[2.6-10.8% std=2.12]
  Expert: layer_8[3.5-11.5% std=2.11]
  Expert: layer_9[2.8-13.3% std=2.38]
  Expert: layer_10[3.5-8.8% std=1.42]
  Expert: layer_12[2.6-10.8% std=2.51]
  Expert: layer_13[4.2-10.9% std=1.86]
  Expert: layer_14[3.0-11.2% std=2.14]
  Expert: layer_15[0.7-12.8% std=2.59]
  Expert: layer_16[3.6-12.3% std=2.28]
  Expert: layer_18[1.8-14.1% std=2.94]
  Expert: layer_19[2.5-15.1% std=2.81]
  Expert: layer_20[3.0-14.7% std=2.93]
  Expert: layer_21[2.0-12.2% std=2.77]
Step     494 | Loss: 3.2261 | LR: 2.50e-04 | GradNorm: 0.38 | Tok/s: 2667 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.47] | layer_5[4.4-8.4% std=1.01] | layer_11[3.1-12.2% std=2.40] | layer_17[2.5-13.7% std=2.28] | layer_22[0.7-26.9% std=5.68]
  Expert: layer_1[5.6-6.7% std=0.31]
  Expert: layer_2[5.7-6.8% std=0.29]
  Expert: layer_3[3.6-10.1% std=1.97]
  Expert: layer_4[4.1-9.6% std=1.57]
  Expert: layer_6[4.2-7.9% std=1.23]
  Expert: layer_7[3.3-10.1% std=2.01]
  Expert: layer_8[3.4-10.8% std=2.03]
  Expert: layer_9[3.0-12.9% std=2.36]
  Expert: layer_10[3.2-8.1% std=1.50]
  Expert: layer_12[3.1-10.9% std=2.45]
  Expert: layer_13[4.2-11.0% std=1.88]
  Expert: layer_14[3.4-10.9% std=2.01]
  Expert: layer_15[0.9-9.8% std=2.05]
  Expert: layer_16[3.5-12.1% std=2.24]
  Expert: layer_18[2.0-14.9% std=2.73]
  Expert: layer_19[2.6-14.9% std=2.80]
  Expert: layer_20[2.4-12.5% std=3.08]
  Expert: layer_21[2.2-13.8% std=3.00]
Step     495 | Loss: 3.2316 | LR: 2.50e-04 | GradNorm: 0.39 | Tok/s: 2666 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.2-7.5% std=0.53] | layer_5[4.2-8.1% std=1.04] | layer_11[3.4-11.0% std=2.42] | layer_17[2.1-16.7% std=3.09] | layer_22[0.8-24.3% std=5.13]
  Expert: layer_1[5.6-6.8% std=0.32]
  Expert: layer_2[5.6-6.9% std=0.33]
  Expert: layer_3[3.4-10.1% std=1.92]
  Expert: layer_4[4.0-9.9% std=1.64]
  Expert: layer_6[4.3-8.1% std=1.23]
  Expert: layer_7[3.5-10.4% std=1.91]
  Expert: layer_8[3.5-11.5% std=2.21]
  Expert: layer_9[3.0-13.3% std=2.51]
  Expert: layer_10[2.8-8.9% std=1.62]
  Expert: layer_12[2.7-11.9% std=2.72]
  Expert: layer_13[4.0-11.5% std=2.07]
  Expert: layer_14[2.9-9.2% std=2.02]
  Expert: layer_15[0.8-10.6% std=2.31]
  Expert: layer_16[3.2-13.0% std=2.82]
  Expert: layer_18[2.0-15.2% std=2.96]
  Expert: layer_19[2.9-17.0% std=3.28]
  Expert: layer_20[1.8-11.8% std=3.08]
  Expert: layer_21[2.3-15.8% std=3.46]
Step     496 | Loss: 3.2237 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2634 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.6% std=0.51] | layer_5[4.2-7.8% std=0.99] | layer_11[2.7-10.8% std=2.40] | layer_17[3.1-14.8% std=2.47] | layer_22[0.6-26.7% std=5.63]
  Expert: layer_1[5.6-6.9% std=0.34]
  Expert: layer_2[5.7-7.1% std=0.37]
  Expert: layer_3[3.4-10.0% std=2.04]
  Expert: layer_4[4.0-9.7% std=1.65]
  Expert: layer_6[4.4-8.0% std=1.20]
  Expert: layer_7[3.6-10.5% std=2.07]
  Expert: layer_8[3.9-10.6% std=1.92]
  Expert: layer_9[2.9-13.5% std=2.45]
  Expert: layer_10[3.2-8.8% std=1.47]
  Expert: layer_12[3.1-10.7% std=2.34]
  Expert: layer_13[4.3-10.6% std=1.94]
  Expert: layer_14[3.2-11.4% std=2.17]
  Expert: layer_15[0.7-10.9% std=2.42]
  Expert: layer_16[3.9-12.2% std=2.37]
  Expert: layer_18[2.2-15.6% std=2.89]
  Expert: layer_19[2.6-15.0% std=2.85]
  Expert: layer_20[2.6-11.8% std=2.88]
  Expert: layer_21[1.8-13.0% std=2.90]
Step     497 | Loss: 3.2006 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2664 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.2-7.4% std=0.53] | layer_5[4.3-7.7% std=0.98] | layer_11[3.0-11.5% std=2.29] | layer_17[3.1-12.9% std=2.24] | layer_22[0.7-25.5% std=5.38]
  Expert: layer_1[5.6-6.8% std=0.32]
  Expert: layer_2[5.5-7.1% std=0.36]
  Expert: layer_3[3.5-9.6% std=1.92]
  Expert: layer_4[4.1-10.1% std=1.71]
  Expert: layer_6[4.3-7.9% std=1.23]
  Expert: layer_7[3.0-10.6% std=2.09]
  Expert: layer_8[3.4-12.2% std=2.29]
  Expert: layer_9[2.8-12.8% std=2.37]
  Expert: layer_10[3.1-8.9% std=1.54]
  Expert: layer_12[2.9-10.6% std=2.26]
  Expert: layer_13[4.4-11.1% std=1.90]
  Expert: layer_14[3.2-11.1% std=2.18]
  Expert: layer_15[0.8-10.7% std=2.23]
  Expert: layer_16[3.6-12.5% std=2.30]
  Expert: layer_18[1.9-14.1% std=2.75]
  Expert: layer_19[2.9-14.2% std=2.66]
  Expert: layer_20[2.4-11.9% std=2.85]
  Expert: layer_21[2.1-13.7% std=2.97]
Step     498 | Loss: 3.2372 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2663 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.6% std=0.50] | layer_5[4.4-7.6% std=0.87] | layer_11[2.9-10.9% std=2.27] | layer_17[2.9-12.9% std=2.09] | layer_22[0.6-27.2% std=5.80]
  Expert: layer_1[5.6-6.9% std=0.34]
  Expert: layer_2[5.8-7.0% std=0.35]
  Expert: layer_3[3.4-9.5% std=1.97]
  Expert: layer_4[4.1-9.9% std=1.71]
  Expert: layer_6[4.3-8.2% std=1.28]
  Expert: layer_7[3.1-9.8% std=2.04]
  Expert: layer_8[3.8-11.0% std=2.05]
  Expert: layer_9[2.9-13.4% std=2.41]
  Expert: layer_10[3.1-8.8% std=1.49]
  Expert: layer_12[2.9-11.5% std=2.43]
  Expert: layer_13[4.2-11.0% std=1.85]
  Expert: layer_14[3.3-10.8% std=2.16]
  Expert: layer_15[0.8-10.8% std=2.18]
  Expert: layer_16[3.0-13.9% std=2.65]
  Expert: layer_18[2.2-14.1% std=2.65]
  Expert: layer_19[2.6-14.8% std=2.85]
  Expert: layer_20[2.2-11.7% std=2.95]
  Expert: layer_21[2.2-15.1% std=3.06]
Step     499 | Loss: 3.2487 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2661 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.2-7.5% std=0.52] | layer_5[4.4-8.1% std=0.94] | layer_11[2.7-11.0% std=2.42] | layer_17[2.6-14.0% std=2.31] | layer_22[0.6-26.5% std=5.65]
  Expert: layer_1[5.5-6.9% std=0.35]
  Expert: layer_2[5.7-6.9% std=0.34]
  Expert: layer_3[3.5-9.4% std=2.00]
  Expert: layer_4[4.1-10.0% std=1.71]
  Expert: layer_6[4.3-8.1% std=1.27]
  Expert: layer_7[3.6-10.1% std=1.96]
  Expert: layer_8[3.6-10.9% std=2.10]
  Expert: layer_9[3.2-13.1% std=2.34]
  Expert: layer_10[3.0-9.0% std=1.54]
  Expert: layer_12[2.8-10.8% std=2.43]
  Expert: layer_13[4.2-11.0% std=1.95]
  Expert: layer_14[3.1-11.7% std=2.22]
  Expert: layer_15[0.7-11.2% std=2.37]
  Expert: layer_16[3.3-12.2% std=2.53]
  Expert: layer_18[2.4-15.1% std=2.86]
  Expert: layer_19[2.7-14.6% std=2.86]
  Expert: layer_20[2.4-12.5% std=3.02]
  Expert: layer_21[2.4-16.1% std=3.29]
Step     500 | Loss: 3.2670 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2667 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.2-8.2% std=0.98] | layer_11[2.9-11.3% std=2.59] | layer_17[2.3-16.1% std=2.93] | layer_22[0.6-25.3% std=5.40]
  Expert: layer_1[5.6-7.0% std=0.37]
  Expert: layer_2[5.6-7.0% std=0.38]
  Expert: layer_3[3.4-9.6% std=2.08]
  Expert: layer_4[4.2-10.1% std=1.76]
  Expert: layer_6[4.3-8.2% std=1.27]
  Expert: layer_7[2.9-10.2% std=2.01]
  Expert: layer_8[3.5-11.6% std=2.30]
  Expert: layer_9[3.2-13.0% std=2.40]
  Expert: layer_10[2.8-9.4% std=1.67]
  Expert: layer_12[3.0-11.7% std=2.54]
  Expert: layer_13[4.1-10.4% std=1.80]
  Expert: layer_14[3.1-11.3% std=2.20]
  Expert: layer_15[0.8-11.7% std=2.57]
  Expert: layer_16[3.3-12.7% std=2.51]
  Expert: layer_18[2.2-15.3% std=3.00]
  Expert: layer_19[2.9-16.9% std=3.20]
  Expert: layer_20[2.3-12.7% std=3.00]
  Expert: layer_21[2.2-15.0% std=3.25]
Step     501 | Loss: 3.1949 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2635 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.5% std=0.51] | layer_5[4.2-8.4% std=1.01] | layer_11[3.1-11.0% std=2.33] | layer_17[2.9-15.3% std=2.62] | layer_22[0.6-25.7% std=5.46]
  Expert: layer_1[5.6-7.1% std=0.37]
  Expert: layer_2[5.6-7.0% std=0.39]
  Expert: layer_3[3.3-9.7% std=2.09]
  Expert: layer_4[4.1-10.4% std=1.79]
  Expert: layer_6[4.2-8.2% std=1.25]
  Expert: layer_7[3.4-10.3% std=1.95]
  Expert: layer_8[3.4-12.6% std=2.55]
  Expert: layer_9[3.3-12.8% std=2.32]
  Expert: layer_10[3.0-9.6% std=1.65]
  Expert: layer_12[2.9-11.1% std=2.42]
  Expert: layer_13[4.2-10.4% std=1.82]
  Expert: layer_14[3.1-12.2% std=2.23]
  Expert: layer_15[0.8-12.0% std=2.55]
  Expert: layer_16[3.3-12.7% std=2.57]
  Expert: layer_18[2.3-15.1% std=3.04]
  Expert: layer_19[2.6-17.7% std=3.38]
  Expert: layer_20[2.4-10.9% std=2.73]
  Expert: layer_21[1.8-15.5% std=3.32]
Step     502 | Loss: 3.2447 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2667 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.2-8.4% std=1.01] | layer_11[2.5-11.2% std=2.35] | layer_17[2.9-13.8% std=2.40] | layer_22[0.6-26.7% std=5.68]
  Expert: layer_1[5.6-7.2% std=0.41]
  Expert: layer_2[5.5-7.1% std=0.42]
  Expert: layer_3[3.4-9.7% std=2.10]
  Expert: layer_4[4.0-10.7% std=1.85]
  Expert: layer_6[4.3-7.8% std=1.12]
  Expert: layer_7[2.7-10.0% std=2.09]
  Expert: layer_8[3.2-13.0% std=2.63]
  Expert: layer_9[3.3-12.3% std=2.20]
  Expert: layer_10[3.0-9.9% std=1.62]
  Expert: layer_12[2.9-12.0% std=2.54]
  Expert: layer_13[4.2-10.3% std=1.81]
  Expert: layer_14[3.3-11.6% std=2.12]
  Expert: layer_15[0.7-9.8% std=2.12]
  Expert: layer_16[3.4-11.7% std=2.37]
  Expert: layer_18[2.3-14.5% std=2.88]
  Expert: layer_19[2.8-15.8% std=3.09]
  Expert: layer_20[2.7-10.8% std=2.68]
  Expert: layer_21[2.2-15.6% std=3.29]
Step     503 | Loss: 3.1704 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2670 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.2-8.5% std=1.03] | layer_11[2.9-10.4% std=2.26] | layer_17[2.8-12.9% std=2.34] | layer_22[0.9-26.6% std=5.63]
  Expert: layer_1[5.6-7.1% std=0.38]
  Expert: layer_2[5.4-7.0% std=0.46]
  Expert: layer_3[3.4-9.7% std=2.07]
  Expert: layer_4[3.9-10.5% std=1.84]
  Expert: layer_6[4.3-8.0% std=1.23]
  Expert: layer_7[3.2-9.6% std=1.92]
  Expert: layer_8[2.8-13.4% std=2.73]
  Expert: layer_9[3.3-12.3% std=2.23]
  Expert: layer_10[2.8-9.6% std=1.71]
  Expert: layer_12[2.4-12.0% std=2.82]
  Expert: layer_13[4.0-10.4% std=1.90]
  Expert: layer_14[2.9-10.4% std=2.06]
  Expert: layer_15[0.8-10.4% std=2.23]
  Expert: layer_16[3.4-12.2% std=2.56]
  Expert: layer_18[2.1-13.6% std=2.82]
  Expert: layer_19[2.6-17.3% std=3.36]
  Expert: layer_20[2.3-11.6% std=2.99]
  Expert: layer_21[2.1-15.5% std=3.37]
Step     504 | Loss: 3.2620 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2669 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.5% std=0.51] | layer_5[4.3-8.6% std=0.98] | layer_11[2.4-10.8% std=2.37] | layer_17[3.1-13.6% std=2.32] | layer_22[0.5-29.7% std=6.38]
  Expert: layer_1[5.5-7.3% std=0.43]
  Expert: layer_2[5.6-7.1% std=0.42]
  Expert: layer_3[3.4-10.0% std=2.15]
  Expert: layer_4[3.9-10.2% std=1.87]
  Expert: layer_6[4.3-8.0% std=1.24]
  Expert: layer_7[3.3-9.8% std=2.07]
  Expert: layer_8[2.9-12.7% std=2.55]
  Expert: layer_9[3.3-12.4% std=2.23]
  Expert: layer_10[3.1-9.9% std=1.63]
  Expert: layer_12[2.5-12.5% std=2.73]
  Expert: layer_13[4.0-10.3% std=1.86]
  Expert: layer_14[3.3-11.8% std=2.24]
  Expert: layer_15[0.8-10.0% std=2.18]
  Expert: layer_16[3.5-12.5% std=2.63]
  Expert: layer_18[2.4-14.2% std=2.78]
  Expert: layer_19[2.6-15.6% std=3.01]
  Expert: layer_20[2.3-12.3% std=3.13]
  Expert: layer_21[2.0-16.6% std=3.58]
Step     505 | Loss: 3.1532 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2667 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.4% std=0.50] | layer_5[4.1-9.0% std=1.14] | layer_11[2.5-10.8% std=2.41] | layer_17[3.6-12.9% std=2.09] | layer_22[0.8-28.2% std=6.04]
  Expert: layer_1[5.5-7.3% std=0.44]
  Expert: layer_2[5.4-7.2% std=0.48]
  Expert: layer_3[3.4-9.7% std=1.97]
  Expert: layer_4[3.8-10.5% std=1.91]
  Expert: layer_6[4.4-8.0% std=1.23]
  Expert: layer_7[3.8-9.8% std=1.96]
  Expert: layer_8[3.0-13.5% std=2.72]
  Expert: layer_9[3.1-12.9% std=2.36]
  Expert: layer_10[2.8-9.9% std=1.72]
  Expert: layer_12[2.4-13.1% std=2.84]
  Expert: layer_13[3.9-10.3% std=2.00]
  Expert: layer_14[2.6-10.7% std=2.26]
  Expert: layer_15[0.6-10.8% std=2.23]
  Expert: layer_16[2.8-14.1% std=2.91]
  Expert: layer_18[2.0-14.3% std=2.90]
  Expert: layer_19[2.7-15.1% std=2.90]
  Expert: layer_20[2.4-11.7% std=2.88]
  Expert: layer_21[1.8-15.4% std=3.30]
Step     506 | Loss: 3.1751 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2639 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.4% std=0.49] | layer_5[4.1-9.3% std=1.16] | layer_11[2.4-11.3% std=2.55] | layer_17[3.3-14.6% std=2.56] | layer_22[1.0-29.2% std=6.28]
  Expert: layer_1[5.5-7.3% std=0.44]
  Expert: layer_2[5.6-7.2% std=0.43]
  Expert: layer_3[3.5-9.7% std=2.05]
  Expert: layer_4[3.7-10.4% std=1.93]
  Expert: layer_6[4.5-8.2% std=1.22]
  Expert: layer_7[3.5-9.9% std=2.02]
  Expert: layer_8[3.0-13.7% std=2.74]
  Expert: layer_9[3.3-12.4% std=2.24]
  Expert: layer_10[2.9-9.6% std=1.71]
  Expert: layer_12[2.5-14.0% std=2.91]
  Expert: layer_13[3.9-10.3% std=1.96]
  Expert: layer_14[3.1-11.9% std=2.43]
  Expert: layer_15[0.6-10.4% std=2.30]
  Expert: layer_16[3.1-13.0% std=2.87]
  Expert: layer_18[2.0-13.4% std=2.80]
  Expert: layer_19[2.6-15.6% std=2.95]
  Expert: layer_20[1.8-11.8% std=3.13]
  Expert: layer_21[1.8-16.8% std=3.63]
Step     507 | Loss: 3.1764 | LR: 2.50e-04 | GradNorm: 0.38 | Tok/s: 2663 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.4% std=0.50] | layer_5[3.9-9.4% std=1.20] | layer_11[2.7-11.9% std=2.75] | layer_17[3.3-13.9% std=2.40] | layer_22[0.9-26.6% std=5.68]
  Expert: layer_1[5.4-7.3% std=0.46]
  Expert: layer_2[5.5-7.3% std=0.47]
  Expert: layer_3[3.4-9.9% std=2.07]
  Expert: layer_4[3.7-10.8% std=2.07]
  Expert: layer_6[4.4-8.2% std=1.26]
  Expert: layer_7[2.8-10.0% std=2.08]
  Expert: layer_8[2.7-15.0% std=3.00]
  Expert: layer_9[3.0-12.3% std=2.25]
  Expert: layer_10[3.1-10.1% std=1.78]
  Expert: layer_12[2.2-12.2% std=2.85]
  Expert: layer_13[3.7-10.2% std=2.00]
  Expert: layer_14[3.2-11.4% std=2.27]
  Expert: layer_15[0.7-11.9% std=2.44]
  Expert: layer_16[3.2-13.5% std=2.75]
  Expert: layer_18[2.3-14.3% std=2.94]
  Expert: layer_19[2.8-16.8% std=3.11]
  Expert: layer_20[2.1-12.5% std=3.12]
  Expert: layer_21[1.9-14.8% std=3.14]
Step     508 | Loss: 3.2116 | LR: 2.50e-04 | GradNorm: 0.41 | Tok/s: 2585 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.48] | layer_5[4.1-9.6% std=1.24] | layer_11[2.6-13.2% std=2.77] | layer_17[2.8-13.8% std=2.43] | layer_22[0.5-28.7% std=6.26]
  Expert: layer_1[5.5-7.2% std=0.43]
  Expert: layer_2[5.5-7.2% std=0.43]
  Expert: layer_3[3.4-9.6% std=2.05]
  Expert: layer_4[3.8-10.6% std=2.03]
  Expert: layer_6[4.4-7.9% std=1.19]
  Expert: layer_7[3.3-10.3% std=2.12]
  Expert: layer_8[2.6-14.8% std=2.98]
  Expert: layer_9[3.2-11.7% std=2.12]
  Expert: layer_10[2.7-9.8% std=1.86]
  Expert: layer_12[2.4-12.8% std=2.86]
  Expert: layer_13[4.1-10.7% std=2.04]
  Expert: layer_14[3.0-11.6% std=2.42]
  Expert: layer_15[0.6-10.8% std=2.32]
  Expert: layer_16[3.2-12.2% std=2.63]
  Expert: layer_18[2.2-13.5% std=2.78]
  Expert: layer_19[2.4-15.6% std=3.07]
  Expert: layer_20[1.9-12.2% std=3.11]
  Expert: layer_21[2.0-16.0% std=3.61]
Step     509 | Loss: 3.2247 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2586 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.48] | layer_5[4.2-9.6% std=1.23] | layer_11[2.7-13.1% std=2.91] | layer_17[3.2-13.9% std=2.47] | layer_22[0.6-27.3% std=5.87]
  Expert: layer_1[5.6-7.0% std=0.39]
  Expert: layer_2[5.6-7.2% std=0.40]
  Expert: layer_3[3.6-9.6% std=2.10]
  Expert: layer_4[4.0-10.8% std=2.05]
  Expert: layer_6[4.3-7.9% std=1.22]
  Expert: layer_7[3.0-10.7% std=2.17]
  Expert: layer_8[2.9-13.8% std=2.80]
  Expert: layer_9[3.0-12.6% std=2.35]
  Expert: layer_10[2.7-9.7% std=1.76]
  Expert: layer_12[2.5-13.2% std=3.06]
  Expert: layer_13[4.0-10.4% std=1.99]
  Expert: layer_14[3.1-11.5% std=2.31]
  Expert: layer_15[0.6-10.7% std=2.14]
  Expert: layer_16[3.2-13.4% std=2.75]
  Expert: layer_18[2.3-15.5% std=3.01]
  Expert: layer_19[2.8-14.8% std=2.76]
  Expert: layer_20[1.9-12.8% std=3.09]
  Expert: layer_21[2.3-15.8% std=3.41]
Step     510 | Loss: 3.2254 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2588 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.4% std=0.49] | layer_5[4.2-9.5% std=1.24] | layer_11[2.6-12.9% std=2.79] | layer_17[3.5-14.5% std=2.47] | layer_22[0.9-27.8% std=6.03]
  Expert: layer_1[5.6-7.1% std=0.40]
  Expert: layer_2[5.6-7.2% std=0.40]
  Expert: layer_3[3.5-9.9% std=2.13]
  Expert: layer_4[3.9-10.8% std=2.04]
  Expert: layer_6[4.4-8.3% std=1.31]
  Expert: layer_7[2.9-10.3% std=2.16]
  Expert: layer_8[3.1-13.9% std=2.79]
  Expert: layer_9[3.0-12.9% std=2.38]
  Expert: layer_10[3.0-9.8% std=1.76]
  Expert: layer_12[2.7-12.1% std=2.72]
  Expert: layer_13[3.6-11.3% std=2.24]
  Expert: layer_14[3.0-12.4% std=2.39]
  Expert: layer_15[0.6-12.3% std=2.45]
  Expert: layer_16[3.3-12.0% std=2.51]
  Expert: layer_18[2.3-15.4% std=3.09]
  Expert: layer_19[2.7-15.1% std=2.75]
  Expert: layer_20[2.1-13.0% std=3.26]
  Expert: layer_21[1.8-14.7% std=3.29]
Step     511 | Loss: 3.2543 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2627 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.48] | layer_5[4.4-9.4% std=1.19] | layer_11[2.5-13.0% std=2.63] | layer_17[3.1-13.2% std=2.30] | layer_22[0.6-30.2% std=6.52]
  Expert: layer_1[5.6-7.0% std=0.38]
  Expert: layer_2[5.6-7.1% std=0.38]
  Expert: layer_3[3.5-9.8% std=2.19]
  Expert: layer_4[3.9-10.8% std=2.02]
  Expert: layer_6[4.4-8.2% std=1.30]
  Expert: layer_7[3.5-10.4% std=2.18]
  Expert: layer_8[3.3-12.8% std=2.54]
  Expert: layer_9[3.0-13.1% std=2.39]
  Expert: layer_10[2.9-9.6% std=1.70]
  Expert: layer_12[2.6-12.9% std=2.72]
  Expert: layer_13[3.9-10.7% std=2.11]
  Expert: layer_14[2.9-12.1% std=2.37]
  Expert: layer_15[0.5-10.0% std=2.14]
  Expert: layer_16[3.5-12.4% std=2.41]
  Expert: layer_18[2.1-14.8% std=2.99]
  Expert: layer_19[2.6-14.4% std=2.69]
  Expert: layer_20[2.5-13.5% std=3.18]
  Expert: layer_21[2.2-15.2% std=3.22]
Step     512 | Loss: 3.1888 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2662 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.3% std=0.46] | layer_5[4.4-9.3% std=1.20] | layer_11[2.6-13.5% std=2.86] | layer_17[3.4-14.1% std=2.41] | layer_22[1.1-29.0% std=6.24]
  Expert: layer_1[5.6-7.0% std=0.39]
  Expert: layer_2[5.5-7.3% std=0.44]
  Expert: layer_3[3.5-9.8% std=2.17]
  Expert: layer_4[3.9-10.8% std=2.04]
  Expert: layer_6[4.4-8.1% std=1.29]
  Expert: layer_7[3.3-10.4% std=2.12]
  Expert: layer_8[3.2-13.2% std=2.61]
  Expert: layer_9[2.8-14.2% std=2.67]
  Expert: layer_10[2.9-9.9% std=1.73]
  Expert: layer_12[2.6-12.0% std=2.72]
  Expert: layer_13[3.6-10.8% std=2.15]
  Expert: layer_14[3.4-12.0% std=2.14]
  Expert: layer_15[0.6-12.2% std=2.46]
  Expert: layer_16[3.6-12.5% std=2.55]
  Expert: layer_18[2.3-14.4% std=2.77]
  Expert: layer_19[3.0-15.7% std=2.89]
  Expert: layer_20[2.3-12.7% std=3.20]
  Expert: layer_21[2.1-17.3% std=3.66]
Step     513 | Loss: 3.1441 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2663 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.3% std=0.44] | layer_5[4.2-9.4% std=1.26] | layer_11[2.7-12.1% std=2.76] | layer_17[3.4-13.5% std=2.30] | layer_22[1.0-27.0% std=5.81]
  Expert: layer_1[5.6-7.0% std=0.40]
  Expert: layer_2[5.4-7.4% std=0.49]
  Expert: layer_3[3.4-9.8% std=2.16]
  Expert: layer_4[3.8-10.6% std=2.06]
  Expert: layer_6[4.4-8.4% std=1.24]
  Expert: layer_7[3.5-10.7% std=2.11]
  Expert: layer_8[3.2-13.7% std=2.73]
  Expert: layer_9[2.5-14.7% std=2.83]
  Expert: layer_10[2.9-10.1% std=1.76]
  Expert: layer_12[2.5-11.7% std=2.87]
  Expert: layer_13[3.3-10.8% std=2.16]
  Expert: layer_14[2.9-11.5% std=2.25]
  Expert: layer_15[0.6-11.4% std=2.29]
  Expert: layer_16[3.4-13.5% std=2.91]
  Expert: layer_18[1.8-15.2% std=3.11]
  Expert: layer_19[2.7-16.4% std=3.16]
  Expert: layer_20[2.2-12.8% std=3.25]
  Expert: layer_21[1.8-16.9% std=3.58]
Step     514 | Loss: 3.2228 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2665 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.5% std=0.48] | layer_5[4.3-9.3% std=1.15] | layer_11[2.6-12.4% std=2.86] | layer_17[3.0-15.7% std=2.71] | layer_22[0.7-29.6% std=6.33]
  Expert: layer_1[5.5-6.9% std=0.39]
  Expert: layer_2[5.6-7.2% std=0.44]
  Expert: layer_3[3.4-10.0% std=2.29]
  Expert: layer_4[4.0-11.2% std=2.07]
  Expert: layer_6[4.3-8.4% std=1.26]
  Expert: layer_7[3.5-11.0% std=2.25]
  Expert: layer_8[3.5-11.9% std=2.41]
  Expert: layer_9[2.6-15.8% std=3.04]
  Expert: layer_10[2.9-9.8% std=1.74]
  Expert: layer_12[2.9-12.3% std=2.82]
  Expert: layer_13[3.4-11.0% std=2.14]
  Expert: layer_14[3.1-12.6% std=2.46]
  Expert: layer_15[0.6-10.3% std=2.23]
  Expert: layer_16[3.4-13.6% std=2.86]
  Expert: layer_18[2.0-15.2% std=2.98]
  Expert: layer_19[2.4-16.8% std=3.18]
  Expert: layer_20[2.2-12.0% std=3.28]
  Expert: layer_21[2.0-17.1% std=3.64]
Step     515 | Loss: 3.1966 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2666 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.3% std=0.45] | layer_5[4.3-9.2% std=1.19] | layer_11[2.9-12.4% std=2.83] | layer_17[3.2-15.0% std=2.61] | layer_22[1.0-28.2% std=6.09]
  Expert: layer_1[5.7-7.0% std=0.39]
  Expert: layer_2[5.4-7.2% std=0.48]
  Expert: layer_3[3.4-9.9% std=2.27]
  Expert: layer_4[3.9-10.5% std=2.03]
  Expert: layer_6[4.2-8.6% std=1.24]
  Expert: layer_7[3.3-10.4% std=2.10]
  Expert: layer_8[3.5-13.1% std=2.60]
  Expert: layer_9[2.6-15.7% std=3.00]
  Expert: layer_10[2.9-9.7% std=1.77]
  Expert: layer_12[2.8-12.3% std=2.89]
  Expert: layer_13[3.6-11.3% std=2.25]
  Expert: layer_14[3.3-11.4% std=2.23]
  Expert: layer_15[0.4-12.4% std=2.49]
  Expert: layer_16[3.6-14.1% std=2.75]
  Expert: layer_18[2.2-15.3% std=3.05]
  Expert: layer_19[2.5-17.8% std=3.38]
  Expert: layer_20[2.1-12.6% std=3.18]
  Expert: layer_21[2.0-16.2% std=3.56]
Step     516 | Loss: 3.1556 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2630 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.45] | layer_5[4.4-8.9% std=1.09] | layer_11[2.8-11.4% std=2.55] | layer_17[3.3-13.8% std=2.28] | layer_22[1.1-27.7% std=5.94]
  Expert: layer_1[5.6-7.0% std=0.40]
  Expert: layer_2[5.5-7.1% std=0.44]
  Expert: layer_3[3.4-9.6% std=2.26]
  Expert: layer_4[3.9-10.4% std=1.97]
  Expert: layer_6[4.3-8.7% std=1.21]
  Expert: layer_7[3.5-10.9% std=2.12]
  Expert: layer_8[3.7-11.8% std=2.39]
  Expert: layer_9[2.5-15.8% std=3.00]
  Expert: layer_10[3.0-9.5% std=1.63]
  Expert: layer_12[2.5-12.6% std=2.90]
  Expert: layer_13[3.5-10.9% std=2.10]
  Expert: layer_14[2.8-11.4% std=2.38]
  Expert: layer_15[0.5-10.3% std=2.23]
  Expert: layer_16[3.5-14.2% std=2.95]
  Expert: layer_18[2.1-16.6% std=3.25]
  Expert: layer_19[2.4-16.0% std=3.04]
  Expert: layer_20[2.0-13.6% std=3.25]
  Expert: layer_21[2.1-16.7% std=3.43]
Step     517 | Loss: 3.1246 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2663 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.6% std=0.47] | layer_5[4.4-8.8% std=1.08] | layer_11[2.8-12.1% std=2.75] | layer_17[3.1-15.6% std=2.68] | layer_22[1.0-27.4% std=5.91]
  Expert: layer_1[5.6-6.9% std=0.38]
  Expert: layer_2[5.6-7.1% std=0.44]
  Expert: layer_3[3.3-10.0% std=2.29]
  Expert: layer_4[3.9-10.7% std=1.99]
  Expert: layer_6[4.2-8.7% std=1.20]
  Expert: layer_7[3.8-10.5% std=2.01]
  Expert: layer_8[3.6-11.9% std=2.39]
  Expert: layer_9[2.6-16.0% std=3.05]
  Expert: layer_10[2.9-9.2% std=1.61]
  Expert: layer_12[2.6-11.8% std=2.82]
  Expert: layer_13[3.2-11.1% std=2.25]
  Expert: layer_14[3.1-12.3% std=2.52]
  Expert: layer_15[0.7-10.5% std=2.28]
  Expert: layer_16[3.0-13.6% std=2.91]
  Expert: layer_18[2.0-15.9% std=3.13]
  Expert: layer_19[2.4-16.7% std=3.15]
  Expert: layer_20[1.9-12.6% std=3.20]
  Expert: layer_21[1.9-16.4% std=3.47]
Step     518 | Loss: 3.1615 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2666 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.6% std=0.49] | layer_5[4.6-8.9% std=1.12] | layer_11[2.5-11.7% std=2.71] | layer_17[2.8-15.1% std=2.63] | layer_22[0.9-29.2% std=6.27]
  Expert: layer_1[5.6-6.9% std=0.38]
  Expert: layer_2[5.6-6.9% std=0.44]
  Expert: layer_3[3.3-10.4% std=2.31]
  Expert: layer_4[4.0-10.5% std=1.94]
  Expert: layer_6[4.3-8.4% std=1.16]
  Expert: layer_7[3.1-10.7% std=2.15]
  Expert: layer_8[3.5-11.7% std=2.30]
  Expert: layer_9[2.7-16.0% std=3.01]
  Expert: layer_10[2.8-8.9% std=1.59]
  Expert: layer_12[2.6-12.5% std=2.81]
  Expert: layer_13[3.5-10.7% std=2.14]
  Expert: layer_14[2.9-12.3% std=2.40]
  Expert: layer_15[0.6-9.6% std=2.19]
  Expert: layer_16[3.6-12.6% std=2.62]
  Expert: layer_18[2.0-16.1% std=3.09]
  Expert: layer_19[2.4-16.0% std=3.11]
  Expert: layer_20[2.3-12.0% std=3.16]
  Expert: layer_21[1.9-16.0% std=3.33]
Step     519 | Loss: 3.1250 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2662 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.49] | layer_5[4.5-9.2% std=1.17] | layer_11[2.8-11.7% std=2.68] | layer_17[2.6-15.4% std=2.85] | layer_22[0.8-30.7% std=6.69]
  Expert: layer_1[5.5-6.8% std=0.36]
  Expert: layer_2[5.5-7.0% std=0.48]
  Expert: layer_3[3.3-10.3% std=2.32]
  Expert: layer_4[3.8-10.8% std=1.99]
  Expert: layer_6[4.2-8.6% std=1.18]
  Expert: layer_7[3.4-10.6% std=2.06]
  Expert: layer_8[3.4-12.4% std=2.46]
  Expert: layer_9[2.7-15.1% std=2.81]
  Expert: layer_10[2.7-8.6% std=1.63]
  Expert: layer_12[2.6-13.9% std=3.16]
  Expert: layer_13[3.7-10.9% std=2.13]
  Expert: layer_14[2.9-11.3% std=2.35]
  Expert: layer_15[0.5-10.3% std=2.24]
  Expert: layer_16[3.4-12.8% std=2.68]
  Expert: layer_18[1.9-16.1% std=3.18]
  Expert: layer_19[2.4-17.3% std=3.31]
  Expert: layer_20[2.4-12.9% std=3.29]
  Expert: layer_21[1.7-16.1% std=3.42]
Step     520 | Loss: 3.1555 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2661 | Mem: 65.6GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.51] | layer_5[4.5-9.1% std=1.14] | layer_11[2.8-11.8% std=2.74] | layer_17[2.9-15.8% std=2.69] | layer_22[0.9-29.4% std=6.28]
  Expert: layer_1[5.7-6.8% std=0.37]
  Expert: layer_2[5.6-7.0% std=0.48]
  Expert: layer_3[3.3-10.3% std=2.27]
  Expert: layer_4[3.7-11.0% std=2.00]
  Expert: layer_6[4.2-8.8% std=1.23]
  Expert: layer_7[3.2-10.6% std=2.15]
  Expert: layer_8[3.4-12.6% std=2.53]
  Expert: layer_9[2.7-15.7% std=2.97]
  Expert: layer_10[2.8-9.1% std=1.68]
  Expert: layer_12[2.5-12.8% std=3.04]
  Expert: layer_13[3.4-11.0% std=2.31]
  Expert: layer_14[3.2-12.4% std=2.38]
  Expert: layer_15[0.7-12.0% std=2.59]
  Expert: layer_16[3.3-13.1% std=2.81]
  Expert: layer_18[1.9-15.3% std=3.04]
  Expert: layer_19[2.5-19.1% std=3.66]
  Expert: layer_20[2.3-13.5% std=3.25]
  Expert: layer_21[1.8-16.3% std=3.50]
Per-layer gamma: 0.0001 ~ 0.0005 (sqrt, 23 MoE layers)

============================================================
학습 시작 (step=520, epoch=0)
============================================================
Resume position: global_idx=220949, block=2, local_idx=21269

=== Epoch 1/1 ===
Step     521 | Loss: 3.2021 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 1131 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.6% std=0.51] | layer_5[4.7-9.0% std=1.06] | layer_11[2.6-12.1% std=2.65] | layer_17[2.7-16.2% std=2.85] | layer_22[0.8-29.7% std=6.37]
  Expert: layer_1[5.6-6.8% std=0.36]
  Expert: layer_2[5.5-7.0% std=0.46]
  Expert: layer_3[3.4-10.4% std=2.31]
  Expert: layer_4[3.8-10.8% std=1.98]
  Expert: layer_6[4.3-8.9% std=1.22]
  Expert: layer_7[3.2-11.0% std=2.33]
  Expert: layer_8[3.2-11.7% std=2.50]
  Expert: layer_9[2.7-15.6% std=2.97]
  Expert: layer_10[3.1-9.1% std=1.63]
  Expert: layer_12[2.7-13.2% std=2.84]
  Expert: layer_13[3.7-11.1% std=2.20]
  Expert: layer_14[3.0-12.9% std=2.51]
  Expert: layer_15[0.5-10.5% std=2.44]
  Expert: layer_16[3.6-13.9% std=2.91]
  Expert: layer_18[1.8-18.2% std=3.57]
  Expert: layer_19[2.4-15.2% std=2.93]
  Expert: layer_20[2.5-13.1% std=3.31]
  Expert: layer_21[2.2-16.9% std=3.49]
Step     522 | Loss: 3.2130 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 1070 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.6% std=0.51] | layer_5[4.8-9.0% std=1.08] | layer_11[2.7-11.0% std=2.62] | layer_17[2.5-15.1% std=2.73] | layer_22[0.7-31.9% std=6.93]
  Expert: layer_1[5.6-6.6% std=0.33]
  Expert: layer_2[5.6-6.9% std=0.46]
  Expert: layer_3[3.4-10.1% std=2.26]
  Expert: layer_4[3.9-10.7% std=1.92]
  Expert: layer_6[4.3-9.2% std=1.24]
  Expert: layer_7[3.6-10.5% std=2.06]
  Expert: layer_8[3.2-11.9% std=2.54]
  Expert: layer_9[2.7-15.4% std=2.93]
  Expert: layer_10[2.9-9.0% std=1.70]
  Expert: layer_12[2.6-14.3% std=3.21]
  Expert: layer_13[3.5-10.7% std=2.07]
  Expert: layer_14[2.9-12.5% std=2.45]
  Expert: layer_15[0.6-11.3% std=2.49]
  Expert: layer_16[3.3-13.9% std=2.86]
  Expert: layer_18[1.8-16.1% std=3.19]
  Expert: layer_19[2.5-17.7% std=3.40]
  Expert: layer_20[2.6-12.5% std=3.12]
  Expert: layer_21[2.0-16.4% std=3.48]
Step     523 | Loss: 3.1574 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 1056 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.4% std=0.49] | layer_5[4.7-9.3% std=1.20] | layer_11[2.9-13.5% std=2.91] | layer_17[3.0-17.5% std=3.20] | layer_22[1.1-28.7% std=6.18]
  Expert: layer_1[5.7-7.0% std=0.32]
  Expert: layer_2[5.5-7.0% std=0.47]
  Expert: layer_3[3.4-9.9% std=2.17]
  Expert: layer_4[3.8-11.3% std=1.94]
  Expert: layer_6[4.6-9.1% std=1.20]
  Expert: layer_7[3.4-10.2% std=1.98]
  Expert: layer_8[3.1-13.9% std=2.90]
  Expert: layer_9[2.9-14.4% std=2.76]
  Expert: layer_10[2.8-9.4% std=1.80]
  Expert: layer_12[2.9-12.0% std=2.97]
  Expert: layer_13[3.4-11.9% std=2.31]
  Expert: layer_14[3.1-13.7% std=2.69]
  Expert: layer_15[0.5-10.8% std=2.50]
  Expert: layer_16[3.0-12.8% std=2.65]
  Expert: layer_18[1.9-16.4% std=3.23]
  Expert: layer_19[2.7-16.9% std=3.21]
  Expert: layer_20[1.8-13.3% std=3.18]
  Expert: layer_21[1.9-15.1% std=3.39]
Step     524 | Loss: 3.1297 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 1050 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.50] | layer_5[4.8-9.1% std=1.11] | layer_11[2.6-11.8% std=2.65] | layer_17[3.1-15.0% std=2.61] | layer_22[0.9-29.1% std=6.22]
  Expert: layer_1[5.8-6.8% std=0.31]
  Expert: layer_2[5.6-7.0% std=0.44]
  Expert: layer_3[3.4-10.5% std=2.22]
  Expert: layer_4[3.8-11.1% std=1.87]
  Expert: layer_6[4.6-9.2% std=1.16]
  Expert: layer_7[3.4-10.8% std=2.09]
  Expert: layer_8[3.0-12.9% std=2.70]
  Expert: layer_9[2.8-14.5% std=2.76]
  Expert: layer_10[2.9-9.4% std=1.72]
  Expert: layer_12[2.6-12.5% std=2.89]
  Expert: layer_13[3.6-10.6% std=2.05]
  Expert: layer_14[2.5-13.1% std=2.56]
  Expert: layer_15[0.7-10.8% std=2.43]
  Expert: layer_16[2.8-13.1% std=2.73]
  Expert: layer_18[1.7-15.0% std=2.97]
  Expert: layer_19[2.5-16.0% std=3.02]
  Expert: layer_20[2.4-11.6% std=2.99]
  Expert: layer_21[2.0-16.8% std=3.51]
Step     525 | Loss: 3.2000 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 1061 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.5% std=0.50] | layer_5[4.7-9.4% std=1.14] | layer_11[3.0-12.5% std=2.75] | layer_17[3.1-16.5% std=2.95] | layer_22[0.7-30.0% std=6.49]
  Expert: layer_1[5.8-6.8% std=0.28]
  Expert: layer_2[5.6-7.0% std=0.47]
  Expert: layer_3[3.4-10.6% std=2.29]
  Expert: layer_4[4.0-11.3% std=1.88]
  Expert: layer_6[4.5-9.3% std=1.25]
  Expert: layer_7[3.0-10.7% std=2.04]
  Expert: layer_8[3.1-13.3% std=2.74]
  Expert: layer_9[2.9-14.5% std=2.82]
  Expert: layer_10[3.0-9.0% std=1.71]
  Expert: layer_12[2.8-11.9% std=2.95]
  Expert: layer_13[3.5-10.9% std=2.09]
  Expert: layer_14[3.0-12.4% std=2.44]
  Expert: layer_15[0.8-12.1% std=2.61]
  Expert: layer_16[2.9-13.5% std=2.77]
  Expert: layer_18[1.7-14.0% std=2.92]
  Expert: layer_19[2.7-17.9% std=3.36]
  Expert: layer_20[2.1-14.7% std=3.39]
  Expert: layer_21[2.1-18.3% std=3.76]
Step     526 | Loss: 3.1816 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 1078 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.47] | layer_5[4.7-9.3% std=1.08] | layer_11[2.1-13.3% std=2.84] | layer_17[3.0-16.7% std=2.95] | layer_22[0.6-30.2% std=6.48]
  Expert: layer_1[5.8-6.7% std=0.28]
  Expert: layer_2[5.5-6.9% std=0.43]
  Expert: layer_3[3.3-10.3% std=2.24]
  Expert: layer_4[3.9-10.9% std=1.89]
  Expert: layer_6[4.4-8.9% std=1.17]
  Expert: layer_7[3.1-10.7% std=2.32]
  Expert: layer_8[3.6-12.6% std=2.58]
  Expert: layer_9[2.8-14.0% std=2.69]
  Expert: layer_10[3.0-8.9% std=1.70]
  Expert: layer_12[3.3-13.3% std=2.91]
  Expert: layer_13[3.2-10.7% std=2.04]
  Expert: layer_14[3.1-12.5% std=2.51]
  Expert: layer_15[0.7-10.0% std=2.32]
  Expert: layer_16[3.1-13.6% std=2.85]
  Expert: layer_18[1.7-15.9% std=3.08]
  Expert: layer_19[2.3-16.8% std=3.15]
  Expert: layer_20[2.1-14.6% std=3.35]
  Expert: layer_21[2.1-16.5% std=3.41]
Step     527 | Loss: 3.0879 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 1107 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.4% std=0.47] | layer_5[4.7-9.5% std=1.19] | layer_11[2.2-12.3% std=2.75] | layer_17[2.8-14.9% std=2.59] | layer_22[1.0-29.6% std=6.37]
  Expert: layer_1[5.7-6.7% std=0.29]
  Expert: layer_2[5.6-6.9% std=0.43]
  Expert: layer_3[3.3-10.1% std=2.18]
  Expert: layer_4[3.8-10.8% std=1.92]
  Expert: layer_6[4.4-8.9% std=1.19]
  Expert: layer_7[3.3-10.7% std=2.28]
  Expert: layer_8[3.6-12.7% std=2.63]
  Expert: layer_9[2.7-14.4% std=2.74]
  Expert: layer_10[2.9-9.1% std=1.77]
  Expert: layer_12[3.2-12.4% std=2.87]
  Expert: layer_13[3.3-10.4% std=2.11]
  Expert: layer_14[3.0-12.2% std=2.44]
  Expert: layer_15[0.7-10.8% std=2.36]
  Expert: layer_16[3.0-13.1% std=2.75]
  Expert: layer_18[1.8-15.6% std=3.08]
  Expert: layer_19[2.5-17.5% std=3.31]
  Expert: layer_20[2.3-13.2% std=3.07]
  Expert: layer_21[1.8-14.8% std=3.30]
Step     528 | Loss: 3.1932 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 1128 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.3% std=0.46] | layer_5[4.7-9.3% std=1.12] | layer_11[2.5-11.5% std=2.58] | layer_17[3.5-14.7% std=2.48] | layer_22[0.8-29.6% std=6.36]
  Expert: layer_1[5.8-6.7% std=0.27]
  Expert: layer_2[5.7-6.9% std=0.41]
  Expert: layer_3[3.4-10.0% std=2.18]
  Expert: layer_4[3.7-10.8% std=1.96]
  Expert: layer_6[4.3-8.7% std=1.15]
  Expert: layer_7[2.9-10.8% std=2.34]
  Expert: layer_8[3.6-13.3% std=2.69]
  Expert: layer_9[2.8-14.6% std=2.73]
  Expert: layer_10[3.2-8.9% std=1.72]
  Expert: layer_12[3.2-12.2% std=2.77]
  Expert: layer_13[3.4-10.7% std=2.10]
  Expert: layer_14[2.8-12.8% std=2.66]
  Expert: layer_15[0.7-11.6% std=2.52]
  Expert: layer_16[3.3-13.8% std=2.83]
  Expert: layer_18[1.9-15.3% std=3.03]
  Expert: layer_19[2.6-18.4% std=3.49]
  Expert: layer_20[2.2-13.8% std=3.25]
  Expert: layer_21[2.2-15.7% std=3.31]
Step     529 | Loss: 3.1887 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 1109 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.4% std=0.47] | layer_5[4.7-9.0% std=1.08] | layer_11[2.5-12.7% std=2.70] | layer_17[3.3-14.5% std=2.41] | layer_22[0.6-29.7% std=6.38]
  Expert: layer_1[5.7-6.7% std=0.27]
  Expert: layer_2[5.7-6.9% std=0.40]
  Expert: layer_3[3.3-10.0% std=2.21]
  Expert: layer_4[3.6-10.9% std=1.97]
  Expert: layer_6[4.3-8.4% std=1.12]
  Expert: layer_7[2.8-11.1% std=2.39]
  Expert: layer_8[3.5-13.7% std=2.71]
  Expert: layer_9[2.6-14.3% std=2.74]
  Expert: layer_10[3.3-9.2% std=1.74]
  Expert: layer_12[3.1-11.6% std=2.71]
  Expert: layer_13[3.6-11.0% std=2.03]
  Expert: layer_14[3.1-13.6% std=2.69]
  Expert: layer_15[0.8-11.9% std=2.41]
  Expert: layer_16[3.4-13.3% std=2.70]
  Expert: layer_18[1.8-15.8% std=3.08]
  Expert: layer_19[2.7-17.7% std=3.33]
  Expert: layer_20[2.5-13.9% std=3.25]
  Expert: layer_21[2.4-15.8% std=3.24]
Step     530 | Loss: 3.1225 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 1111 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.3% std=0.45] | layer_5[4.7-9.2% std=1.14] | layer_11[2.6-12.8% std=2.81] | layer_17[2.9-14.3% std=2.41] | layer_22[0.9-29.3% std=6.28]
  Expert: layer_1[5.7-6.8% std=0.32]
  Expert: layer_2[5.7-7.1% std=0.40]
  Expert: layer_3[3.3-9.9% std=2.17]
  Expert: layer_4[3.6-10.8% std=1.92]
  Expert: layer_6[4.2-8.6% std=1.16]
  Expert: layer_7[2.8-11.1% std=2.29]
  Expert: layer_8[3.7-14.5% std=2.89]
  Expert: layer_9[2.4-14.3% std=2.72]
  Expert: layer_10[3.1-9.5% std=1.83]
  Expert: layer_12[3.2-11.8% std=2.75]
  Expert: layer_13[3.3-10.9% std=2.16]
  Expert: layer_14[3.0-12.5% std=2.53]
  Expert: layer_15[0.7-10.7% std=2.24]
  Expert: layer_16[3.0-13.6% std=2.84]
  Expert: layer_18[1.8-15.6% std=3.05]
  Expert: layer_19[2.7-17.8% std=3.40]
  Expert: layer_20[2.3-14.0% std=3.34]
  Expert: layer_21[2.1-17.2% std=3.53]
Step     531 | Loss: 3.1334 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 1532 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.2-7.3% std=0.46] | layer_5[4.7-9.5% std=1.20] | layer_11[2.9-13.0% std=2.76] | layer_17[3.0-13.9% std=2.37] | layer_22[0.8-29.3% std=6.31]
  Expert: layer_1[5.8-6.8% std=0.27]
  Expert: layer_2[5.7-7.1% std=0.41]
  Expert: layer_3[3.4-10.1% std=2.23]
  Expert: layer_4[3.6-10.8% std=1.94]
  Expert: layer_6[4.2-8.7% std=1.17]
  Expert: layer_7[3.1-11.1% std=2.29]
  Expert: layer_8[3.2-14.2% std=2.87]
  Expert: layer_9[2.6-13.8% std=2.64]
  Expert: layer_10[2.9-9.6% std=1.84]
  Expert: layer_12[3.0-11.7% std=2.77]
  Expert: layer_13[3.5-11.3% std=2.21]
  Expert: layer_14[2.8-12.8% std=2.59]
  Expert: layer_15[0.7-12.3% std=2.38]
  Expert: layer_16[3.4-13.3% std=2.71]
  Expert: layer_18[1.9-16.5% std=3.18]
  Expert: layer_19[2.7-17.1% std=3.28]
  Expert: layer_20[2.2-12.9% std=3.32]
  Expert: layer_21[2.0-16.7% std=3.58]
Step     532 | Loss: 3.1443 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 1703 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.3% std=0.46] | layer_5[4.6-8.9% std=1.07] | layer_11[2.3-12.7% std=2.70] | layer_17[3.0-14.4% std=2.45] | layer_22[0.7-30.9% std=6.68]
  Expert: layer_1[5.8-6.8% std=0.27]
  Expert: layer_2[5.8-7.1% std=0.40]
  Expert: layer_3[3.5-10.5% std=2.29]
  Expert: layer_4[3.6-10.8% std=2.00]
  Expert: layer_6[4.1-8.3% std=1.11]
  Expert: layer_7[2.9-11.0% std=2.40]
  Expert: layer_8[3.2-13.7% std=2.84]
  Expert: layer_9[2.5-13.0% std=2.48]
  Expert: layer_10[3.0-9.1% std=1.70]
  Expert: layer_12[2.9-12.5% std=2.69]
  Expert: layer_13[3.4-10.6% std=1.96]
  Expert: layer_14[2.9-12.8% std=2.58]
  Expert: layer_15[0.7-9.7% std=2.08]
  Expert: layer_16[3.0-13.5% std=2.86]
  Expert: layer_18[1.7-17.5% std=3.40]
  Expert: layer_19[2.7-16.1% std=2.98]
  Expert: layer_20[2.1-13.6% std=3.57]
  Expert: layer_21[2.0-17.7% std=3.56]
Step     533 | Loss: 3.0844 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 1107 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.2-7.2% std=0.47] | layer_5[4.8-8.6% std=1.07] | layer_11[2.4-12.4% std=2.71] | layer_17[3.3-14.5% std=2.42] | layer_22[0.9-30.0% std=6.44]
  Expert: layer_1[5.8-6.8% std=0.28]
  Expert: layer_2[5.7-7.3% std=0.40]
  Expert: layer_3[3.5-9.9% std=2.20]
  Expert: layer_4[3.5-10.7% std=1.96]
  Expert: layer_6[4.3-8.8% std=1.18]
  Expert: layer_7[3.2-10.6% std=2.17]
  Expert: layer_8[3.3-14.2% std=2.89]
  Expert: layer_9[2.3-13.9% std=2.64]
  Expert: layer_10[2.8-9.7% std=1.79]
  Expert: layer_12[2.4-11.6% std=2.87]
  Expert: layer_13[3.2-10.6% std=2.06]
  Expert: layer_14[2.8-12.5% std=2.48]
  Expert: layer_15[0.8-11.7% std=2.38]
  Expert: layer_16[2.9-12.9% std=2.77]
  Expert: layer_18[1.6-15.7% std=3.04]
  Expert: layer_19[2.8-19.3% std=3.63]
  Expert: layer_20[2.3-13.1% std=3.22]
  Expert: layer_21[2.1-16.3% std=3.45]
Step     534 | Loss: 3.2029 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 1316 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.3% std=0.45] | layer_5[4.7-8.8% std=1.07] | layer_11[2.8-13.2% std=2.73] | layer_17[3.4-14.2% std=2.48] | layer_22[0.6-28.7% std=6.25]
  Expert: layer_1[5.8-6.7% std=0.23]
  Expert: layer_2[5.7-7.1% std=0.39]
  Expert: layer_3[3.6-10.2% std=2.33]
  Expert: layer_4[3.6-10.8% std=1.94]
  Expert: layer_6[4.2-8.5% std=1.18]
  Expert: layer_7[3.2-10.8% std=2.29]
  Expert: layer_8[3.0-14.5% std=2.98]
  Expert: layer_9[2.3-13.9% std=2.68]
  Expert: layer_10[3.1-9.4% std=1.73]
  Expert: layer_12[2.5-12.0% std=2.78]
  Expert: layer_13[3.5-10.7% std=1.98]
  Expert: layer_14[3.2-11.7% std=2.37]
  Expert: layer_15[0.8-11.2% std=2.22]
  Expert: layer_16[3.2-13.6% std=2.62]
  Expert: layer_18[1.8-16.9% std=3.34]
  Expert: layer_19[2.6-16.7% std=3.08]
  Expert: layer_20[2.3-13.4% std=3.34]
  Expert: layer_21[1.9-15.8% std=3.44]
Step     535 | Loss: 3.1917 | LR: 2.50e-04 | GradNorm: 0.39 | Tok/s: 1111 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.3% std=0.44] | layer_5[4.7-8.4% std=0.99] | layer_11[2.3-11.6% std=2.61] | layer_17[3.3-14.0% std=2.31] | layer_22[0.8-30.4% std=6.54]
  Expert: layer_1[5.7-6.7% std=0.25]
  Expert: layer_2[5.8-7.1% std=0.35]
  Expert: layer_3[3.5-10.5% std=2.31]
  Expert: layer_4[3.6-10.3% std=1.98]
  Expert: layer_6[4.2-8.5% std=1.18]
  Expert: layer_7[3.2-10.7% std=2.42]
  Expert: layer_8[3.3-13.8% std=2.94]
  Expert: layer_9[2.2-13.8% std=2.64]
  Expert: layer_10[3.4-9.0% std=1.58]
  Expert: layer_12[2.9-12.1% std=2.80]
  Expert: layer_13[3.4-9.8% std=1.78]
  Expert: layer_14[3.4-12.7% std=2.41]
  Expert: layer_15[0.7-9.6% std=2.18]
  Expert: layer_16[3.1-13.3% std=2.86]
  Expert: layer_18[1.8-17.4% std=3.39]
  Expert: layer_19[2.7-16.5% std=3.05]
  Expert: layer_20[2.0-13.3% std=3.28]
  Expert: layer_21[2.2-17.7% std=3.57]
Step     536 | Loss: 3.1236 | LR: 2.50e-04 | GradNorm: 0.42 | Tok/s: 1107 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.2% std=0.41] | layer_5[4.9-8.6% std=1.03] | layer_11[2.5-11.8% std=2.78] | layer_17[3.4-15.8% std=2.67] | layer_22[0.6-29.2% std=6.28]
  Expert: layer_1[5.8-6.6% std=0.21]
  Expert: layer_2[5.8-7.1% std=0.34]
  Expert: layer_3[3.4-10.3% std=2.27]
  Expert: layer_4[3.6-10.5% std=1.95]
  Expert: layer_6[4.3-8.5% std=1.19]
  Expert: layer_7[3.2-10.7% std=2.26]
  Expert: layer_8[3.3-13.9% std=2.91]
  Expert: layer_9[2.3-13.6% std=2.64]
  Expert: layer_10[3.2-9.6% std=1.72]
  Expert: layer_12[2.4-12.3% std=2.94]
  Expert: layer_13[3.3-11.1% std=2.19]
  Expert: layer_14[2.9-12.1% std=2.55]
  Expert: layer_15[0.7-13.5% std=2.62]
  Expert: layer_16[3.1-14.4% std=2.94]
  Expert: layer_18[2.0-18.2% std=3.56]
  Expert: layer_19[2.5-18.0% std=3.41]
  Expert: layer_20[1.8-13.7% std=3.41]
  Expert: layer_21[2.1-16.8% std=3.59]
Step     537 | Loss: 3.1174 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 497 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.2% std=0.40] | layer_5[4.9-8.9% std=1.09] | layer_11[2.3-11.4% std=2.53] | layer_17[2.9-13.6% std=2.33] | layer_22[0.8-30.7% std=6.61]
  Expert: layer_1[5.8-6.6% std=0.24]
  Expert: layer_2[5.8-7.0% std=0.34]
  Expert: layer_3[3.5-10.2% std=2.19]
  Expert: layer_4[3.8-10.2% std=1.89]
  Expert: layer_6[4.3-8.5% std=1.16]
  Expert: layer_7[3.2-10.8% std=2.28]
  Expert: layer_8[3.4-13.2% std=2.80]
  Expert: layer_9[2.0-13.9% std=2.72]
  Expert: layer_10[3.1-9.1% std=1.71]
  Expert: layer_12[2.4-12.7% std=2.96]
  Expert: layer_13[3.3-10.2% std=1.97]
  Expert: layer_14[2.6-11.5% std=2.38]
  Expert: layer_15[0.7-10.8% std=2.15]
  Expert: layer_16[3.2-12.7% std=2.68]
  Expert: layer_18[1.7-16.1% std=3.21]
  Expert: layer_19[2.6-16.8% std=3.13]
  Expert: layer_20[2.4-12.9% std=3.19]
  Expert: layer_21[1.8-17.3% std=3.58]
Step     538 | Loss: 3.0711 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2422 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.2-7.2% std=0.44] | layer_5[4.8-8.6% std=1.05] | layer_11[2.4-12.7% std=2.60] | layer_17[3.0-13.7% std=2.31] | layer_22[0.7-29.0% std=6.25]
  Expert: layer_1[5.8-6.7% std=0.28]
  Expert: layer_2[5.8-7.0% std=0.32]
  Expert: layer_3[3.2-10.4% std=2.20]
  Expert: layer_4[3.8-10.2% std=1.94]
  Expert: layer_6[4.4-8.7% std=1.16]
  Expert: layer_7[3.3-11.0% std=2.32]
  Expert: layer_8[3.4-14.4% std=3.00]
  Expert: layer_9[1.9-14.2% std=2.78]
  Expert: layer_10[3.4-9.5% std=1.71]
  Expert: layer_12[2.4-11.9% std=2.81]
  Expert: layer_13[3.5-10.6% std=1.96]
  Expert: layer_14[2.8-12.4% std=2.33]
  Expert: layer_15[0.6-11.3% std=2.29]
  Expert: layer_16[3.1-13.4% std=2.79]
  Expert: layer_18[1.7-16.8% std=3.24]
  Expert: layer_19[2.5-15.9% std=3.00]
  Expert: layer_20[2.4-12.6% std=3.22]
  Expert: layer_21[2.2-17.8% std=3.55]
Step     539 | Loss: 3.0375 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2669 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.1% std=0.38] | layer_5[4.7-8.7% std=1.09] | layer_11[2.3-13.2% std=2.88] | layer_17[2.6-14.1% std=2.46] | layer_22[0.9-29.3% std=6.27]
  Expert: layer_1[5.8-6.6% std=0.24]
  Expert: layer_2[5.8-6.9% std=0.32]
  Expert: layer_3[3.3-10.2% std=2.22]
  Expert: layer_4[3.9-10.3% std=1.89]
  Expert: layer_6[4.5-8.3% std=1.11]
  Expert: layer_7[3.1-10.5% std=2.28]
  Expert: layer_8[3.4-13.6% std=2.89]
  Expert: layer_9[1.8-14.5% std=2.80]
  Expert: layer_10[3.1-9.5% std=1.76]
  Expert: layer_12[2.3-13.1% std=3.05]
  Expert: layer_13[3.2-10.9% std=2.00]
  Expert: layer_14[2.9-12.2% std=2.48]
  Expert: layer_15[0.8-10.0% std=2.09]
  Expert: layer_16[2.7-12.9% std=2.79]
  Expert: layer_18[1.9-17.3% std=3.27]
  Expert: layer_19[2.3-16.3% std=3.01]
  Expert: layer_20[2.1-13.7% std=3.32]
  Expert: layer_21[1.7-15.9% std=3.45]
Step     540 | Loss: 3.0631 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2674 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.3-7.2% std=0.41] | layer_5[4.8-8.9% std=1.14] | layer_11[2.2-11.6% std=2.73] | layer_17[3.4-13.9% std=2.28] | layer_22[0.7-28.3% std=6.05]
  Expert: layer_1[5.7-6.6% std=0.29]
  Expert: layer_2[5.7-7.1% std=0.35]
  Expert: layer_3[3.3-10.3% std=2.21]
  Expert: layer_4[3.9-10.1% std=1.84]
  Expert: layer_6[4.5-8.5% std=1.08]
  Expert: layer_7[3.3-11.2% std=2.29]
  Expert: layer_8[3.5-13.3% std=2.83]
  Expert: layer_9[1.7-15.2% std=2.88]
  Expert: layer_10[3.2-10.2% std=1.77]
  Expert: layer_12[2.1-12.8% std=3.00]
  Expert: layer_13[3.3-10.1% std=2.04]
  Expert: layer_14[2.5-12.6% std=2.40]
  Expert: layer_15[0.8-12.9% std=2.53]
  Expert: layer_16[2.9-12.6% std=2.67]
  Expert: layer_18[2.0-16.2% std=3.17]
  Expert: layer_19[2.6-18.1% std=3.38]
  Expert: layer_20[2.3-12.2% std=3.05]
  Expert: layer_21[1.9-15.4% std=3.39]
Step     541 | Loss: 3.0466 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2629 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.4-7.1% std=0.39] | layer_5[4.7-9.0% std=1.15] | layer_11[2.5-12.4% std=2.72] | layer_17[3.1-14.3% std=2.50] | layer_22[1.0-28.2% std=6.04]
  Expert: layer_1[5.7-6.6% std=0.28]
  Expert: layer_2[5.8-7.0% std=0.34]
  Expert: layer_3[3.2-10.3% std=2.17]
  Expert: layer_4[3.9-10.1% std=1.84]
  Expert: layer_6[4.5-8.7% std=1.16]
  Expert: layer_7[3.3-11.6% std=2.28]
  Expert: layer_8[3.3-13.2% std=2.87]
  Expert: layer_9[1.6-15.5% std=2.91]
  Expert: layer_10[3.1-10.1% std=1.75]
  Expert: layer_12[2.1-12.1% std=2.97]
  Expert: layer_13[3.4-10.3% std=1.98]
  Expert: layer_14[2.7-12.5% std=2.42]
  Expert: layer_15[0.6-13.1% std=2.63]
  Expert: layer_16[3.0-12.5% std=2.68]
  Expert: layer_18[1.8-16.8% std=3.32]
  Expert: layer_19[2.8-17.6% std=3.24]
  Expert: layer_20[2.3-12.3% std=2.97]
  Expert: layer_21[2.1-15.6% std=3.34]
Step     542 | Loss: 3.0677 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2657 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.3% std=0.38] | layer_5[4.8-8.9% std=1.07] | layer_11[2.0-11.9% std=2.65] | layer_17[2.7-15.2% std=2.66] | layer_22[0.8-28.8% std=6.16]
  Expert: layer_1[5.8-6.6% std=0.26]
  Expert: layer_2[5.9-7.1% std=0.32]
  Expert: layer_3[3.4-10.4% std=2.23]
  Expert: layer_4[3.8-10.1% std=1.89]
  Expert: layer_6[4.6-8.4% std=1.13]
  Expert: layer_7[3.3-11.7% std=2.40]
  Expert: layer_8[3.5-12.8% std=2.74]
  Expert: layer_9[1.6-15.2% std=2.81]
  Expert: layer_10[3.3-10.0% std=1.63]
  Expert: layer_12[2.2-13.4% std=3.01]
  Expert: layer_13[3.4-10.0% std=1.84]
  Expert: layer_14[2.7-12.6% std=2.41]
  Expert: layer_15[0.7-10.4% std=2.23]
  Expert: layer_16[3.0-13.4% std=2.83]
  Expert: layer_18[1.7-17.9% std=3.47]
  Expert: layer_19[2.5-16.2% std=3.05]
  Expert: layer_20[2.2-12.1% std=3.06]
  Expert: layer_21[2.2-17.1% std=3.45]
Step     543 | Loss: 3.1169 | LR: 2.50e-04 | GradNorm: 0.26 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.3% std=0.39] | layer_5[4.6-9.2% std=1.18] | layer_11[2.4-11.8% std=2.65] | layer_17[2.7-15.7% std=2.74] | layer_22[0.8-28.4% std=6.09]
  Expert: layer_1[5.8-6.7% std=0.26]
  Expert: layer_2[5.8-7.2% std=0.34]
  Expert: layer_3[3.3-10.5% std=2.21]
  Expert: layer_4[3.9-10.3% std=1.88]
  Expert: layer_6[4.5-8.4% std=1.12]
  Expert: layer_7[2.8-11.8% std=2.38]
  Expert: layer_8[3.4-12.6% std=2.75]
  Expert: layer_9[1.7-15.0% std=2.74]
  Expert: layer_10[3.1-9.9% std=1.67]
  Expert: layer_12[2.2-13.1% std=3.06]
  Expert: layer_13[3.6-10.6% std=1.92]
  Expert: layer_14[2.6-13.4% std=2.55]
  Expert: layer_15[0.7-11.6% std=2.34]
  Expert: layer_16[3.0-12.6% std=2.65]
  Expert: layer_18[1.9-16.9% std=3.28]
  Expert: layer_19[2.5-17.9% std=3.38]
  Expert: layer_20[2.3-12.9% std=3.23]
  Expert: layer_21[2.0-17.6% std=3.63]
Step     544 | Loss: 3.1499 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2654 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.3% std=0.36] | layer_5[4.7-9.1% std=1.14] | layer_11[2.3-11.5% std=2.67] | layer_17[3.1-15.4% std=2.61] | layer_22[0.9-27.6% std=5.83]
  Expert: layer_1[5.8-6.7% std=0.25]
  Expert: layer_2[5.8-7.3% std=0.35]
  Expert: layer_3[3.1-10.6% std=2.32]
  Expert: layer_4[4.0-10.2% std=1.84]
  Expert: layer_6[4.4-8.5% std=1.14]
  Expert: layer_7[2.7-11.9% std=2.41]
  Expert: layer_8[3.4-12.1% std=2.65]
  Expert: layer_9[1.6-16.0% std=2.93]
  Expert: layer_10[3.1-10.0% std=1.70]
  Expert: layer_12[2.3-12.7% std=3.01]
  Expert: layer_13[3.5-10.5% std=1.95]
  Expert: layer_14[2.6-13.1% std=2.56]
  Expert: layer_15[0.6-11.0% std=2.35]
  Expert: layer_16[3.1-12.9% std=2.70]
  Expert: layer_18[2.1-16.8% std=3.18]
  Expert: layer_19[2.6-19.2% std=3.63]
  Expert: layer_20[2.2-12.6% std=3.20]
  Expert: layer_21[2.1-16.5% std=3.41]
Step     545 | Loss: 3.1393 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2655 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.2% std=0.36] | layer_5[4.8-9.2% std=1.14] | layer_11[2.3-11.8% std=2.60] | layer_17[2.9-14.7% std=2.53] | layer_22[0.7-28.0% std=5.94]
  Expert: layer_1[5.7-6.7% std=0.27]
  Expert: layer_2[5.6-7.3% std=0.36]
  Expert: layer_3[3.2-10.8% std=2.36]
  Expert: layer_4[3.9-10.4% std=1.81]
  Expert: layer_6[4.2-8.5% std=1.22]
  Expert: layer_7[2.6-12.2% std=2.40]
  Expert: layer_8[3.3-12.1% std=2.68]
  Expert: layer_9[1.6-15.4% std=2.85]
  Expert: layer_10[3.1-10.1% std=1.68]
  Expert: layer_12[2.3-12.2% std=2.87]
  Expert: layer_13[3.8-9.9% std=1.80]
  Expert: layer_14[2.7-13.0% std=2.48]
  Expert: layer_15[0.7-10.5% std=2.21]
  Expert: layer_16[2.9-12.1% std=2.50]
  Expert: layer_18[1.8-15.8% std=3.14]
  Expert: layer_19[2.6-17.7% std=3.31]
  Expert: layer_20[2.5-12.0% std=3.05]
  Expert: layer_21[2.1-15.8% std=3.19]
Step     546 | Loss: 3.0850 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2557 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.3% std=0.35] | layer_5[4.7-9.2% std=1.16] | layer_11[2.4-11.5% std=2.55] | layer_17[3.1-14.5% std=2.47] | layer_22[0.8-27.3% std=5.76]
  Expert: layer_1[5.8-6.8% std=0.30]
  Expert: layer_2[5.6-7.2% std=0.39]
  Expert: layer_3[3.2-10.9% std=2.36]
  Expert: layer_4[4.0-10.3% std=1.77]
  Expert: layer_6[4.4-8.6% std=1.26]
  Expert: layer_7[2.6-12.3% std=2.34]
  Expert: layer_8[3.3-12.6% std=2.71]
  Expert: layer_9[1.7-15.0% std=2.77]
  Expert: layer_10[3.2-10.3% std=1.66]
  Expert: layer_12[2.3-11.8% std=2.78]
  Expert: layer_13[3.6-10.1% std=1.84]
  Expert: layer_14[2.8-13.5% std=2.46]
  Expert: layer_15[1.1-12.0% std=2.35]
  Expert: layer_16[3.3-11.8% std=2.39]
  Expert: layer_18[1.8-17.1% std=3.39]
  Expert: layer_19[2.7-19.2% std=3.62]
  Expert: layer_20[2.4-13.0% std=3.11]
  Expert: layer_21[1.9-16.3% std=3.34]
Step     547 | Loss: 3.0529 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2621 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.3% std=0.37] | layer_5[4.7-9.6% std=1.21] | layer_11[2.5-10.5% std=2.48] | layer_17[3.1-12.6% std=2.08] | layer_22[0.9-26.2% std=5.50]
  Expert: layer_1[5.8-6.9% std=0.29]
  Expert: layer_2[5.6-7.3% std=0.38]
  Expert: layer_3[3.3-10.9% std=2.28]
  Expert: layer_4[4.0-10.1% std=1.74]
  Expert: layer_6[4.3-8.6% std=1.22]
  Expert: layer_7[2.7-12.2% std=2.30]
  Expert: layer_8[3.3-12.1% std=2.65]
  Expert: layer_9[1.6-15.3% std=2.77]
  Expert: layer_10[3.0-10.2% std=1.69]
  Expert: layer_12[2.4-13.5% std=2.96]
  Expert: layer_13[3.7-9.8% std=1.80]
  Expert: layer_14[2.2-12.5% std=2.38]
  Expert: layer_15[0.8-9.9% std=2.12]
  Expert: layer_16[2.6-13.2% std=2.72]
  Expert: layer_18[1.7-16.4% std=3.23]
  Expert: layer_19[2.6-17.9% std=3.38]
  Expert: layer_20[2.2-12.1% std=2.91]
  Expert: layer_21[1.9-15.4% std=3.14]
Step     548 | Loss: 3.1287 | LR: 2.50e-04 | GradNorm: 0.36 | Tok/s: 2629 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.3% std=0.36] | layer_5[4.6-9.5% std=1.20] | layer_11[2.3-11.4% std=2.69] | layer_17[2.8-14.2% std=2.43] | layer_22[1.0-28.7% std=6.12]
  Expert: layer_1[5.8-6.8% std=0.26]
  Expert: layer_2[5.6-7.1% std=0.38]
  Expert: layer_3[3.4-10.9% std=2.31]
  Expert: layer_4[4.1-10.3% std=1.69]
  Expert: layer_6[4.3-8.7% std=1.23]
  Expert: layer_7[3.1-12.0% std=2.20]
  Expert: layer_8[3.0-12.2% std=2.61]
  Expert: layer_9[1.8-15.2% std=2.79]
  Expert: layer_10[3.0-10.1% std=1.71]
  Expert: layer_12[2.5-12.1% std=2.86]
  Expert: layer_13[3.6-10.5% std=1.97]
  Expert: layer_14[2.8-14.3% std=2.67]
  Expert: layer_15[0.9-11.4% std=2.39]
  Expert: layer_16[3.3-11.9% std=2.44]
  Expert: layer_18[1.9-16.0% std=3.11]
  Expert: layer_19[2.6-18.9% std=3.54]
  Expert: layer_20[2.2-12.6% std=3.18]
  Expert: layer_21[1.7-15.8% std=3.50]
Step     549 | Loss: 3.0716 | LR: 2.50e-04 | GradNorm: 0.37 | Tok/s: 2627 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.3% std=0.37] | layer_5[4.5-9.5% std=1.19] | layer_11[2.0-12.8% std=2.67] | layer_17[2.9-13.5% std=2.24] | layer_22[1.0-27.4% std=5.76]
  Expert: layer_1[5.8-6.8% std=0.27]
  Expert: layer_2[5.6-7.2% std=0.39]
  Expert: layer_3[3.3-10.9% std=2.30]
  Expert: layer_4[3.9-10.2% std=1.71]
  Expert: layer_6[4.4-8.7% std=1.15]
  Expert: layer_7[2.9-11.6% std=2.26]
  Expert: layer_8[3.2-12.7% std=2.56]
  Expert: layer_9[1.7-14.9% std=2.69]
  Expert: layer_10[3.1-10.5% std=1.69]
  Expert: layer_12[2.5-11.8% std=2.72]
  Expert: layer_13[3.8-10.5% std=1.94]
  Expert: layer_14[2.3-13.3% std=2.56]
  Expert: layer_15[0.8-9.5% std=2.09]
  Expert: layer_16[3.0-11.8% std=2.39]
  Expert: layer_18[1.6-16.3% std=3.11]
  Expert: layer_19[2.3-15.4% std=2.94]
  Expert: layer_20[2.3-11.9% std=2.86]
  Expert: layer_21[2.0-16.2% std=3.25]
Step     550 | Loss: 3.1262 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2591 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.4% std=0.37] | layer_5[4.5-9.8% std=1.22] | layer_11[2.4-11.7% std=2.83] | layer_17[2.7-14.2% std=2.42] | layer_22[1.0-28.0% std=5.97]
  Expert: layer_1[5.8-6.7% std=0.25]
  Expert: layer_2[5.6-7.1% std=0.38]
  Expert: layer_3[3.4-11.0% std=2.29]
  Expert: layer_4[4.1-10.4% std=1.65]
  Expert: layer_6[4.4-8.6% std=1.21]
  Expert: layer_7[3.1-11.0% std=2.11]
  Expert: layer_8[3.2-12.6% std=2.64]
  Expert: layer_9[1.8-15.5% std=2.86]
  Expert: layer_10[3.1-9.6% std=1.67]
  Expert: layer_12[2.8-11.9% std=2.88]
  Expert: layer_13[3.3-10.2% std=1.93]
  Expert: layer_14[2.7-13.0% std=2.53]
  Expert: layer_15[1.0-10.2% std=2.17]
  Expert: layer_16[2.7-12.9% std=2.75]
  Expert: layer_18[1.7-16.7% std=3.27]
  Expert: layer_19[2.9-18.7% std=3.47]
  Expert: layer_20[2.2-12.4% std=3.27]
  Expert: layer_21[1.6-15.9% std=3.30]
Step     551 | Loss: 3.1484 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2621 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.4% std=0.38] | layer_5[4.3-9.7% std=1.18] | layer_11[2.2-11.9% std=2.88] | layer_17[2.7-14.3% std=2.43] | layer_22[0.7-28.3% std=6.04]
  Expert: layer_1[5.8-6.7% std=0.22]
  Expert: layer_2[5.6-7.1% std=0.40]
  Expert: layer_3[3.4-10.8% std=2.30]
  Expert: layer_4[4.1-10.5% std=1.66]
  Expert: layer_6[4.2-8.5% std=1.20]
  Expert: layer_7[3.1-10.5% std=1.99]
  Expert: layer_8[3.1-12.5% std=2.56]
  Expert: layer_9[1.9-15.3% std=2.80]
  Expert: layer_10[3.1-9.5% std=1.63]
  Expert: layer_12[2.6-12.4% std=2.91]
  Expert: layer_13[3.5-10.3% std=1.89]
  Expert: layer_14[3.0-13.8% std=2.67]
  Expert: layer_15[1.1-10.4% std=2.17]
  Expert: layer_16[3.1-14.4% std=2.93]
  Expert: layer_18[1.8-16.8% std=3.25]
  Expert: layer_19[2.5-18.1% std=3.40]
  Expert: layer_20[2.3-12.7% std=3.20]
  Expert: layer_21[1.8-16.2% std=3.44]
Step     552 | Loss: 3.0300 | LR: 2.50e-04 | GradNorm: 0.35 | Tok/s: 2642 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.3% std=0.37] | layer_5[4.2-9.6% std=1.17] | layer_11[2.3-12.5% std=2.75] | layer_17[3.1-12.4% std=1.90] | layer_22[1.2-27.0% std=5.69]
  Expert: layer_1[5.8-6.7% std=0.24]
  Expert: layer_2[5.5-7.1% std=0.39]
  Expert: layer_3[3.2-10.4% std=2.20]
  Expert: layer_4[4.0-10.6% std=1.67]
  Expert: layer_6[4.4-8.4% std=1.17]
  Expert: layer_7[3.3-10.5% std=1.96]
  Expert: layer_8[3.6-12.7% std=2.50]
  Expert: layer_9[1.7-15.2% std=2.76]
  Expert: layer_10[2.9-9.7% std=1.63]
  Expert: layer_12[2.3-11.7% std=2.72]
  Expert: layer_13[3.5-10.0% std=1.91]
  Expert: layer_14[2.4-13.4% std=2.46]
  Expert: layer_15[1.0-10.7% std=2.13]
  Expert: layer_16[2.9-12.7% std=2.62]
  Expert: layer_18[1.8-16.3% std=3.03]
  Expert: layer_19[2.3-18.3% std=3.53]
  Expert: layer_20[2.4-11.1% std=2.79]
  Expert: layer_21[1.9-15.5% std=3.13]
Step     553 | Loss: 3.1338 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2651 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.4% std=0.37] | layer_5[4.3-10.1% std=1.23] | layer_11[2.5-14.0% std=2.90] | layer_17[3.2-13.9% std=2.27] | layer_22[0.7-28.6% std=6.13]
  Expert: layer_1[5.7-6.7% std=0.23]
  Expert: layer_2[5.6-7.0% std=0.38]
  Expert: layer_3[3.2-10.5% std=2.26]
  Expert: layer_4[4.2-10.2% std=1.56]
  Expert: layer_6[4.4-8.4% std=1.17]
  Expert: layer_7[3.0-10.9% std=2.12]
  Expert: layer_8[3.5-13.6% std=2.66]
  Expert: layer_9[1.9-14.7% std=2.67]
  Expert: layer_10[2.9-9.5% std=1.69]
  Expert: layer_12[2.4-12.3% std=2.80]
  Expert: layer_13[3.5-10.6% std=1.97]
  Expert: layer_14[3.1-12.1% std=2.37]
  Expert: layer_15[0.9-10.9% std=2.15]
  Expert: layer_16[3.1-12.3% std=2.46]
  Expert: layer_18[1.7-15.3% std=2.85]
  Expert: layer_19[2.7-17.9% std=3.35]
  Expert: layer_20[2.0-12.3% std=2.95]
  Expert: layer_21[2.0-16.3% std=3.39]
Step     554 | Loss: 3.0904 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2653 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.2% std=0.35] | layer_5[4.2-10.1% std=1.22] | layer_11[2.6-12.4% std=2.79] | layer_17[2.9-12.8% std=2.04] | layer_22[0.9-30.0% std=6.47]
  Expert: layer_1[5.8-6.7% std=0.23]
  Expert: layer_2[5.6-7.1% std=0.38]
  Expert: layer_3[3.1-10.2% std=2.20]
  Expert: layer_4[4.5-10.3% std=1.57]
  Expert: layer_6[4.4-8.7% std=1.17]
  Expert: layer_7[2.8-10.9% std=2.03]
  Expert: layer_8[3.4-13.0% std=2.56]
  Expert: layer_9[1.8-14.5% std=2.64]
  Expert: layer_10[2.9-9.3% std=1.70]
  Expert: layer_12[2.3-12.2% std=3.07]
  Expert: layer_13[3.8-9.9% std=1.78]
  Expert: layer_14[2.9-11.9% std=2.27]
  Expert: layer_15[0.9-11.3% std=2.21]
  Expert: layer_16[3.1-13.4% std=2.76]
  Expert: layer_18[1.9-15.8% std=3.01]
  Expert: layer_19[2.8-18.7% std=3.52]
  Expert: layer_20[2.3-12.0% std=3.05]
  Expert: layer_21[1.9-16.0% std=3.32]
Step     555 | Loss: 3.0971 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2653 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.3% std=0.39] | layer_5[4.3-9.7% std=1.14] | layer_11[2.2-12.3% std=2.77] | layer_17[3.3-12.8% std=2.02] | layer_22[0.9-28.5% std=6.08]
  Expert: layer_1[5.7-6.8% std=0.25]
  Expert: layer_2[5.6-7.0% std=0.39]
  Expert: layer_3[2.9-10.2% std=2.21]
  Expert: layer_4[4.3-10.1% std=1.57]
  Expert: layer_6[4.6-8.6% std=1.11]
  Expert: layer_7[3.0-11.0% std=2.11]
  Expert: layer_8[3.6-12.8% std=2.56]
  Expert: layer_9[1.8-14.6% std=2.65]
  Expert: layer_10[2.9-9.4% std=1.65]
  Expert: layer_12[2.4-12.4% std=2.93]
  Expert: layer_13[3.5-9.6% std=1.79]
  Expert: layer_14[2.7-13.1% std=2.44]
  Expert: layer_15[0.7-10.1% std=2.14]
  Expert: layer_16[3.1-13.8% std=2.86]
  Expert: layer_18[1.8-15.6% std=3.04]
  Expert: layer_19[2.5-16.8% std=3.10]
  Expert: layer_20[2.3-13.0% std=3.14]
  Expert: layer_21[1.9-16.4% std=3.27]
Step     556 | Loss: 3.0752 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2606 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.3% std=0.37] | layer_5[4.4-9.8% std=1.15] | layer_11[2.6-14.0% std=3.07] | layer_17[3.3-14.4% std=2.36] | layer_22[1.0-28.3% std=6.04]
  Expert: layer_1[5.7-6.9% std=0.27]
  Expert: layer_2[5.7-7.1% std=0.41]
  Expert: layer_3[2.9-10.3% std=2.22]
  Expert: layer_4[4.4-10.2% std=1.59]
  Expert: layer_6[4.5-8.5% std=1.14]
  Expert: layer_7[2.9-10.5% std=2.03]
  Expert: layer_8[3.8-13.9% std=2.69]
  Expert: layer_9[1.8-14.3% std=2.65]
  Expert: layer_10[2.8-9.3% std=1.71]
  Expert: layer_12[2.3-12.4% std=2.99]
  Expert: layer_13[3.4-10.2% std=1.89]
  Expert: layer_14[3.0-12.6% std=2.40]
  Expert: layer_15[0.7-10.8% std=2.16]
  Expert: layer_16[3.0-12.8% std=2.64]
  Expert: layer_18[1.8-15.9% std=3.00]
  Expert: layer_19[2.2-16.5% std=3.05]
  Expert: layer_20[2.1-12.9% std=3.23]
  Expert: layer_21[2.0-16.0% std=3.33]
Step     557 | Loss: 3.0989 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2648 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.4% std=0.38] | layer_5[4.4-9.8% std=1.10] | layer_11[2.4-12.1% std=2.82] | layer_17[3.3-13.4% std=2.11] | layer_22[0.7-30.0% std=6.48]
  Expert: layer_1[5.8-6.9% std=0.26]
  Expert: layer_2[5.7-7.1% std=0.39]
  Expert: layer_3[2.9-10.2% std=2.31]
  Expert: layer_4[4.5-9.9% std=1.53]
  Expert: layer_6[4.4-9.0% std=1.24]
  Expert: layer_7[2.8-10.4% std=2.03]
  Expert: layer_8[3.8-13.4% std=2.59]
  Expert: layer_9[1.9-14.6% std=2.67]
  Expert: layer_10[2.8-9.1% std=1.69]
  Expert: layer_12[2.2-13.3% std=3.12]
  Expert: layer_13[3.5-9.4% std=1.76]
  Expert: layer_14[3.0-12.1% std=2.29]
  Expert: layer_15[0.8-10.5% std=2.14]
  Expert: layer_16[3.1-14.3% std=2.91]
  Expert: layer_18[2.0-15.3% std=2.90]
  Expert: layer_19[2.5-18.1% std=3.41]
  Expert: layer_20[2.4-14.1% std=3.21]
  Expert: layer_21[1.8-17.3% std=3.51]
Step     558 | Loss: 3.1240 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2640 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.3% std=0.36] | layer_5[4.3-10.0% std=1.15] | layer_11[2.5-13.2% std=2.93] | layer_17[3.7-12.8% std=1.96] | layer_22[1.0-28.6% std=6.11]
  Expert: layer_1[5.8-6.9% std=0.25]
  Expert: layer_2[5.7-7.2% std=0.38]
  Expert: layer_3[2.9-10.5% std=2.34]
  Expert: layer_4[4.5-10.1% std=1.52]
  Expert: layer_6[4.2-8.9% std=1.27]
  Expert: layer_7[2.8-10.2% std=2.06]
  Expert: layer_8[3.7-13.2% std=2.54]
  Expert: layer_9[2.0-14.4% std=2.65]
  Expert: layer_10[2.8-8.9% std=1.70]
  Expert: layer_12[2.2-12.8% std=3.04]
  Expert: layer_13[3.5-10.2% std=1.77]
  Expert: layer_14[3.1-13.6% std=2.54]
  Expert: layer_15[0.8-11.0% std=2.24]
  Expert: layer_16[3.2-12.8% std=2.64]
  Expert: layer_18[2.3-17.1% std=3.15]
  Expert: layer_19[2.5-16.6% std=2.99]
  Expert: layer_20[2.3-12.9% std=3.13]
  Expert: layer_21[2.0-15.7% std=3.28]
Step     559 | Loss: 3.1409 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.4% std=0.40] | layer_5[4.4-10.0% std=1.14] | layer_11[2.4-13.3% std=2.95] | layer_17[3.3-13.4% std=2.13] | layer_22[0.7-29.0% std=6.21]
  Expert: layer_1[5.8-6.9% std=0.24]
  Expert: layer_2[5.6-7.1% std=0.39]
  Expert: layer_3[2.8-10.5% std=2.36]
  Expert: layer_4[4.5-10.3% std=1.60]
  Expert: layer_6[4.3-9.1% std=1.28]
  Expert: layer_7[2.8-10.4% std=2.12]
  Expert: layer_8[3.8-12.8% std=2.51]
  Expert: layer_9[2.0-14.1% std=2.58]
  Expert: layer_10[2.6-9.0% std=1.72]
  Expert: layer_12[2.3-12.3% std=2.98]
  Expert: layer_13[3.4-10.0% std=1.79]
  Expert: layer_14[2.9-12.8% std=2.43]
  Expert: layer_15[0.8-10.1% std=2.08]
  Expert: layer_16[3.1-13.1% std=2.67]
  Expert: layer_18[2.1-17.4% std=3.27]
  Expert: layer_19[2.3-16.0% std=2.90]
  Expert: layer_20[2.1-12.6% std=3.20]
  Expert: layer_21[1.9-15.8% std=3.23]
Step     560 | Loss: 3.1572 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2650 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.4% std=0.34] | layer_5[4.4-9.8% std=1.07] | layer_11[2.5-12.1% std=2.67] | layer_17[3.0-14.1% std=2.28] | layer_22[0.5-28.7% std=6.13]
  Expert: layer_1[5.7-6.9% std=0.27]
  Expert: layer_2[5.6-7.1% std=0.41]
  Expert: layer_3[2.8-10.5% std=2.44]
  Expert: layer_4[4.4-10.4% std=1.68]
  Expert: layer_6[4.3-9.7% std=1.37]
  Expert: layer_7[3.1-10.5% std=2.07]
  Expert: layer_8[3.6-12.5% std=2.44]
  Expert: layer_9[2.0-13.6% std=2.50]
  Expert: layer_10[2.9-9.2% std=1.71]
  Expert: layer_12[2.0-11.8% std=3.00]
  Expert: layer_13[3.4-9.7% std=1.77]
  Expert: layer_14[2.9-12.7% std=2.26]
  Expert: layer_15[0.8-10.9% std=2.24]
  Expert: layer_16[3.5-13.5% std=2.71]
  Expert: layer_18[2.0-16.4% std=3.08]
  Expert: layer_19[2.1-17.0% std=3.16]
  Expert: layer_20[2.2-11.8% std=2.97]
  Expert: layer_21[1.9-16.9% std=3.31]
Step     561 | Loss: 3.0984 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2622 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.3% std=0.33] | layer_5[4.4-9.9% std=1.13] | layer_11[2.7-12.8% std=2.67] | layer_17[2.7-14.0% std=2.31] | layer_22[0.7-27.5% std=5.87]
  Expert: layer_1[5.8-6.7% std=0.25]
  Expert: layer_2[5.5-7.0% std=0.39]
  Expert: layer_3[2.7-10.4% std=2.36]
  Expert: layer_4[4.4-10.3% std=1.75]
  Expert: layer_6[4.4-9.7% std=1.31]
  Expert: layer_7[3.2-10.6% std=2.03]
  Expert: layer_8[3.8-13.6% std=2.70]
  Expert: layer_9[2.0-12.9% std=2.36]
  Expert: layer_10[2.7-10.1% std=1.81]
  Expert: layer_12[2.1-12.4% std=2.91]
  Expert: layer_13[3.5-10.6% std=1.81]
  Expert: layer_14[2.8-12.5% std=2.37]
  Expert: layer_15[0.8-9.7% std=2.06]
  Expert: layer_16[3.0-13.3% std=2.78]
  Expert: layer_18[1.9-16.6% std=3.16]
  Expert: layer_19[2.2-16.7% std=3.15]
  Expert: layer_20[2.2-13.1% std=3.05]
  Expert: layer_21[1.8-16.6% std=3.35]
Step     562 | Loss: 3.0399 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2636 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.4% std=0.36] | layer_5[4.4-9.9% std=1.10] | layer_11[2.8-13.3% std=2.77] | layer_17[2.9-12.8% std=2.10] | layer_22[0.9-25.7% std=5.43]
  Expert: layer_1[5.8-6.7% std=0.24]
  Expert: layer_2[5.5-6.9% std=0.37]
  Expert: layer_3[2.6-10.6% std=2.44]
  Expert: layer_4[4.4-10.6% std=1.83]
  Expert: layer_6[4.4-9.7% std=1.30]
  Expert: layer_7[2.9-10.4% std=2.02]
  Expert: layer_8[3.8-14.1% std=2.76]
  Expert: layer_9[2.0-13.0% std=2.38]
  Expert: layer_10[2.8-9.6% std=1.75]
  Expert: layer_12[2.0-12.3% std=2.92]
  Expert: layer_13[3.6-10.9% std=1.80]
  Expert: layer_14[2.9-13.1% std=2.45]
  Expert: layer_15[0.7-9.7% std=1.97]
  Expert: layer_16[2.8-13.0% std=2.68]
  Expert: layer_18[1.8-16.2% std=3.05]
  Expert: layer_19[2.3-16.2% std=2.97]
  Expert: layer_20[2.2-12.3% std=2.89]
  Expert: layer_21[1.6-14.9% std=3.13]
Step     563 | Loss: 3.0472 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.4% std=0.35] | layer_5[4.4-9.7% std=1.07] | layer_11[2.3-12.2% std=2.63] | layer_17[3.5-12.5% std=1.98] | layer_22[0.8-23.9% std=5.01]
  Expert: layer_1[5.8-6.6% std=0.23]
  Expert: layer_2[5.5-6.9% std=0.36]
  Expert: layer_3[2.5-10.6% std=2.50]
  Expert: layer_4[4.2-10.5% std=1.90]
  Expert: layer_6[4.4-9.8% std=1.29]
  Expert: layer_7[2.8-10.7% std=2.06]
  Expert: layer_8[3.8-14.0% std=2.77]
  Expert: layer_9[2.0-13.1% std=2.39]
  Expert: layer_10[3.0-9.3% std=1.70]
  Expert: layer_12[2.0-11.8% std=2.87]
  Expert: layer_13[3.4-10.1% std=1.73]
  Expert: layer_14[2.9-12.3% std=2.21]
  Expert: layer_15[0.7-10.6% std=2.15]
  Expert: layer_16[3.1-13.1% std=2.71]
  Expert: layer_18[2.1-16.0% std=3.03]
  Expert: layer_19[2.3-15.8% std=2.84]
  Expert: layer_20[2.4-11.4% std=2.78]
  Expert: layer_21[1.9-14.9% std=2.92]
Step     564 | Loss: 3.0451 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2638 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.33] | layer_5[4.4-9.6% std=1.06] | layer_11[2.5-12.1% std=2.53] | layer_17[3.2-13.4% std=2.11] | layer_22[1.0-25.7% std=5.43]
  Expert: layer_1[5.8-6.6% std=0.22]
  Expert: layer_2[5.6-6.8% std=0.35]
  Expert: layer_3[2.5-10.6% std=2.41]
  Expert: layer_4[4.2-10.9% std=1.94]
  Expert: layer_6[4.5-9.7% std=1.24]
  Expert: layer_7[3.3-10.5% std=1.97]
  Expert: layer_8[3.6-14.3% std=2.84]
  Expert: layer_9[2.1-13.2% std=2.36]
  Expert: layer_10[2.8-9.8% std=1.83]
  Expert: layer_12[2.0-11.6% std=2.76]
  Expert: layer_13[3.6-10.6% std=1.81]
  Expert: layer_14[2.7-11.8% std=2.21]
  Expert: layer_15[0.6-11.2% std=2.26]
  Expert: layer_16[2.8-12.9% std=2.71]
  Expert: layer_18[2.0-15.8% std=2.98]
  Expert: layer_19[2.2-17.3% std=3.21]
  Expert: layer_20[2.2-11.9% std=2.88]
  Expert: layer_21[2.0-16.1% std=3.19]
Step     565 | Loss: 3.0355 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2618 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.34] | layer_5[4.3-9.5% std=1.07] | layer_11[2.4-11.6% std=2.58] | layer_17[3.1-13.7% std=2.21] | layer_22[1.0-25.8% std=5.46]
  Expert: layer_1[5.7-6.7% std=0.23]
  Expert: layer_2[5.6-6.7% std=0.31]
  Expert: layer_3[2.5-10.9% std=2.44]
  Expert: layer_4[4.3-11.0% std=2.03]
  Expert: layer_6[4.5-9.7% std=1.25]
  Expert: layer_7[3.3-10.3% std=2.00]
  Expert: layer_8[3.6-14.4% std=2.87]
  Expert: layer_9[2.0-13.7% std=2.49]
  Expert: layer_10[2.7-9.4% std=1.81]
  Expert: layer_12[2.0-11.3% std=2.83]
  Expert: layer_13[3.4-10.7% std=1.86]
  Expert: layer_14[2.9-12.3% std=2.29]
  Expert: layer_15[0.6-10.4% std=2.13]
  Expert: layer_16[2.9-13.2% std=2.68]
  Expert: layer_18[1.7-16.1% std=3.07]
  Expert: layer_19[2.5-17.9% std=3.35]
  Expert: layer_20[2.2-13.1% std=3.14]
  Expert: layer_21[1.9-15.3% std=3.13]
Step     566 | Loss: 3.0904 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2607 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.34] | layer_5[4.3-9.7% std=1.08] | layer_11[2.6-12.1% std=2.51] | layer_17[3.0-14.1% std=2.31] | layer_22[1.0-26.2% std=5.55]
  Expert: layer_1[5.8-6.5% std=0.18]
  Expert: layer_2[5.6-6.7% std=0.31]
  Expert: layer_3[2.4-10.6% std=2.50]
  Expert: layer_4[4.3-11.3% std=2.07]
  Expert: layer_6[4.5-9.4% std=1.21]
  Expert: layer_7[3.0-10.5% std=2.09]
  Expert: layer_8[3.4-14.6% std=2.96]
  Expert: layer_9[2.1-13.4% std=2.47]
  Expert: layer_10[2.6-9.1% std=1.80]
  Expert: layer_12[2.2-11.6% std=2.81]
  Expert: layer_13[3.6-10.8% std=1.79]
  Expert: layer_14[2.8-12.9% std=2.41]
  Expert: layer_15[0.6-9.9% std=2.03]
  Expert: layer_16[3.2-12.8% std=2.66]
  Expert: layer_18[1.6-16.0% std=3.00]
  Expert: layer_19[2.5-17.3% std=3.23]
  Expert: layer_20[2.2-12.4% std=3.09]
  Expert: layer_21[2.0-15.2% std=3.17]
Step     567 | Loss: 3.1367 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2525 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.9-7.3% std=0.32] | layer_5[4.4-9.3% std=1.02] | layer_11[2.4-11.8% std=2.51] | layer_17[3.3-14.3% std=2.28] | layer_22[0.8-26.7% std=5.66]
  Expert: layer_1[5.8-6.7% std=0.20]
  Expert: layer_2[5.6-6.8% std=0.29]
  Expert: layer_3[2.4-10.7% std=2.51]
  Expert: layer_4[4.2-11.3% std=2.09]
  Expert: layer_6[4.4-9.4% std=1.23]
  Expert: layer_7[2.4-10.7% std=2.14]
  Expert: layer_8[3.6-14.1% std=2.89]
  Expert: layer_9[2.0-13.6% std=2.48]
  Expert: layer_10[2.9-9.5% std=1.74]
  Expert: layer_12[2.2-11.7% std=2.77]
  Expert: layer_13[3.8-10.9% std=1.80]
  Expert: layer_14[2.5-12.3% std=2.37]
  Expert: layer_15[0.6-10.9% std=2.19]
  Expert: layer_16[2.9-13.2% std=2.82]
  Expert: layer_18[1.7-16.7% std=3.12]
  Expert: layer_19[2.5-16.4% std=3.02]
  Expert: layer_20[2.2-11.9% std=2.92]
  Expert: layer_21[2.1-16.3% std=3.22]
Step     568 | Loss: 3.0812 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2553 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.32] | layer_5[4.3-9.3% std=1.01] | layer_11[2.4-12.6% std=2.62] | layer_17[3.2-14.1% std=2.24] | layer_22[1.0-27.3% std=5.79]
  Expert: layer_1[5.8-6.6% std=0.20]
  Expert: layer_2[5.6-6.7% std=0.29]
  Expert: layer_3[2.4-10.4% std=2.34]
  Expert: layer_4[4.2-11.4% std=2.04]
  Expert: layer_6[4.5-9.2% std=1.16]
  Expert: layer_7[3.3-10.4% std=2.00]
  Expert: layer_8[3.3-14.5% std=3.00]
  Expert: layer_9[2.1-14.1% std=2.57]
  Expert: layer_10[2.7-9.4% std=1.83]
  Expert: layer_12[2.4-11.6% std=2.86]
  Expert: layer_13[3.4-11.3% std=1.84]
  Expert: layer_14[2.9-11.8% std=2.24]
  Expert: layer_15[0.5-10.9% std=2.17]
  Expert: layer_16[2.8-12.7% std=2.64]
  Expert: layer_18[1.9-16.1% std=2.99]
  Expert: layer_19[2.6-17.0% std=3.07]
  Expert: layer_20[2.0-12.0% std=2.96]
  Expert: layer_21[2.0-16.9% std=3.45]
Step     569 | Loss: 3.0772 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2574 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.33] | layer_5[4.2-9.5% std=1.08] | layer_11[2.6-12.1% std=2.47] | layer_17[3.7-12.9% std=2.03] | layer_22[1.0-26.6% std=5.68]
  Expert: layer_1[5.9-6.7% std=0.21]
  Expert: layer_2[5.6-6.8% std=0.33]
  Expert: layer_3[2.4-10.1% std=2.25]
  Expert: layer_4[4.2-11.5% std=1.97]
  Expert: layer_6[4.4-9.1% std=1.20]
  Expert: layer_7[2.6-10.6% std=2.03]
  Expert: layer_8[3.2-15.9% std=3.20]
  Expert: layer_9[2.2-14.0% std=2.54]
  Expert: layer_10[2.6-9.6% std=1.85]
  Expert: layer_12[2.3-11.5% std=2.76]
  Expert: layer_13[3.6-11.0% std=1.76]
  Expert: layer_14[2.7-12.1% std=2.30]
  Expert: layer_15[0.5-11.6% std=2.27]
  Expert: layer_16[3.0-12.4% std=2.46]
  Expert: layer_18[1.7-16.3% std=3.17]
  Expert: layer_19[2.3-16.9% std=3.08]
  Expert: layer_20[2.3-13.3% std=2.94]
  Expert: layer_21[2.1-16.1% std=3.22]
Step     570 | Loss: 3.0764 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2402 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.32] | layer_5[4.2-9.0% std=0.97] | layer_11[2.4-12.7% std=2.72] | layer_17[3.0-14.8% std=2.38] | layer_22[1.0-27.5% std=5.82]
  Expert: layer_1[5.8-6.7% std=0.20]
  Expert: layer_2[5.7-6.8% std=0.28]
  Expert: layer_3[2.5-10.1% std=2.32]
  Expert: layer_4[4.2-11.1% std=1.88]
  Expert: layer_6[4.5-9.1% std=1.22]
  Expert: layer_7[3.3-10.2% std=1.98]
  Expert: layer_8[3.3-14.8% std=2.95]
  Expert: layer_9[2.3-14.3% std=2.56]
  Expert: layer_10[2.9-9.8% std=1.76]
  Expert: layer_12[2.5-11.6% std=2.69]
  Expert: layer_13[3.4-11.2% std=1.84]
  Expert: layer_14[3.0-12.8% std=2.33]
  Expert: layer_15[0.5-11.0% std=2.22]
  Expert: layer_16[3.1-12.6% std=2.41]
  Expert: layer_18[1.8-16.2% std=2.97]
  Expert: layer_19[2.8-17.1% std=3.08]
  Expert: layer_20[2.3-11.7% std=2.90]
  Expert: layer_21[2.0-15.0% std=3.05]
Step     571 | Loss: 3.0836 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2547 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.32] | layer_5[4.2-9.0% std=0.99] | layer_11[2.6-12.0% std=2.53] | layer_17[2.6-14.7% std=2.44] | layer_22[1.0-27.9% std=5.97]
  Expert: layer_1[5.9-6.6% std=0.17]
  Expert: layer_2[5.7-6.8% std=0.29]
  Expert: layer_3[2.5-9.8% std=2.23]
  Expert: layer_4[4.2-11.1% std=1.90]
  Expert: layer_6[4.4-9.1% std=1.19]
  Expert: layer_7[3.2-10.5% std=1.98]
  Expert: layer_8[3.3-14.6% std=2.96]
  Expert: layer_9[2.3-14.4% std=2.58]
  Expert: layer_10[2.6-9.4% std=1.73]
  Expert: layer_12[2.4-12.1% std=2.81]
  Expert: layer_13[3.4-11.2% std=1.80]
  Expert: layer_14[2.9-11.8% std=2.32]
  Expert: layer_15[0.6-10.7% std=2.19]
  Expert: layer_16[2.8-11.9% std=2.41]
  Expert: layer_18[1.4-14.6% std=2.71]
  Expert: layer_19[2.7-17.3% std=3.23]
  Expert: layer_20[2.0-12.8% std=3.14]
  Expert: layer_21[2.2-16.3% std=3.29]
Step     572 | Loss: 3.0644 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2511 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.32] | layer_5[4.3-8.8% std=0.95] | layer_11[2.2-11.1% std=2.54] | layer_17[3.1-14.1% std=2.25] | layer_22[0.9-26.7% std=5.65]
  Expert: layer_1[5.8-6.6% std=0.19]
  Expert: layer_2[5.6-6.8% std=0.28]
  Expert: layer_3[2.4-9.8% std=2.28]
  Expert: layer_4[4.1-10.8% std=1.85]
  Expert: layer_6[4.4-9.0% std=1.16]
  Expert: layer_7[3.1-10.9% std=2.03]
  Expert: layer_8[3.2-14.1% std=2.87]
  Expert: layer_9[2.2-15.1% std=2.70]
  Expert: layer_10[2.9-9.5% std=1.70]
  Expert: layer_12[2.5-12.1% std=2.75]
  Expert: layer_13[3.3-10.1% std=1.63]
  Expert: layer_14[2.9-11.7% std=2.12]
  Expert: layer_15[0.6-10.8% std=2.13]
  Expert: layer_16[3.0-12.8% std=2.48]
  Expert: layer_18[1.7-16.1% std=3.00]
  Expert: layer_19[2.7-17.7% std=3.22]
  Expert: layer_20[2.2-11.6% std=2.98]
  Expert: layer_21[2.4-15.9% std=3.05]
Step     573 | Loss: 3.0846 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2556 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.32] | layer_5[4.3-9.0% std=0.99] | layer_11[2.3-11.9% std=2.61] | layer_17[3.6-13.8% std=2.12] | layer_22[1.0-26.2% std=5.54]
  Expert: layer_1[6.0-6.7% std=0.17]
  Expert: layer_2[5.7-6.9% std=0.29]
  Expert: layer_3[2.4-10.1% std=2.23]
  Expert: layer_4[4.0-10.9% std=1.85]
  Expert: layer_6[4.3-8.9% std=1.18]
  Expert: layer_7[2.9-11.2% std=2.15]
  Expert: layer_8[3.2-14.8% std=3.00]
  Expert: layer_9[2.3-15.3% std=2.74]
  Expert: layer_10[2.8-9.8% std=1.80]
  Expert: layer_12[2.4-10.8% std=2.61]
  Expert: layer_13[3.4-10.6% std=1.75]
  Expert: layer_14[2.7-13.4% std=2.38]
  Expert: layer_15[0.6-13.3% std=2.49]
  Expert: layer_16[3.0-11.4% std=2.16]
  Expert: layer_18[1.6-16.0% std=3.03]
  Expert: layer_19[2.7-18.1% std=3.31]
  Expert: layer_20[2.3-12.0% std=2.95]
  Expert: layer_21[2.4-15.6% std=3.01]
Step     574 | Loss: 3.0847 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2536 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.32] | layer_5[4.3-8.9% std=0.99] | layer_11[2.0-12.3% std=2.71] | layer_17[3.0-14.7% std=2.42] | layer_22[0.7-27.9% std=5.95]
  Expert: layer_1[5.9-6.6% std=0.18]
  Expert: layer_2[5.7-6.8% std=0.31]
  Expert: layer_3[2.4-9.6% std=2.15]
  Expert: layer_4[3.9-10.9% std=1.89]
  Expert: layer_6[4.4-8.8% std=1.11]
  Expert: layer_7[3.1-11.3% std=2.16]
  Expert: layer_8[3.0-14.7% std=3.05]
  Expert: layer_9[2.1-15.2% std=2.71]
  Expert: layer_10[2.7-9.6% std=1.82]
  Expert: layer_12[2.6-12.4% std=2.84]
  Expert: layer_13[3.1-10.5% std=1.70]
  Expert: layer_14[2.9-12.6% std=2.33]
  Expert: layer_15[0.6-10.3% std=2.05]
  Expert: layer_16[2.7-12.0% std=2.42]
  Expert: layer_18[1.5-15.7% std=3.00]
  Expert: layer_19[2.5-18.2% std=3.38]
  Expert: layer_20[1.8-13.1% std=3.12]
  Expert: layer_21[2.0-15.6% std=3.11]
Step     575 | Loss: 3.0373 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2507 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.31] | layer_5[4.3-8.8% std=0.94] | layer_11[2.2-12.0% std=2.69] | layer_17[3.0-13.9% std=2.20] | layer_22[0.7-27.1% std=5.79]
  Expert: layer_1[5.9-6.6% std=0.20]
  Expert: layer_2[5.7-6.9% std=0.30]
  Expert: layer_3[2.3-9.8% std=2.22]
  Expert: layer_4[4.0-10.8% std=1.85]
  Expert: layer_6[4.3-9.1% std=1.16]
  Expert: layer_7[3.2-11.4% std=2.14]
  Expert: layer_8[3.0-14.6% std=3.01]
  Expert: layer_9[2.3-15.5% std=2.78]
  Expert: layer_10[2.7-9.6% std=1.81]
  Expert: layer_12[2.5-12.5% std=2.81]
  Expert: layer_13[3.2-10.8% std=1.80]
  Expert: layer_14[2.8-12.7% std=2.23]
  Expert: layer_15[0.6-12.1% std=2.27]
  Expert: layer_16[3.0-12.0% std=2.35]
  Expert: layer_18[1.6-16.6% std=3.23]
  Expert: layer_19[2.3-19.3% std=3.66]
  Expert: layer_20[2.2-13.5% std=3.15]
  Expert: layer_21[2.0-15.6% std=3.16]
Step     576 | Loss: 3.0943 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2488 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.32] | layer_5[4.7-8.7% std=0.86] | layer_11[1.9-12.2% std=2.74] | layer_17[3.3-14.1% std=2.19] | layer_22[0.8-26.9% std=5.68]
  Expert: layer_1[5.9-6.7% std=0.18]
  Expert: layer_2[5.8-6.8% std=0.27]
  Expert: layer_3[2.4-9.9% std=2.27]
  Expert: layer_4[4.1-10.2% std=1.77]
  Expert: layer_6[4.4-8.8% std=1.10]
  Expert: layer_7[3.0-11.5% std=2.20]
  Expert: layer_8[3.2-13.2% std=2.66]
  Expert: layer_9[2.5-15.5% std=2.75]
  Expert: layer_10[2.6-9.9% std=1.74]
  Expert: layer_12[2.5-11.9% std=2.70]
  Expert: layer_13[3.3-11.3% std=1.82]
  Expert: layer_14[2.9-13.0% std=2.30]
  Expert: layer_15[0.5-10.0% std=2.07]
  Expert: layer_16[2.6-12.3% std=2.52]
  Expert: layer_18[1.6-17.5% std=3.25]
  Expert: layer_19[2.5-15.7% std=2.78]
  Expert: layer_20[2.0-13.6% std=3.05]
  Expert: layer_21[2.2-16.6% std=3.20]
Step     577 | Loss: 3.0742 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2489 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.2% std=0.32] | layer_5[4.5-9.0% std=0.97] | layer_11[2.1-12.1% std=2.60] | layer_17[3.2-15.1% std=2.45] | layer_22[0.8-25.7% std=5.50]
  Expert: layer_1[5.9-6.6% std=0.19]
  Expert: layer_2[5.8-6.8% std=0.25]
  Expert: layer_3[2.4-9.6% std=2.14]
  Expert: layer_4[4.2-10.4% std=1.72]
  Expert: layer_6[4.3-8.8% std=1.09]
  Expert: layer_7[3.2-11.9% std=2.34]
  Expert: layer_8[3.2-14.6% std=2.96]
  Expert: layer_9[2.6-14.3% std=2.53]
  Expert: layer_10[2.5-10.0% std=1.84]
  Expert: layer_12[2.4-11.5% std=2.61]
  Expert: layer_13[3.3-10.8% std=1.81]
  Expert: layer_14[2.5-13.3% std=2.40]
  Expert: layer_15[0.6-11.6% std=2.21]
  Expert: layer_16[2.8-10.9% std=2.23]
  Expert: layer_18[1.3-17.6% std=3.41]
  Expert: layer_19[2.1-16.6% std=3.08]
  Expert: layer_20[1.9-14.0% std=3.20]
  Expert: layer_21[2.0-15.6% std=3.16]
Step     578 | Loss: 3.0436 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2521 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.2% std=0.32] | layer_5[4.6-8.7% std=0.88] | layer_11[1.6-11.0% std=2.59] | layer_17[2.9-13.8% std=2.18] | layer_22[0.7-26.9% std=5.73]
  Expert: layer_1[6.0-6.7% std=0.18]
  Expert: layer_2[5.8-6.6% std=0.24]
  Expert: layer_3[2.3-9.7% std=2.15]
  Expert: layer_4[4.1-10.3% std=1.78]
  Expert: layer_6[4.3-8.9% std=1.07]
  Expert: layer_7[3.2-11.9% std=2.33]
  Expert: layer_8[3.2-14.1% std=2.89]
  Expert: layer_9[2.4-14.9% std=2.65]
  Expert: layer_10[2.6-10.2% std=1.79]
  Expert: layer_12[2.7-12.7% std=2.76]
  Expert: layer_13[3.2-10.0% std=1.62]
  Expert: layer_14[2.7-12.7% std=2.20]
  Expert: layer_15[0.6-10.3% std=2.07]
  Expert: layer_16[2.7-12.7% std=2.67]
  Expert: layer_18[1.6-17.0% std=3.21]
  Expert: layer_19[2.6-18.2% std=3.38]
  Expert: layer_20[2.1-12.7% std=3.11]
  Expert: layer_21[2.0-16.5% std=3.33]
Step     579 | Loss: 2.9811 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2512 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.2% std=0.38] | layer_5[4.6-8.7% std=0.92] | layer_11[2.2-11.8% std=2.65] | layer_17[3.2-12.3% std=1.97] | layer_22[1.1-24.7% std=5.24]
  Expert: layer_1[5.9-6.6% std=0.21]
  Expert: layer_2[5.7-6.8% std=0.27]
  Expert: layer_3[2.4-9.6% std=2.07]
  Expert: layer_4[4.2-10.6% std=1.76]
  Expert: layer_6[4.6-9.1% std=1.11]
  Expert: layer_7[2.8-11.6% std=2.36]
  Expert: layer_8[3.1-15.5% std=3.20]
  Expert: layer_9[2.3-15.0% std=2.69]
  Expert: layer_10[2.6-10.1% std=1.84]
  Expert: layer_12[2.5-12.9% std=2.79]
  Expert: layer_13[3.2-10.7% std=1.82]
  Expert: layer_14[2.7-12.2% std=2.30]
  Expert: layer_15[0.5-12.1% std=2.31]
  Expert: layer_16[2.6-13.5% std=2.62]
  Expert: layer_18[1.6-17.1% std=3.39]
  Expert: layer_19[2.5-17.9% std=3.34]
  Expert: layer_20[1.9-12.8% std=3.05]
  Expert: layer_21[2.2-15.2% std=3.18]
Step     580 | Loss: 3.0752 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2508 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.1% std=0.31] | layer_5[4.9-8.8% std=0.88] | layer_11[1.8-13.1% std=2.77] | layer_17[2.6-13.0% std=2.05] | layer_22[0.8-24.6% std=5.18]
  Expert: layer_1[6.0-6.5% std=0.17]
  Expert: layer_2[5.9-6.6% std=0.21]
  Expert: layer_3[2.5-9.8% std=2.18]
  Expert: layer_4[4.2-10.5% std=1.76]
  Expert: layer_6[4.4-8.4% std=1.00]
  Expert: layer_7[2.8-11.2% std=2.50]
  Expert: layer_8[3.1-13.2% std=2.78]
  Expert: layer_9[2.3-15.5% std=2.77]
  Expert: layer_10[2.5-9.6% std=1.69]
  Expert: layer_12[2.7-13.2% std=2.74]
  Expert: layer_13[3.2-10.7% std=1.75]
  Expert: layer_14[2.7-11.9% std=2.25]
  Expert: layer_15[0.5-9.7% std=2.01]
  Expert: layer_16[2.8-13.4% std=2.50]
  Expert: layer_18[1.8-17.2% std=3.18]
  Expert: layer_19[2.7-16.0% std=2.85]
  Expert: layer_20[1.9-12.9% std=3.04]
  Expert: layer_21[2.0-14.2% std=2.97]
Step     581 | Loss: 2.9410 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2520 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.5-7.2% std=0.33] | layer_5[4.7-8.3% std=0.78] | layer_11[1.7-11.7% std=2.59] | layer_17[2.6-12.8% std=2.04] | layer_22[0.7-26.0% std=5.48]
  Expert: layer_1[6.0-6.6% std=0.16]
  Expert: layer_2[5.8-6.5% std=0.20]
  Expert: layer_3[2.4-9.6% std=2.15]
  Expert: layer_4[3.9-10.5% std=1.86]
  Expert: layer_6[4.5-8.9% std=1.07]
  Expert: layer_7[3.1-11.1% std=2.35]
  Expert: layer_8[3.2-13.1% std=2.77]
  Expert: layer_9[2.1-15.4% std=2.81]
  Expert: layer_10[2.6-10.0% std=1.71]
  Expert: layer_12[2.5-12.9% std=2.76]
  Expert: layer_13[3.1-9.8% std=1.61]
  Expert: layer_14[2.6-11.7% std=2.17]
  Expert: layer_15[0.6-9.4% std=1.97]
  Expert: layer_16[2.6-12.5% std=2.56]
  Expert: layer_18[1.6-16.1% std=3.01]
  Expert: layer_19[2.3-15.6% std=2.81]
  Expert: layer_20[1.9-13.2% std=3.26]
  Expert: layer_21[2.0-15.3% std=3.05]
Step     582 | Loss: 3.0209 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2545 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.4% std=0.36] | layer_5[4.7-8.5% std=0.85] | layer_11[1.7-11.8% std=2.69] | layer_17[3.1-12.7% std=1.92] | layer_22[0.7-24.4% std=5.16]
  Expert: layer_1[6.0-6.7% std=0.18]
  Expert: layer_2[5.8-6.5% std=0.17]
  Expert: layer_3[2.4-9.7% std=2.17]
  Expert: layer_4[3.9-10.8% std=1.88]
  Expert: layer_6[4.6-8.7% std=1.00]
  Expert: layer_7[2.9-11.2% std=2.53]
  Expert: layer_8[3.3-13.8% std=2.87]
  Expert: layer_9[2.0-16.2% std=2.98]
  Expert: layer_10[2.5-10.4% std=1.79]
  Expert: layer_12[2.5-12.2% std=2.79]
  Expert: layer_13[3.2-10.2% std=1.76]
  Expert: layer_14[2.5-11.9% std=2.14]
  Expert: layer_15[0.5-10.7% std=2.13]
  Expert: layer_16[2.5-12.0% std=2.43]
  Expert: layer_18[1.7-16.5% std=3.10]
  Expert: layer_19[2.5-15.8% std=2.80]
  Expert: layer_20[1.7-13.1% std=3.18]
  Expert: layer_21[2.0-15.4% std=3.06]
Step     583 | Loss: 3.0082 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2541 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.2% std=0.35] | layer_5[4.7-8.4% std=0.81] | layer_11[1.8-11.8% std=2.79] | layer_17[3.1-13.2% std=2.05] | layer_22[0.9-25.9% std=5.46]
  Expert: layer_1[5.9-6.7% std=0.18]
  Expert: layer_2[5.8-6.5% std=0.20]
  Expert: layer_3[2.5-9.9% std=2.23]
  Expert: layer_4[3.9-10.5% std=1.83]
  Expert: layer_6[4.5-8.8% std=1.01]
  Expert: layer_7[2.9-11.0% std=2.33]
  Expert: layer_8[3.5-12.1% std=2.54]
  Expert: layer_9[1.8-16.9% std=3.12]
  Expert: layer_10[2.5-10.5% std=1.77]
  Expert: layer_12[2.4-12.5% std=2.79]
  Expert: layer_13[3.3-10.0% std=1.70]
  Expert: layer_14[2.5-12.7% std=2.25]
  Expert: layer_15[0.5-10.7% std=2.17]
  Expert: layer_16[2.6-12.9% std=2.53]
  Expert: layer_18[1.8-16.4% std=3.02]
  Expert: layer_19[2.7-16.6% std=2.96]
  Expert: layer_20[1.9-12.4% std=3.00]
  Expert: layer_21[1.9-16.8% std=3.34]
Step     584 | Loss: 3.0047 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2546 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.2% std=0.32] | layer_5[4.6-8.6% std=0.86] | layer_11[2.0-12.9% std=2.80] | layer_17[3.1-14.1% std=2.27] | layer_22[0.8-25.0% std=5.28]
  Expert: layer_1[5.9-6.6% std=0.18]
  Expert: layer_2[5.9-6.6% std=0.18]
  Expert: layer_3[2.4-9.8% std=2.19]
  Expert: layer_4[3.9-11.0% std=1.85]
  Expert: layer_6[4.6-8.7% std=1.03]
  Expert: layer_7[3.0-11.2% std=2.36]
  Expert: layer_8[3.4-12.5% std=2.65]
  Expert: layer_9[1.8-15.8% std=2.91]
  Expert: layer_10[2.8-10.4% std=1.77]
  Expert: layer_12[2.4-12.8% std=2.87]
  Expert: layer_13[3.6-10.6% std=1.75]
  Expert: layer_14[2.7-12.0% std=2.26]
  Expert: layer_15[0.5-10.6% std=2.15]
  Expert: layer_16[2.8-11.3% std=2.14]
  Expert: layer_18[1.6-16.3% std=3.05]
  Expert: layer_19[2.7-15.9% std=2.84]
  Expert: layer_20[1.8-11.8% std=3.05]
  Expert: layer_21[1.7-15.7% std=3.17]
Step     585 | Loss: 3.0091 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2529 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.2% std=0.34] | layer_5[4.4-8.7% std=0.90] | layer_11[1.9-12.1% std=2.80] | layer_17[3.0-14.3% std=2.28] | layer_22[1.0-26.2% std=5.51]
  Expert: layer_1[5.9-6.7% std=0.18]
  Expert: layer_2[5.9-6.5% std=0.17]
  Expert: layer_3[2.4-9.7% std=2.20]
  Expert: layer_4[3.9-10.8% std=1.85]
  Expert: layer_6[4.6-9.2% std=1.12]
  Expert: layer_7[3.2-11.3% std=2.28]
  Expert: layer_8[3.5-12.3% std=2.56]
  Expert: layer_9[1.6-16.4% std=3.06]
  Expert: layer_10[2.6-10.2% std=1.74]
  Expert: layer_12[2.5-12.5% std=2.78]
  Expert: layer_13[3.6-9.9% std=1.77]
  Expert: layer_14[2.5-11.9% std=2.17]
  Expert: layer_15[0.5-10.9% std=2.22]
  Expert: layer_16[2.4-12.3% std=2.35]
  Expert: layer_18[1.7-14.9% std=2.74]
  Expert: layer_19[2.6-16.6% std=2.95]
  Expert: layer_20[2.1-11.4% std=2.82]
  Expert: layer_21[2.1-15.1% std=2.99]
Step     586 | Loss: 3.0176 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2533 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.3% std=0.35] | layer_5[4.5-8.8% std=0.91] | layer_11[2.0-12.3% std=2.67] | layer_17[3.4-13.9% std=2.18] | layer_22[0.8-26.5% std=5.60]
  Expert: layer_1[6.0-6.6% std=0.15]
  Expert: layer_2[5.9-6.5% std=0.18]
  Expert: layer_3[2.4-9.8% std=2.22]
  Expert: layer_4[3.8-10.7% std=1.85]
  Expert: layer_6[4.6-9.0% std=1.06]
  Expert: layer_7[3.1-11.2% std=2.26]
  Expert: layer_8[3.4-12.1% std=2.52]
  Expert: layer_9[1.6-16.1% std=2.96]
  Expert: layer_10[2.7-10.0% std=1.69]
  Expert: layer_12[2.3-12.1% std=2.74]
  Expert: layer_13[4.1-10.2% std=1.69]
  Expert: layer_14[2.4-12.4% std=2.26]
  Expert: layer_15[0.5-11.1% std=2.21]
  Expert: layer_16[2.8-11.7% std=2.26]
  Expert: layer_18[1.9-16.6% std=3.03]
  Expert: layer_19[2.6-15.0% std=2.64]
  Expert: layer_20[1.9-12.4% std=2.86]
  Expert: layer_21[2.0-15.0% std=2.99]
Per-layer gamma: 0.0001 ~ 0.0005 (sqrt, 23 MoE layers)

============================================================
학습 시작 (step=585, epoch=0)
============================================================
Resume position: global_idx=256082, block=2, local_idx=56402

=== Epoch 1/1 ===
Step     586 | Loss: 3.0176 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2677 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.3% std=0.35] | layer_5[4.5-8.8% std=0.91] | layer_11[2.0-12.3% std=2.67] | layer_17[3.4-13.9% std=2.18] | layer_22[0.8-26.5% std=5.60]
  Expert: layer_1[6.0-6.6% std=0.15]
  Expert: layer_2[5.9-6.5% std=0.18]
  Expert: layer_3[2.4-9.8% std=2.22]
  Expert: layer_4[3.8-10.7% std=1.85]
  Expert: layer_6[4.6-9.0% std=1.06]
  Expert: layer_7[3.1-11.2% std=2.26]
  Expert: layer_8[3.4-12.1% std=2.52]
  Expert: layer_9[1.6-16.1% std=2.96]
  Expert: layer_10[2.7-10.0% std=1.69]
  Expert: layer_12[2.3-12.1% std=2.74]
  Expert: layer_13[4.1-10.2% std=1.69]
  Expert: layer_14[2.4-12.4% std=2.26]
  Expert: layer_15[0.5-11.1% std=2.21]
  Expert: layer_16[2.8-11.7% std=2.26]
  Expert: layer_18[1.9-16.6% std=3.03]
  Expert: layer_19[2.6-15.0% std=2.64]
  Expert: layer_20[1.9-12.4% std=2.86]
  Expert: layer_21[2.0-15.0% std=2.99]
Step     587 | Loss: 3.0435 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2637 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.3% std=0.33] | layer_5[4.5-8.8% std=0.91] | layer_11[2.2-12.2% std=2.68] | layer_17[3.1-13.4% std=2.07] | layer_22[0.7-25.9% std=5.46]
  Expert: layer_1[5.9-6.7% std=0.16]
  Expert: layer_2[5.9-6.5% std=0.16]
  Expert: layer_3[2.5-9.8% std=2.22]
  Expert: layer_4[3.9-10.8% std=1.89]
  Expert: layer_6[4.6-9.5% std=1.17]
  Expert: layer_7[3.3-11.0% std=2.21]
  Expert: layer_8[3.6-11.5% std=2.38]
  Expert: layer_9[1.4-16.8% std=3.18]
  Expert: layer_10[2.8-10.1% std=1.70]
  Expert: layer_12[2.3-12.9% std=2.84]
  Expert: layer_13[4.0-9.5% std=1.53]
  Expert: layer_14[2.4-12.7% std=2.26]
  Expert: layer_15[0.6-10.0% std=2.03]
  Expert: layer_16[2.4-12.9% std=2.56]
  Expert: layer_18[1.9-16.9% std=3.11]
  Expert: layer_19[2.6-16.3% std=2.89]
  Expert: layer_20[1.8-11.6% std=2.98]
  Expert: layer_21[2.0-15.4% std=2.99]
Step     588 | Loss: 3.0145 | LR: 2.50e-04 | GradNorm: 0.26 | Tok/s: 2652 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.34] | layer_5[4.4-8.8% std=0.93] | layer_11[2.2-11.4% std=2.51] | layer_17[3.8-14.2% std=2.20] | layer_22[0.7-24.8% std=5.22]
  Expert: layer_1[6.0-6.6% std=0.16]
  Expert: layer_2[5.9-6.6% std=0.17]
  Expert: layer_3[2.4-10.0% std=2.18]
  Expert: layer_4[3.9-10.5% std=1.84]
  Expert: layer_6[4.4-9.6% std=1.16]
  Expert: layer_7[3.6-10.6% std=2.06]
  Expert: layer_8[3.8-12.1% std=2.39]
  Expert: layer_9[1.4-16.4% std=3.07]
  Expert: layer_10[2.9-9.9% std=1.71]
  Expert: layer_12[2.2-12.5% std=2.84]
  Expert: layer_13[3.6-10.1% std=1.68]
  Expert: layer_14[2.5-13.3% std=2.38]
  Expert: layer_15[0.5-11.1% std=2.18]
  Expert: layer_16[2.7-12.5% std=2.46]
  Expert: layer_18[1.7-15.8% std=2.94]
  Expert: layer_19[2.5-17.3% std=3.16]
  Expert: layer_20[1.8-13.2% std=3.04]
  Expert: layer_21[2.2-16.4% std=3.17]
Step     589 | Loss: 3.0735 | LR: 2.50e-04 | GradNorm: 0.27 | Tok/s: 2647 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.33] | layer_5[4.7-9.0% std=0.95] | layer_11[1.9-11.5% std=2.53] | layer_17[3.4-13.0% std=1.97] | layer_22[0.7-26.3% std=5.53]
  Expert: layer_1[5.9-6.7% std=0.16]
  Expert: layer_2[6.0-6.6% std=0.16]
  Expert: layer_3[2.5-10.1% std=2.23]
  Expert: layer_4[3.8-10.5% std=1.86]
  Expert: layer_6[4.4-9.3% std=1.12]
  Expert: layer_7[3.2-10.7% std=2.24]
  Expert: layer_8[3.5-11.4% std=2.29]
  Expert: layer_9[1.4-16.4% std=3.07]
  Expert: layer_10[3.1-9.6% std=1.60]
  Expert: layer_12[2.1-13.0% std=2.88]
  Expert: layer_13[3.9-9.8% std=1.55]
  Expert: layer_14[2.4-11.8% std=2.07]
  Expert: layer_15[0.5-9.9% std=2.02]
  Expert: layer_16[2.6-13.2% std=2.44]
  Expert: layer_18[1.6-17.5% std=3.27]
  Expert: layer_19[2.5-14.6% std=2.56]
  Expert: layer_20[2.4-12.2% std=2.83]
  Expert: layer_21[1.8-15.4% std=3.05]
Step     590 | Loss: 3.0292 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2673 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.34] | layer_5[4.8-9.3% std=1.00] | layer_11[1.9-11.6% std=2.61] | layer_17[3.7-13.1% std=1.98] | layer_22[0.6-27.4% std=5.84]
  Expert: layer_1[5.8-6.6% std=0.19]
  Expert: layer_2[5.9-6.7% std=0.20]
  Expert: layer_3[2.5-10.4% std=2.25]
  Expert: layer_4[3.7-10.4% std=1.82]
  Expert: layer_6[4.3-9.4% std=1.12]
  Expert: layer_7[2.9-10.3% std=2.18]
  Expert: layer_8[3.6-12.0% std=2.40]
  Expert: layer_9[1.6-16.5% std=3.03]
  Expert: layer_10[2.9-9.7% std=1.68]
  Expert: layer_12[2.5-13.0% std=2.84]
  Expert: layer_13[3.7-9.6% std=1.61]
  Expert: layer_14[2.3-12.2% std=2.23]
  Expert: layer_15[0.6-10.7% std=2.15]
  Expert: layer_16[2.8-12.1% std=2.43]
  Expert: layer_18[1.6-16.1% std=3.00]
  Expert: layer_19[2.6-17.1% std=3.08]
  Expert: layer_20[2.1-12.0% std=2.94]
  Expert: layer_21[1.7-16.1% std=3.27]
Step     591 | Loss: 3.0610 | LR: 2.50e-04 | GradNorm: 0.34 | Tok/s: 2640 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.34] | layer_5[4.9-9.4% std=1.01] | layer_11[1.9-11.6% std=2.60] | layer_17[3.4-13.4% std=2.03] | layer_22[0.7-25.2% std=5.24]
  Expert: layer_1[5.9-6.6% std=0.17]
  Expert: layer_2[5.9-6.6% std=0.20]
  Expert: layer_3[2.6-10.6% std=2.36]
  Expert: layer_4[3.9-10.4% std=1.85]
  Expert: layer_6[4.4-9.4% std=1.14]
  Expert: layer_7[2.8-10.5% std=2.26]
  Expert: layer_8[3.7-11.0% std=2.20]
  Expert: layer_9[1.5-16.6% std=3.07]
  Expert: layer_10[3.1-9.7% std=1.61]
  Expert: layer_12[2.4-13.1% std=2.87]
  Expert: layer_13[3.8-9.7% std=1.48]
  Expert: layer_14[2.5-13.3% std=2.34]
  Expert: layer_15[0.5-9.3% std=2.03]
  Expert: layer_16[2.9-12.3% std=2.48]
  Expert: layer_18[1.9-16.6% std=3.03]
  Expert: layer_19[2.4-15.9% std=2.87]
  Expert: layer_20[2.0-12.1% std=2.84]
  Expert: layer_21[2.4-15.5% std=2.92]
Step     592 | Loss: 3.0463 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2660 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.30] | layer_5[4.8-9.4% std=0.97] | layer_11[2.2-11.3% std=2.36] | layer_17[3.1-14.1% std=2.24] | layer_22[0.7-26.7% std=5.63]
  Expert: layer_1[5.8-6.8% std=0.19]
  Expert: layer_2[5.9-6.7% std=0.18]
  Expert: layer_3[2.8-10.5% std=2.27]
  Expert: layer_4[3.8-10.1% std=1.77]
  Expert: layer_6[4.4-9.6% std=1.19]
  Expert: layer_7[3.2-10.3% std=2.16]
  Expert: layer_8[3.8-11.6% std=2.31]
  Expert: layer_9[1.6-16.6% std=3.08]
  Expert: layer_10[2.8-9.5% std=1.73]
  Expert: layer_12[2.3-12.1% std=2.65]
  Expert: layer_13[3.5-9.8% std=1.59]
  Expert: layer_14[2.2-13.2% std=2.46]
  Expert: layer_15[0.7-9.0% std=1.95]
  Expert: layer_16[2.5-12.1% std=2.40]
  Expert: layer_18[1.7-16.0% std=2.91]
  Expert: layer_19[2.5-15.3% std=2.77]
  Expert: layer_20[1.9-13.7% std=3.21]
  Expert: layer_21[1.9-17.2% std=3.40]
Step     593 | Loss: 2.9846 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2655 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.2% std=0.32] | layer_5[4.7-9.2% std=0.95] | layer_11[2.2-11.3% std=2.51] | layer_17[3.9-12.0% std=1.71] | layer_22[0.9-25.9% std=5.45]
  Expert: layer_1[5.9-6.7% std=0.20]
  Expert: layer_2[5.9-6.7% std=0.20]
  Expert: layer_3[2.7-10.6% std=2.24]
  Expert: layer_4[3.8-10.3% std=1.77]
  Expert: layer_6[4.5-9.7% std=1.19]
  Expert: layer_7[2.8-10.0% std=2.12]
  Expert: layer_8[3.7-12.1% std=2.44]
  Expert: layer_9[1.5-16.4% std=3.01]
  Expert: layer_10[2.7-10.1% std=1.79]
  Expert: layer_12[2.5-12.4% std=2.79]
  Expert: layer_13[3.5-9.2% std=1.61]
  Expert: layer_14[2.4-12.4% std=2.33]
  Expert: layer_15[0.4-10.8% std=2.16]
  Expert: layer_16[2.9-11.7% std=2.24]
  Expert: layer_18[1.9-15.5% std=2.83]
  Expert: layer_19[2.5-16.4% std=2.96]
  Expert: layer_20[2.3-13.2% std=2.88]
  Expert: layer_21[1.9-13.0% std=2.68]
Step     594 | Loss: 3.0191 | LR: 2.50e-04 | GradNorm: 0.30 | Tok/s: 2646 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.33] | layer_5[4.8-9.1% std=0.92] | layer_11[1.9-11.4% std=2.39] | layer_17[4.0-13.1% std=1.96] | layer_22[0.7-25.2% std=5.26]
  Expert: layer_1[5.9-6.6% std=0.17]
  Expert: layer_2[5.9-6.7% std=0.18]
  Expert: layer_3[2.8-10.9% std=2.29]
  Expert: layer_4[3.9-10.2% std=1.77]
  Expert: layer_6[4.4-9.3% std=1.12]
  Expert: layer_7[2.9-10.9% std=2.28]
  Expert: layer_8[3.7-11.9% std=2.36]
  Expert: layer_9[1.6-15.5% std=2.80]
  Expert: layer_10[2.9-10.1% std=1.73]
  Expert: layer_12[2.6-12.0% std=2.60]
  Expert: layer_13[3.6-9.2% std=1.51]
  Expert: layer_14[2.6-12.6% std=2.24]
  Expert: layer_15[0.5-9.8% std=2.10]
  Expert: layer_16[3.2-11.6% std=2.28]
  Expert: layer_18[1.9-14.6% std=2.61]
  Expert: layer_19[2.5-15.3% std=2.67]
  Expert: layer_20[1.9-11.3% std=2.61]
  Expert: layer_21[2.3-15.2% std=2.89]
Step     595 | Loss: 3.0085 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2642 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.6-7.1% std=0.33] | layer_5[4.7-9.6% std=1.05] | layer_11[2.6-12.1% std=2.56] | layer_17[3.4-14.6% std=2.35] | layer_22[0.9-24.2% std=5.08]
  Expert: layer_1[6.0-6.5% std=0.15]
  Expert: layer_2[5.9-7.0% std=0.24]
  Expert: layer_3[2.9-10.4% std=2.23]
  Expert: layer_4[4.0-10.4% std=1.76]
  Expert: layer_6[4.3-9.6% std=1.21]
  Expert: layer_7[3.1-10.3% std=2.19]
  Expert: layer_8[3.8-13.0% std=2.56]
  Expert: layer_9[1.6-15.1% std=2.82]
  Expert: layer_10[2.6-9.7% std=1.78]
  Expert: layer_12[2.6-11.8% std=2.67]
  Expert: layer_13[3.5-10.1% std=1.74]
  Expert: layer_14[2.7-12.0% std=2.16]
  Expert: layer_15[0.6-10.8% std=2.04]
  Expert: layer_16[3.0-11.3% std=2.19]
  Expert: layer_18[1.8-17.4% std=3.20]
  Expert: layer_19[2.9-17.4% std=3.15]
  Expert: layer_20[1.8-12.6% std=2.97]
  Expert: layer_21[1.8-15.6% std=3.16]
Step     596 | Loss: 3.0568 | LR: 2.50e-04 | GradNorm: 0.28 | Tok/s: 2615 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.1% std=0.33] | layer_5[4.8-9.5% std=0.98] | layer_11[2.1-12.0% std=2.68] | layer_17[3.1-14.4% std=2.27] | layer_22[0.8-25.2% std=5.27]
  Expert: layer_1[5.9-6.6% std=0.17]
  Expert: layer_2[5.9-6.8% std=0.22]
  Expert: layer_3[2.8-10.7% std=2.29]
  Expert: layer_4[4.0-10.1% std=1.72]
  Expert: layer_6[4.5-9.4% std=1.16]
  Expert: layer_7[3.2-10.4% std=2.21]
  Expert: layer_8[3.6-11.2% std=2.25]
  Expert: layer_9[1.6-15.4% std=2.85]
  Expert: layer_10[2.7-9.1% std=1.69]
  Expert: layer_12[2.7-12.3% std=2.75]
  Expert: layer_13[3.4-9.6% std=1.62]
  Expert: layer_14[2.7-12.5% std=2.27]
  Expert: layer_15[0.5-9.3% std=1.93]
  Expert: layer_16[2.8-13.0% std=2.52]
  Expert: layer_18[1.7-17.2% std=3.14]
  Expert: layer_19[2.7-17.3% std=3.06]
  Expert: layer_20[2.1-11.6% std=2.70]
  Expert: layer_21[2.1-14.9% std=2.85]
Step     597 | Loss: 3.0212 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2648 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.2% std=0.33] | layer_5[5.0-9.3% std=0.95] | layer_11[2.4-12.4% std=2.57] | layer_17[3.7-12.3% std=1.77] | layer_22[1.1-25.0% std=5.21]
  Expert: layer_1[5.9-6.6% std=0.17]
  Expert: layer_2[5.9-6.9% std=0.22]
  Expert: layer_3[2.8-10.8% std=2.19]
  Expert: layer_4[3.8-10.1% std=1.74]
  Expert: layer_6[4.4-9.2% std=1.16]
  Expert: layer_7[2.2-10.5% std=2.24]
  Expert: layer_8[3.7-13.1% std=2.52]
  Expert: layer_9[1.7-15.0% std=2.69]
  Expert: layer_10[2.9-9.6% std=1.69]
  Expert: layer_12[2.6-11.7% std=2.65]
  Expert: layer_13[3.4-9.6% std=1.65]
  Expert: layer_14[2.5-14.0% std=2.56]
  Expert: layer_15[0.4-11.0% std=2.17]
  Expert: layer_16[3.1-12.0% std=2.25]
  Expert: layer_18[1.8-16.5% std=3.01]
  Expert: layer_19[2.6-16.4% std=2.93]
  Expert: layer_20[2.2-12.1% std=2.73]
  Expert: layer_21[2.1-14.4% std=2.78]
Step     598 | Loss: 3.0953 | LR: 2.50e-04 | GradNorm: 0.26 | Tok/s: 2653 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.31] | layer_5[4.9-9.4% std=0.97] | layer_11[2.9-12.7% std=2.72] | layer_17[3.0-14.1% std=2.21] | layer_22[1.1-25.4% std=5.33]
  Expert: layer_1[6.0-6.6% std=0.17]
  Expert: layer_2[5.8-6.9% std=0.23]
  Expert: layer_3[2.9-10.8% std=2.19]
  Expert: layer_4[3.9-10.1% std=1.71]
  Expert: layer_6[4.3-9.3% std=1.20]
  Expert: layer_7[2.9-10.1% std=2.12]
  Expert: layer_8[3.6-13.6% std=2.61]
  Expert: layer_9[1.8-15.0% std=2.72]
  Expert: layer_10[2.7-9.3% std=1.69]
  Expert: layer_12[2.4-12.3% std=2.77]
  Expert: layer_13[3.2-10.3% std=1.72]
  Expert: layer_14[3.0-12.6% std=2.31]
  Expert: layer_15[0.6-10.9% std=2.14]
  Expert: layer_16[2.7-12.2% std=2.37]
  Expert: layer_18[1.8-16.1% std=2.88]
  Expert: layer_19[2.8-17.3% std=3.09]
  Expert: layer_20[1.6-12.7% std=2.85]
  Expert: layer_21[2.0-15.3% std=3.08]
Step     599 | Loss: 3.0178 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2653 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.2% std=0.31] | layer_5[5.0-8.9% std=0.87] | layer_11[2.2-11.4% std=2.52] | layer_17[3.5-13.2% std=1.99] | layer_22[1.0-23.8% std=4.92]
  Expert: layer_1[6.0-6.6% std=0.18]
  Expert: layer_2[5.9-6.9% std=0.21]
  Expert: layer_3[2.8-10.8% std=2.18]
  Expert: layer_4[3.7-9.9% std=1.71]
  Expert: layer_6[4.4-9.4% std=1.22]
  Expert: layer_7[3.2-10.7% std=2.07]
  Expert: layer_8[3.6-13.1% std=2.53]
  Expert: layer_9[1.8-15.1% std=2.73]
  Expert: layer_10[3.0-9.1% std=1.55]
  Expert: layer_12[2.3-12.8% std=2.72]
  Expert: layer_13[3.0-9.2% std=1.60]
  Expert: layer_14[2.9-12.0% std=2.16]
  Expert: layer_15[0.6-9.6% std=1.99]
  Expert: layer_16[2.7-12.1% std=2.43]
  Expert: layer_18[1.7-16.2% std=2.94]
  Expert: layer_19[2.8-17.6% std=3.15]
  Expert: layer_20[1.9-11.6% std=2.65]
  Expert: layer_21[2.0-14.8% std=2.83]
Step     600 | Loss: 3.0313 | LR: 2.50e-04 | GradNorm: 0.33 | Tok/s: 2645 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.3% std=0.32] | layer_5[4.9-9.2% std=0.90] | layer_11[2.5-11.9% std=2.55] | layer_17[3.5-12.4% std=1.88] | layer_22[1.2-23.5% std=4.84]
  Expert: layer_1[5.9-6.7% std=0.19]
  Expert: layer_2[5.9-6.9% std=0.23]
  Expert: layer_3[2.8-10.3% std=2.13]
  Expert: layer_4[3.8-9.8% std=1.67]
  Expert: layer_6[4.3-9.3% std=1.24]
  Expert: layer_7[3.2-10.4% std=1.97]
  Expert: layer_8[3.9-12.8% std=2.48]
  Expert: layer_9[1.9-14.7% std=2.64]
  Expert: layer_10[2.9-8.8% std=1.51]
  Expert: layer_12[2.4-11.5% std=2.65]
  Expert: layer_13[3.2-9.4% std=1.61]
  Expert: layer_14[3.1-13.5% std=2.37]
  Expert: layer_15[0.6-10.5% std=2.04]
  Expert: layer_16[3.1-10.3% std=1.83]
  Expert: layer_18[1.9-16.6% std=3.00]
  Expert: layer_19[2.9-16.1% std=2.83]
  Expert: layer_20[2.1-11.3% std=2.50]
  Expert: layer_21[1.9-13.6% std=2.70]
Step     601 | Loss: 3.0719 | LR: 2.50e-04 | GradNorm: 0.32 | Tok/s: 2639 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.1% std=0.29] | layer_5[4.8-9.1% std=0.88] | layer_11[2.5-10.8% std=2.36] | layer_17[3.9-12.4% std=1.85] | layer_22[0.8-24.6% std=5.13]
  Expert: layer_1[5.9-6.8% std=0.20]
  Expert: layer_2[5.9-7.0% std=0.26]
  Expert: layer_3[2.7-10.7% std=2.13]
  Expert: layer_4[3.9-9.7% std=1.67]
  Expert: layer_6[4.3-9.5% std=1.24]
  Expert: layer_7[3.2-10.3% std=1.96]
  Expert: layer_8[3.8-13.5% std=2.69]
  Expert: layer_9[1.8-14.9% std=2.66]
  Expert: layer_10[2.9-9.0% std=1.55]
  Expert: layer_12[2.4-11.9% std=2.52]
  Expert: layer_13[3.1-8.9% std=1.51]
  Expert: layer_14[2.7-12.6% std=2.32]
  Expert: layer_15[0.6-10.2% std=1.98]
  Expert: layer_16[2.9-12.5% std=2.31]
  Expert: layer_18[1.7-16.7% std=3.11]
  Expert: layer_19[2.6-16.7% std=2.97]
  Expert: layer_20[2.3-11.5% std=2.59]
  Expert: layer_21[2.1-14.6% std=2.82]
Step     602 | Loss: 2.9824 | LR: 2.50e-04 | GradNorm: 0.29 | Tok/s: 2641 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.7-7.1% std=0.28] | layer_5[4.9-9.3% std=0.91] | layer_11[2.2-11.5% std=2.57] | layer_17[3.6-13.0% std=1.91] | layer_22[1.0-25.5% std=5.34]
  Expert: layer_1[6.0-6.8% std=0.20]
  Expert: layer_2[5.9-7.0% std=0.25]
  Expert: layer_3[2.7-10.5% std=2.11]
  Expert: layer_4[3.8-9.7% std=1.65]
  Expert: layer_6[4.3-9.0% std=1.21]
  Expert: layer_7[3.3-9.7% std=1.89]
  Expert: layer_8[3.9-13.4% std=2.56]
  Expert: layer_9[1.9-15.0% std=2.66]
  Expert: layer_10[3.0-8.7% std=1.51]
  Expert: layer_12[2.4-11.4% std=2.66]
  Expert: layer_13[3.0-9.2% std=1.52]
  Expert: layer_14[3.2-12.2% std=2.15]
  Expert: layer_15[0.7-10.3% std=1.99]
  Expert: layer_16[2.9-11.3% std=2.18]
  Expert: layer_18[1.8-15.1% std=2.72]
  Expert: layer_19[2.6-16.9% std=3.03]
  Expert: layer_20[2.0-11.5% std=2.56]
  Expert: layer_21[2.0-14.5% std=2.78]
Step     603 | Loss: 2.9668 | LR: 2.50e-04 | GradNorm: 0.31 | Tok/s: 2619 | Mem: 65.5GB | Data: train_2k/019.npy
  Expert: layer_0[5.8-7.1% std=0.28] | layer_5[4.9-9.5% std=0.95] | layer_11[2.6-12.5% std=2.58] | layer_17[4.0-12.7% std=1.82] | layer_22[1.2-23.7% std=4.92]
  Expert: layer_1[6.0-6.8% std=0.19]
  Expert: layer_2[5.9-7.1% std=0.34]
  Expert: layer_3[2.7-9.8% std=2.03]
  Expert: layer_4[3.8-9.9% std=1.59]
  Expert: layer_6[4.2-8.8% std=1.17]
  Expert: layer_7[3.1-9.5% std=1.93]
  Expert: layer_8[3.8-14.7% std=2.80]
  Expert: layer_9[2.0-14.8% std=2.60]
  Expert: layer_10[2.8-8.9% std=1.56]
  Expert: layer_12[2.6-11.7% std=2.58]
  Expert: layer_13[3.4-10.3% std=1.71]
  Expert: layer_14[2.9-12.9% std=2.37]
  Expert: layer_15[0.6-10.5% std=1.98]
  Expert: layer_16[2.9-11.7% std=2.03]
  Expert: layer_18[1.8-15.7% std=2.84]
  Expert: layer_19[2.3-15.7% std=2.84]
  Expert: layer_20[1.8-11.6% std=2.60]
  Expert: layer_21[2.0-13.5% std=2.68]
